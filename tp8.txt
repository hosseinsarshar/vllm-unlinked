(vllm) (base) hosseins@t1v-n-a996901e-w-0:~$ vllm serve "meta-llama/Meta-Llama-3.1-8B" --download_dir /dev/shm --num-scheduler-steps 4 --swap-space 16 --disable-log-requests --tensor_parallel_size=8 --max-model-len=256 --dtype=bfloat16
INFO 01-18 04:04:33 api_server.py:773] vLLM API server version 0.1.dev4049+g630020f.d20250116
INFO 01-18 04:04:33 api_server.py:774] args: Namespace(subparser='serve', model_tag='meta-llama/Meta-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=256, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=4, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7d5049e8a4d0>)
hosseins: api_server.py run_server
hosseins: api_server.py build_async_engine_client() args=Namespace(subparser='serve', model_tag='meta-llama/Meta-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=256, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=4, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7d5049e8a4d0>)
INFO 01-18 04:04:33 __init__.py:179] Automatically detected platform tpu.
hosseins: api_server.py build_async_engine_client_from_engine_args() engine_args=AsyncEngineArgs(model='meta-llama/Meta-Llama-3.1-8B', served_model_name=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', task='auto', skip_tokenizer_init=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, seed=0, max_model_len=256, worker_use_ray=False, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, revision=None, code_revision=None, rope_scaling=None, rope_theta=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, fully_sharded_loras=False, lora_extra_vocab_size=256, long_lora_scaling_factors=None, lora_dtype='auto', max_cpu_loras=None, device='auto', num_scheduler_steps=4, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, speculative_model=None, speculative_model_quantization=None, speculative_draft_tensor_parallel_size=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, qlora_adapter_name_or_path=None, disable_logprobs_during_spec_decoding=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto', kv_transfer_config=None, generation_config=None, disable_log_requests=True)
hosseins: build_async_engine_client_from_engine_args() -> else if PROMETHEUS_MULTIPROC_DIR not in
INFO 01-18 04:04:33 api_server.py:199] Started engine process with PID 1643477
INFO 01-18 04:04:36 __init__.py:179] Automatically detected platform tpu.
INFO 01-18 04:04:39 config.py:517] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
hosseins: ray_utils assert_ray_available()
INFO 01-18 04:04:42 config.py:517] This model supports multiple tasks: {'classify', 'score', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.
hosseins: ray_utils assert_ray_available()
hosseins: LLMEngine -> _get_executor_cls(): engine_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')
hosseins: ray_utils initialize_ray_cluster parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0)
hosseins: ray_utils initialize_ray_cluster ray_address=None
hosseins: ray_utils assert_ray_available()
2025-01-18 04:04:43,179 INFO worker.py:1812 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 
hosseins: ray_utils initialize_ray_cluster current_placement_group=None
hosseins: ray_utils initialize_ray_cluster num_devices_in_cluster=8.0
hosseins: ray_utils initialize_ray_cluster placement_group_specs=[{'TPU': 1.0}, {'TPU': 1.0}, {'TPU': 1.0}, {'TPU': 1.0}, {'TPU': 1.0}, {'TPU': 1.0}, {'TPU': 1.0}, {'TPU': 1.0}]
hosseins: ray_utils initialize_ray_cluster current_ip='10.130.0.54'
hosseins: ray_utils initialize_ray_cluster current_node_id='fed67d48475d141b155908ade7df8f6f22c7eec2ac519b7ec9c215be'
hosseins: ray_utils initialize_ray_cluster current_node_resource={'node:__internal_head__': 1.0, 'TPU': 8.0, 'TPU-v6e-8-head': 1.0, 'accelerator_type:TPU-V6E': 1.0, 'CPU': 180.0, 'memory': 1304568327168.0, 'object_store_memory': 200000000000.0, 'hosseins-profile-test-v6e-8': 1.0, 'node:10.130.0.54': 1.0}
hosseins: ray_utils _wait_until_pg_ready()
hosseins: ray_utils _verify_bundles() placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>
hosseins: ray_utils _verify_bundles() parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0)
hosseins: LLMEngine -> _get_executor_cls(): executor_class=<class 'vllm.executor.ray_tpu_executor.RayTPUExecutor'>
hosseins: LLMEngine -> __init__() vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')
INFO 01-18 04:04:44 llm_engine.py:235] Initializing an LLM engine (v0.1.dev4049+g630020f.d20250116) with config: model='meta-llama/Meta-Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir='/dev/shm', load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B, num_scheduler_steps=4, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
hosseins: LLMEngine -> _init_tokenizer()
hosseins: LLMEngine -> get_tokenizer_group() group_type=<class 'vllm.transformers_utils.tokenizer_group.base_tokenizer_group.BaseTokenizerGroup'>
hosseins: RayTPUExecutor -> __init__() args=()
hosseins: RayTPUExecutor -> __init__() kwargs={'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')}
hosseins: RayTPUExecutor -> _init_executor()
hosseins: RayTPUExecutor -> _init_workers_ray() ray_remote_kwargs={}
hosseins: ray_utils RayWorkerWrapper -> __init__() args=()
hosseins: ray_utils RayWorkerWrapper -> __init__() kwargs={'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')}
(RayWorkerWrapper pid=1644170) hosseins: ray_utils RayWorkerWrapper -> __init__() args=()
(RayWorkerWrapper pid=1644170) hosseins: ray_utils RayWorkerWrapper -> __init__() kwargs={'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7f5450baf8e0>, cache_config=<vllm.config.CacheConfig object at 0x7f5450baf970>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x7f5450bafb50>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7f5450bafbe0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')}
(RayWorkerWrapper pid=1644176) hosseins: ray_utils RayWorkerWrapper -> __init__() args=() [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)
(RayWorkerWrapper pid=1644176) hosseins: ray_utils RayWorkerWrapper -> __init__() kwargs={'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7c14314af8b0>, cache_config=<vllm.config.CacheConfig object at 0x7c14314af940>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x7c14314afb20>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7c14314afbb0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')} [repeated 3x across cluster]
(RayWorkerWrapper pid=1644180) hosseins: ray_utils RayWorkerWrapper -> __init__() args=() [repeated 3x across cluster]
(RayWorkerWrapper pid=1644180) hosseins: ray_utils RayWorkerWrapper -> __init__() kwargs={'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x78080999f8b0>, cache_config=<vllm.config.CacheConfig object at 0x78080999f940>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x78080999fb20>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x78080999fbb0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')} [repeated 3x across cluster]
hosseins: RayTPUExecutor -> _run_workers() all_args=[({'VLLM_TRACE_FUNCTION': '0'},), ({'VLLM_TRACE_FUNCTION': '0'},), ({'VLLM_TRACE_FUNCTION': '0'},), ({'VLLM_TRACE_FUNCTION': '0'},), ({'VLLM_TRACE_FUNCTION': '0'},), ({'VLLM_TRACE_FUNCTION': '0'},), ({'VLLM_TRACE_FUNCTION': '0'},), ({'VLLM_TRACE_FUNCTION': '0'},)]
hosseins: RayTPUExecutor -> _run_workers() all_kwargs=None
hosseins: RayTPUExecutor -> _run_workers() all_args=None
hosseins: RayTPUExecutor -> _run_workers() all_kwargs=[{'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af'), 'local_rank': 0, 'rank': 0, 'distributed_init_method': 'tcp://127.0.0.1:36451', 'is_driver_worker': True}, {'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af'), 'local_rank': 1, 'rank': 1, 'distributed_init_method': 'tcp://127.0.0.1:36451', 'is_driver_worker': False}, {'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af'), 'local_rank': 2, 'rank': 2, 'distributed_init_method': 'tcp://127.0.0.1:36451', 'is_driver_worker': False}, {'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af'), 'local_rank': 3, 'rank': 3, 'distributed_init_method': 'tcp://127.0.0.1:36451', 'is_driver_worker': False}, {'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af'), 'local_rank': 4, 'rank': 4, 'distributed_init_method': 'tcp://127.0.0.1:36451', 'is_driver_worker': False}, {'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af'), 'local_rank': 5, 'rank': 5, 'distributed_init_method': 'tcp://127.0.0.1:36451', 'is_driver_worker': False}, {'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af'), 'local_rank': 6, 'rank': 6, 'distributed_init_method': 'tcp://127.0.0.1:36451', 'is_driver_worker': False}, {'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af'), 'local_rank': 7, 'rank': 7, 'distributed_init_method': 'tcp://127.0.0.1:36451', 'is_driver_worker': False}]
(RayWorkerWrapper pid=1644170) hosseins: ray_utils RayWorkerWrapper -> get_node_and_gpu_ids() node_id='fed67d48475d141b155908ade7df8f6f22c7eec2ac519b7ec9c215be'
(RayWorkerWrapper pid=1644170) hosseins: ray_utils RayWorkerWrapper -> get_node_and_gpu_ids() gpu_ids=[]
(RayWorkerWrapper pid=1644175) INFO 01-18 04:05:03 __init__.py:179] Automatically detected platform tpu.
(RayWorkerWrapper pid=1644176) hosseins: linear.py is called
(RayWorkerWrapper pid=1644176) INFO 01-18 04:05:04 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.
(RayWorkerWrapper pid=1644176) INFO 01-18 04:05:04 selector.py:163] Using Pallas backend.
(RayWorkerWrapper pid=1644176) WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
hosseins: linear.py is called
INFO 01-18 04:05:04 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.
INFO 01-18 04:05:04 selector.py:163] Using Pallas backend.
hosseins: RayTPUExecutor -> _run_workers() all_args=None
hosseins: RayTPUExecutor -> _run_workers() all_kwargs=None
hosseins: parallel_state.py -> init_distributed_environment rank=0
hosseins: parallel_state.py -> init_distributed_environment distributed_init_method='tcp://127.0.0.1:36451'
hosseins: parallel_state.py -> init_distributed_environment local_rank=0
hosseins: parallel_state.py -> init_distributed_environment backend='gloo'
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_distributed_environment rank=1
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_distributed_environment distributed_init_method='tcp://127.0.0.1:36451'
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_distributed_environment local_rank=1
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_distributed_environment backend='gloo'
hosseins: parallel_state.py -> init_world_group ranks=[0, 1, 2, 3, 4, 5, 6, 7]
hosseins: parallel_state.py -> init_world_group backend='gloo'
hosseins: parallel_state.py -> init_world_group local_rank=0
hosseins: GroupCoordinator __init__() group_ranks=[[0, 1, 2, 3, 4, 5, 6, 7]]
hosseins: GroupCoordinator __init__() local_rank=0
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_world_group ranks=[0, 1, 2, 3, 4, 5, 6, 7]
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_world_group backend='gloo'
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_world_group local_rank=1
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator __init__() group_ranks=[[0, 1, 2, 3, 4, 5, 6, 7]]
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator __init__() local_rank=1
hosseins: GroupCoordinator -> __init__() self.ranks=[0, 1, 2, 3, 4, 5, 6, 7]
hosseins: GroupCoordinator -> __init__() self.world_size=8
hosseins: GroupCoordinator -> __init__() self.rank_in_group=0
hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x701ad41b6d70>
hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x701ac44891f0>
hosseins: parallel_state.py -> ensure_model_parallel_initialized tensor_model_parallel_size=8
hosseins: parallel_state.py -> ensure_model_parallel_initialized pipeline_model_parallel_size=1
hosseins: parallel_state.py -> ensure_model_parallel_initialized backend=None
hosseins: parallel_state.py -> initialize_model_parallel tensor_model_parallel_size=8
hosseins: parallel_state.py -> initialize_model_parallel pipeline_model_parallel_size=1
hosseins: parallel_state.py -> initialize_model_parallel backend='gloo'
hosseins: parallel_state.py -> init_model_parallel_group group_ranks=[[0, 1, 2, 3, 4, 5, 6, 7]]
hosseins: parallel_state.py -> init_model_parallel_group local_rank=0
hosseins: parallel_state.py -> init_model_parallel_group backend='gloo'
hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=None
hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=True
hosseins: parallel_state.py -> init_model_parallel_group group_name='tp'
hosseins: GroupCoordinator __init__() group_ranks=[[0, 1, 2, 3, 4, 5, 6, 7]]
hosseins: GroupCoordinator __init__() local_rank=0
hosseins: GroupCoordinator -> __init__() self.ranks=[0, 1, 2, 3, 4, 5, 6, 7]
hosseins: GroupCoordinator -> __init__() self.world_size=8
hosseins: GroupCoordinator -> __init__() self.rank_in_group=0
hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x701ac445b670>
hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x701ac445b870>
hosseins: TpuCommunicator __init__() group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x701ac445b870>
hosseins: TpuCommunicator __init__() global_rank=0
hosseins: TpuCommunicator __init__() global_world_size=8
hosseins: ray_utils get_num_tpu_nodes total_tpus=8
hosseins: ray_utils get_num_tpu_nodes tpus_per_node=8
hosseins: ray_utils get_num_tpu_nodes num_tpu_nodes=1
hosseins: TpuCommunicator __init__() num_nodes=1
hosseins: ray_utils get_num_nodes_in_placement_group num_nodes=0
hosseins: TpuCommunicator __init__() num_nodes_in_pg=0
hosseins: TpuCommunicator __init__() local_world_size=8
hosseins: TpuCommunicator __init__() local_rank=0
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.ranks=[0, 1, 2, 3, 4, 5, 6, 7]
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.world_size=8
hosseins: parallel_state.py -> in_the_same_node_as pg=<torch.distributed.distributed_c10d.ProcessGroup object at 0x701ac445b870>
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.rank_in_group=1
hosseins: parallel_state.py -> in_the_same_node_as source_rank=0
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af8ecb980b0>
hosseins: parallel_state -> in_the_same_node_as source_rank=0
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af902e0c8b0>
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> ensure_model_parallel_initialized tensor_model_parallel_size=8
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> ensure_model_parallel_initialized pipeline_model_parallel_size=1
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> ensure_model_parallel_initialized backend=None
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> initialize_model_parallel tensor_model_parallel_size=8
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> initialize_model_parallel pipeline_model_parallel_size=1
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> initialize_model_parallel backend='gloo'
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_model_parallel_group group_ranks=[[0, 1, 2, 3, 4, 5, 6, 7]]
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_model_parallel_group local_rank=1
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_model_parallel_group backend='gloo'
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=None
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=True
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_model_parallel_group group_name='tp'
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.ranks=[0, 1, 2, 3, 4, 5, 6, 7]
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.world_size=8
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.rank_in_group=1
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af8ecb395f0>
(RayWorkerWrapper pid=1644175) hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af8ecb39ab0>
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator __init__() group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af8ecb39ab0>
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator __init__() global_rank=1
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator __init__() global_world_size=8
(RayWorkerWrapper pid=1644175) hosseins: ray_utils get_num_tpu_nodes total_tpus=8
(RayWorkerWrapper pid=1644175) hosseins: ray_utils get_num_tpu_nodes tpus_per_node=8
(RayWorkerWrapper pid=1644175) hosseins: ray_utils get_num_tpu_nodes num_tpu_nodes=1
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator __init__() num_nodes=1
(RayWorkerWrapper pid=1644175) hosseins: ray_utils get_num_nodes_in_placement_group num_nodes=1
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator __init__() num_nodes_in_pg=1
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator __init__() local_world_size=8
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator __init__() local_rank=1
(RayWorkerWrapper pid=1644172) hosseins: ray_utils RayWorkerWrapper -> __init__() args=()
(RayWorkerWrapper pid=1644172) hosseins: ray_utils RayWorkerWrapper -> __init__() kwargs={'vllm_config': VllmConfig(model_config=<vllm.config.ModelConfig object at 0x76985eda7910>, cache_config=<vllm.config.CacheConfig object at 0x76985eda79a0>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x76985eda7b80>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x76985eda7c10>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')}
(RayWorkerWrapper pid=1644172) hosseins: ray_utils RayWorkerWrapper -> get_node_and_gpu_ids() node_id='fed67d48475d141b155908ade7df8f6f22c7eec2ac519b7ec9c215be' [repeated 7x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: ray_utils RayWorkerWrapper -> get_node_and_gpu_ids() gpu_ids=[] [repeated 7x across cluster]
(RayWorkerWrapper pid=1644172) INFO 01-18 04:05:03 __init__.py:179] Automatically detected platform tpu. [repeated 6x across cluster]
(RayWorkerWrapper pid=1644180) hosseins: linear.py is called [repeated 6x across cluster]
(RayWorkerWrapper pid=1644180) INFO 01-18 04:05:04 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU. [repeated 6x across cluster]
(RayWorkerWrapper pid=1644180) INFO 01-18 04:05:04 selector.py:163] Using Pallas backend. [repeated 6x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_distributed_environment local_rank=7 [repeated 18x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_distributed_environment backend='gloo' [repeated 6x across cluster]
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> init_world_group local_rank=3 [repeated 4x across cluster]
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> init_world_group backend='gloo' [repeated 2x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: GroupCoordinator __init__() local_rank=2 [repeated 8x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: GroupCoordinator -> __init__() self.rank_in_group=2 [repeated 6x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7cdafd7e90f0> [repeated 4x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> ensure_model_parallel_initialized pipeline_model_parallel_size=1 [repeated 2x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> ensure_model_parallel_initialized backend=None
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> initialize_model_parallel pipeline_model_parallel_size=1 [repeated 2x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> initialize_model_parallel backend='gloo'
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> init_model_parallel_group local_rank=2 [repeated 2x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> init_model_parallel_group backend='gloo'
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=None
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=True
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> init_model_parallel_group group_name='tp'
(RayWorkerWrapper pid=1644178) hosseins: TpuCommunicator __init__() group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7cdafd7e90f0>
(RayWorkerWrapper pid=1644178) hosseins: TpuCommunicator __init__() local_rank=2 [repeated 6x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: ray_utils get_num_tpu_nodes num_tpu_nodes=1 [repeated 3x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: ray_utils get_num_nodes_in_placement_group num_nodes=1
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> ensure_model_parallel_initialized backend=None
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> initialize_model_parallel backend='gloo'
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> init_model_parallel_group backend='gloo'
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=None
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=True
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> init_model_parallel_group group_name='tp'
(RayWorkerWrapper pid=1644176) hosseins: TpuCommunicator __init__() group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7c141a7b5730>
(RayWorkerWrapper pid=1644176) hosseins: ray_utils get_num_nodes_in_placement_group num_nodes=1
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> in_the_same_node_as pg=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7cdafd7e90f0>
(RayWorkerWrapper pid=1644178) hosseins: parallel_state.py -> in_the_same_node_as source_rank=0
(RayWorkerWrapper pid=1644178) hosseins: parallel_state -> in_the_same_node_as source_rank=0
INFO 01-18 04:05:21 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_915f7ae6'), local_subscribe_port=58237, remote_subscribe_port=None)
hosseins: parallel_state.py -> init_model_parallel_group group_ranks=[[0], [1], [2], [3], [4], [5], [6], [7]]
hosseins: parallel_state.py -> init_model_parallel_group local_rank=0
hosseins: parallel_state.py -> init_model_parallel_group backend='gloo'
hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=False
hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=False
hosseins: parallel_state.py -> init_model_parallel_group group_name='pp'
hosseins: GroupCoordinator __init__() group_ranks=[[0], [1], [2], [3], [4], [5], [6], [7]]
hosseins: GroupCoordinator __init__() local_rank=0
hosseins: GroupCoordinator -> __init__() self.ranks=[0]
hosseins: GroupCoordinator -> __init__() self.world_size=1
hosseins: GroupCoordinator -> __init__() self.rank_in_group=0
hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x701ac449eaf0>
hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x701ac449f4f0>
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=False
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=False
(RayWorkerWrapper pid=1644175) hosseins: parallel_state.py -> init_model_parallel_group group_name='pp'
hosseins: RayTPUExecutor -> _run_workers() all_args=None
hosseins: RayTPUExecutor -> _run_workers() all_kwargs=None
hosseins: LlamaForCausalLM -> __init__() vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')
hosseins: LlamaForCausalLM -> _init_model - vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')
hossein: LlamaModel -> __init__ : [vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x701a2078f4c0>, cache_config=<vllm.config.CacheConfig object at 0x701a85da6020>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x701ad4fa7fd0>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x701aeefd7df0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')]
hosseins: utils.py -> make_layers() [num_hidden_layers=32]
hosseins: utils.py -> make_layers() [layer_fn=<function LlamaModel.__init__.<locals>.<lambda> at 0x701ac4001a20>]
hosseins: utils.py -> make_layers() [prefix='model.layers']
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=0]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=1]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=2]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=3]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=4]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> __init__() vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7af902e14460>, cache_config=<vllm.config.CacheConfig object at 0x7af902e14430>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x7af902e14340>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=1), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7af902e14370>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> _init_model - vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7af902e14460>, cache_config=<vllm.config.CacheConfig object at 0x7af902e14430>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x7af902e14340>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=1), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7af902e14370>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')
hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hossein: LlamaModel -> __init__ : [vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7af902e14460>, cache_config=<vllm.config.CacheConfig object at 0x7af902e14430>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x7af902e14340>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=1), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7af902e14370>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')]
hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: utils.py -> make_layers() [num_hidden_layers=32]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644175) hosseins: utils.py -> make_layers() [layer_fn=<function LlamaModel.__init__.<locals>.<lambda> at 0x7af8b0f19b40>]
(RayWorkerWrapper pid=1644175) hosseins: utils.py -> make_layers() [prefix='model.layers']
(RayWorkerWrapper pid=1644175) hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
(RayWorkerWrapper pid=1644175)   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
(RayWorkerWrapper pid=1644175)   "architectures": [
(RayWorkerWrapper pid=1644175)     "LlamaForCausalLM"
(RayWorkerWrapper pid=1644175)   ],
(RayWorkerWrapper pid=1644175)   "attention_bias": false,
(RayWorkerWrapper pid=1644175)   "attention_dropout": 0.0,
(RayWorkerWrapper pid=1644175)   "bos_token_id": 128000,
(RayWorkerWrapper pid=1644175)   "eos_token_id": 128001,
(RayWorkerWrapper pid=1644175)   "head_dim": 128,
(RayWorkerWrapper pid=1644175)   "hidden_act": "silu",
(RayWorkerWrapper pid=1644175)   "hidden_size": 4096,
(RayWorkerWrapper pid=1644175)   "initializer_range": 0.02,
(RayWorkerWrapper pid=1644175)   "intermediate_size": 14336,
hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175)   "max_position_embeddings": 131072,
hosseins: LinearBase -> __init__() [input_size=14336]
(RayWorkerWrapper pid=1644175)   "mlp_bias": false,
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175)   "model_type": "llama",
(RayWorkerWrapper pid=1644175)   "num_attention_heads": 32,
(RayWorkerWrapper pid=1644175)   "num_hidden_layers": 32,
(RayWorkerWrapper pid=1644175)   "num_key_value_heads": 8,
(RayWorkerWrapper pid=1644175)   "pretraining_tp": 1,
(RayWorkerWrapper pid=1644175)   "rms_norm_eps": 1e-05,
(RayWorkerWrapper pid=1644175)   "rope_scaling": {
(RayWorkerWrapper pid=1644175)     "factor": 8.0,
(RayWorkerWrapper pid=1644175)     "high_freq_factor": 4.0,
(RayWorkerWrapper pid=1644175)     "low_freq_factor": 1.0,
(RayWorkerWrapper pid=1644175)     "original_max_position_embeddings": 8192,
(RayWorkerWrapper pid=1644175)     "rope_type": "llama3"
(RayWorkerWrapper pid=1644175)   },
(RayWorkerWrapper pid=1644175)   "rope_theta": 500000.0,
(RayWorkerWrapper pid=1644175)   "tie_word_embeddings": false,
(RayWorkerWrapper pid=1644175)   "torch_dtype": "bfloat16",
(RayWorkerWrapper pid=1644175)   "transformers_version": "4.47.0",
(RayWorkerWrapper pid=1644175)   "use_cache": true,
(RayWorkerWrapper pid=1644175)   "vocab_size": 128256
(RayWorkerWrapper pid=1644175) }
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644175) hosseins: LlamaAttention -> __init__() : [layer_idx=0]
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> __init__: hidden_size=4096
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LlamaAttention -> __init__() : [layer_idx=5]
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
(RayWorkerWrapper pid=1644175) hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> __init__()
hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
(RayWorkerWrapper pid=1644175)   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
(RayWorkerWrapper pid=1644175)   "architectures": [
(RayWorkerWrapper pid=1644175)     "LlamaForCausalLM"
(RayWorkerWrapper pid=1644175)   ],
(RayWorkerWrapper pid=1644175)   "attention_bias": false,
(RayWorkerWrapper pid=1644175)   "attention_dropout": 0.0,
(RayWorkerWrapper pid=1644175)   "bos_token_id": 128000,
(RayWorkerWrapper pid=1644175)   "eos_token_id": 128001,
(RayWorkerWrapper pid=1644175)   "head_dim": 128,
(RayWorkerWrapper pid=1644175)   "hidden_act": "silu",
(RayWorkerWrapper pid=1644175)   "hidden_size": 4096,
(RayWorkerWrapper pid=1644175)   "initializer_range": 0.02,
(RayWorkerWrapper pid=1644175)   "intermediate_size": 14336,
(RayWorkerWrapper pid=1644175)   "max_position_embeddings": 131072,
(RayWorkerWrapper pid=1644175)   "mlp_bias": false,
(RayWorkerWrapper pid=1644175)   "model_type": "llama",
hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175)   "num_attention_heads": 32,
hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175)   "num_hidden_layers": 32,
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175)   "num_key_value_heads": 8,
(RayWorkerWrapper pid=1644175)   "pretraining_tp": 1,
(RayWorkerWrapper pid=1644175)   "rms_norm_eps": 1e-05,
(RayWorkerWrapper pid=1644175)   "rope_scaling": {
(RayWorkerWrapper pid=1644175)     "factor": 8.0,
(RayWorkerWrapper pid=1644175)     "high_freq_factor": 4.0,
(RayWorkerWrapper pid=1644175)     "low_freq_factor": 1.0,
(RayWorkerWrapper pid=1644175)     "original_max_position_embeddings": 8192,
(RayWorkerWrapper pid=1644175)     "rope_type": "llama3"
(RayWorkerWrapper pid=1644175)   },
(RayWorkerWrapper pid=1644175)   "rope_theta": 500000.0,
(RayWorkerWrapper pid=1644175)   "tie_word_embeddings": false,
(RayWorkerWrapper pid=1644175)   "torch_dtype": "bfloat16",
(RayWorkerWrapper pid=1644175)   "transformers_version": "4.47.0",
(RayWorkerWrapper pid=1644175)   "use_cache": true,
(RayWorkerWrapper pid=1644175)   "vocab_size": 128256
(RayWorkerWrapper pid=1644175) }
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644175) hosseins: LlamaAttention -> __init__() : [layer_idx=1]
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> __init__: hidden_size=4096
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=14336]
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
(RayWorkerWrapper pid=1644175)   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
(RayWorkerWrapper pid=1644175)   "architectures": [
(RayWorkerWrapper pid=1644175)     "LlamaForCausalLM"
(RayWorkerWrapper pid=1644175)   ],
(RayWorkerWrapper pid=1644175)   "attention_bias": false,
(RayWorkerWrapper pid=1644175)   "attention_dropout": 0.0,
(RayWorkerWrapper pid=1644175)   "bos_token_id": 128000,
(RayWorkerWrapper pid=1644175)   "eos_token_id": 128001,
(RayWorkerWrapper pid=1644175)   "head_dim": 128,
(RayWorkerWrapper pid=1644175)   "hidden_act": "silu",
(RayWorkerWrapper pid=1644175)   "hidden_size": 4096,
(RayWorkerWrapper pid=1644175)   "initializer_range": 0.02,
(RayWorkerWrapper pid=1644175)   "intermediate_size": 14336,
(RayWorkerWrapper pid=1644175)   "max_position_embeddings": 131072,
(RayWorkerWrapper pid=1644175)   "mlp_bias": false,
(RayWorkerWrapper pid=1644175)   "model_type": "llama",
(RayWorkerWrapper pid=1644175)   "num_attention_heads": 32,
(RayWorkerWrapper pid=1644175)   "num_hidden_layers": 32,
(RayWorkerWrapper pid=1644175)   "num_key_value_heads": 8,
(RayWorkerWrapper pid=1644175)   "pretraining_tp": 1,
(RayWorkerWrapper pid=1644175)   "rms_norm_eps": 1e-05,
(RayWorkerWrapper pid=1644175)   "rope_scaling": {
(RayWorkerWrapper pid=1644175)     "factor": 8.0,
(RayWorkerWrapper pid=1644175)     "high_freq_factor": 4.0,
(RayWorkerWrapper pid=1644175)     "low_freq_factor": 1.0,
(RayWorkerWrapper pid=1644175)     "original_max_position_embeddings": 8192,
(RayWorkerWrapper pid=1644175)     "rope_type": "llama3"
(RayWorkerWrapper pid=1644175)   },
(RayWorkerWrapper pid=1644175)   "rope_theta": 500000.0,
(RayWorkerWrapper pid=1644175)   "tie_word_embeddings": false,
(RayWorkerWrapper pid=1644175)   "torch_dtype": "bfloat16",
(RayWorkerWrapper pid=1644175)   "transformers_version": "4.47.0",
(RayWorkerWrapper pid=1644175)   "use_cache": true,
(RayWorkerWrapper pid=1644175)   "vocab_size": 128256
(RayWorkerWrapper pid=1644175) }
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644175) hosseins: LlamaAttention -> __init__() : [layer_idx=2]
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> __init__: hidden_size=4096
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=14336]
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
(RayWorkerWrapper pid=1644175)   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
(RayWorkerWrapper pid=1644175)   "architectures": [
(RayWorkerWrapper pid=1644175)     "LlamaForCausalLM"
(RayWorkerWrapper pid=1644175)   ],
(RayWorkerWrapper pid=1644175)   "attention_bias": false,
(RayWorkerWrapper pid=1644175)   "attention_dropout": 0.0,
(RayWorkerWrapper pid=1644175)   "bos_token_id": 128000,
(RayWorkerWrapper pid=1644175)   "eos_token_id": 128001,
(RayWorkerWrapper pid=1644175)   "head_dim": 128,
(RayWorkerWrapper pid=1644175)   "hidden_act": "silu",
(RayWorkerWrapper pid=1644175)   "hidden_size": 4096,
(RayWorkerWrapper pid=1644175)   "initializer_range": 0.02,
(RayWorkerWrapper pid=1644175)   "intermediate_size": 14336,
(RayWorkerWrapper pid=1644175)   "max_position_embeddings": 131072,
(RayWorkerWrapper pid=1644175)   "mlp_bias": false,
(RayWorkerWrapper pid=1644175)   "model_type": "llama",
(RayWorkerWrapper pid=1644175)   "num_attention_heads": 32,
(RayWorkerWrapper pid=1644175)   "num_hidden_layers": 32,
(RayWorkerWrapper pid=1644175)   "num_key_value_heads": 8,
(RayWorkerWrapper pid=1644175)   "pretraining_tp": 1,
(RayWorkerWrapper pid=1644175)   "rms_norm_eps": 1e-05,
(RayWorkerWrapper pid=1644175)   "rope_scaling": {
(RayWorkerWrapper pid=1644175)     "factor": 8.0,
(RayWorkerWrapper pid=1644175)     "high_freq_factor": 4.0,
(RayWorkerWrapper pid=1644175)     "low_freq_factor": 1.0,
(RayWorkerWrapper pid=1644175)     "original_max_position_embeddings": 8192,
(RayWorkerWrapper pid=1644175)     "rope_type": "llama3"
(RayWorkerWrapper pid=1644175)   },
(RayWorkerWrapper pid=1644175)   "rope_theta": 500000.0,
(RayWorkerWrapper pid=1644175)   "tie_word_embeddings": false,
(RayWorkerWrapper pid=1644175)   "torch_dtype": "bfloat16",
(RayWorkerWrapper pid=1644175)   "transformers_version": "4.47.0",
(RayWorkerWrapper pid=1644175)   "use_cache": true,
(RayWorkerWrapper pid=1644175)   "vocab_size": 128256
(RayWorkerWrapper pid=1644175) }
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644175) hosseins: LlamaAttention -> __init__() : [layer_idx=3]
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> __init__: hidden_size=4096
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644175) hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644178) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=6]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644175) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

(RayWorkerWrapper pid=1644178) 
hosseins: LlamaAttention -> __init__() : [layer_idx=7]
(RayWorkerWrapper pid=1644171) 
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
(RayWorkerWrapper pid=1644180) 
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644178) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644176) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
(RayWorkerWrapper pid=1644181) 
hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644180) 
hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644172) 
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

(RayWorkerWrapper pid=1644176) 
hosseins: LlamaAttention -> __init__() : [layer_idx=8]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644172) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644175) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644178) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

(RayWorkerWrapper pid=1644176) 
hosseins: LlamaAttention -> __init__() : [layer_idx=9]
(RayWorkerWrapper pid=1644181) 
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644180) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644175) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644178) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

(RayWorkerWrapper pid=1644171) 
hosseins: LlamaAttention -> __init__() : [layer_idx=10]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=11]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644171) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644180) 
hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644172) 
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=12]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644175) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644176) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644180) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=13]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644175) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644172) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=14]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644180) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644178) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

(RayWorkerWrapper pid=1644176) 
hosseins: LlamaAttention -> __init__() : [layer_idx=15]
(RayWorkerWrapper pid=1644181) 
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
(RayWorkerWrapper pid=1644171) 
hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644180) 
hosseins: ColumnParallelLinear -> __init__()
(RayWorkerWrapper pid=1644172) 
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644175) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644176) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644180) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=16]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
(RayWorkerWrapper pid=1644178) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644171) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
(RayWorkerWrapper pid=1644180) 
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644178) 
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644171) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

(RayWorkerWrapper pid=1644180) 
hosseins: LlamaAttention -> __init__() : [layer_idx=17]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644176) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
(RayWorkerWrapper pid=1644181) 
hosseins: LinearBase -> __init__() [input_size=4096]
(RayWorkerWrapper pid=1644172) 
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=18]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644175) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644172) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=19]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644180) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644176) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=20]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644175) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644180) 
(RayWorkerWrapper pid=1644172) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=21]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
(RayWorkerWrapper pid=1644176) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644171) 
(RayWorkerWrapper pid=1644180) hosseins: LlamaForCausalLM -> load_weights
(RayWorkerWrapper pid=1644172) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644175) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
(RayWorkerWrapper pid=1644176) 
hosseins: LinearBase -> __init__() [input_size=14336]
(RayWorkerWrapper pid=1644181) 
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=22]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644175) 
(RayWorkerWrapper pid=1644178) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644171) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644172) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=23]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644178) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
(RayWorkerWrapper pid=1644171) 
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
(RayWorkerWrapper pid=1644172) 
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644171) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=24]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644172) 
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
(RayWorkerWrapper pid=1644172) 
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=25]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644172) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=26]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
(RayWorkerWrapper pid=1644172) 
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=27]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=28]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=29]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=30]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=31]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hossein: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=8
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: LlamaForCausalLM -> load_weights
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644172) 
(RayWorkerWrapper pid=1644175) INFO 01-18 04:05:22 weight_utils.py:251] Using model weights format ['*.safetensors']
INFO 01-18 04:05:22 weight_utils.py:251] Using model weights format ['*.safetensors']
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='lm_head.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaModel -> load_weights
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.norm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.input_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.down_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.post_attention_layernorm.weight'
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.q_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.v_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.gate_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.up_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.k_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.o_proj.weight'
(RayWorkerWrapper pid=1644175) hosseins: RowParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.embed_tokens.weight'
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_world_group local_rank=7 [repeated 8x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_world_group backend='gloo' [repeated 4x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: GroupCoordinator __init__() local_rank=7 [repeated 32x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: GroupCoordinator -> __init__() self.rank_in_group=0 [repeated 51x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x76983b591cb0> [repeated 34x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> ensure_model_parallel_initialized pipeline_model_parallel_size=1 [repeated 10x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> initialize_model_parallel pipeline_model_parallel_size=1 [repeated 10x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_model_parallel_group local_rank=7 [repeated 24x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: TpuCommunicator __init__() local_rank=7 [repeated 30x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: ray_utils get_num_tpu_nodes num_tpu_nodes=1 [repeated 15x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> ensure_model_parallel_initialized backend=None [repeated 4x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> initialize_model_parallel backend='gloo' [repeated 4x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_model_parallel_group backend='gloo' [repeated 11x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=None [repeated 4x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=True [repeated 4x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_model_parallel_group group_name='tp' [repeated 4x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: TpuCommunicator __init__() group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x76983b5b0ef0> [repeated 4x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: ray_utils get_num_nodes_in_placement_group num_nodes=1 [repeated 4x across cluster]
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> in_the_same_node_as pg=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7c141a7b5730> [repeated 6x across cluster]
(RayWorkerWrapper pid=1644176) hosseins: parallel_state.py -> in_the_same_node_as source_rank=0 [repeated 6x across cluster]
(RayWorkerWrapper pid=1644176) hosseins: parallel_state -> in_the_same_node_as source_rank=0 [repeated 6x across cluster]
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='lm_head.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.input_layernorm.weight'
hosseins: LlamaModel -> load_weights
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.norm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.input_layernorm.weight'
(RayWorkerWrapper pid=1644172) 
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.80it/s]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.embed_tokens.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=False [repeated 6x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=False [repeated 6x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: parallel_state.py -> init_model_parallel_group group_name='pp' [repeated 6x across cluster]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644172) hosseins: LlamaForCausalLM -> __init__() vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x76985ea385e0>, cache_config=<vllm.config.CacheConfig object at 0x76985ea38550>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x76985ea38520>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=7), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x76985ea382b0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af') [repeated 6x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: LlamaForCausalLM -> _init_model - vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x76985ea385e0>, cache_config=<vllm.config.CacheConfig object at 0x76985ea38550>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x76985ea38520>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=7), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x76985ea382b0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af') [repeated 6x across cluster]
(RayWorkerWrapper pid=1644172) hossein: LlamaModel -> __init__ : [vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x76985ea385e0>, cache_config=<vllm.config.CacheConfig object at 0x76985ea38550>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=8, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=<ray.util.placement_group.PlacementGroup object at 0x76985ea38520>, distributed_executor_backend='ray', worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=8, rank=7), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x76985ea382b0>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='b03af')] [repeated 6x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: utils.py -> make_layers() [num_hidden_layers=32] [repeated 6x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: utils.py -> make_layers() [layer_fn=<function LlamaModel.__init__.<locals>.<lambda> at 0x769811d1db40>] [repeated 6x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: utils.py -> make_layers() [prefix='model.layers'] [repeated 6x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig { [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B", [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "architectures": [ [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)     "LlamaForCausalLM" [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   ], [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "attention_bias": false, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "attention_dropout": 0.0, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "bos_token_id": 128000, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "eos_token_id": 128001, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "head_dim": 128, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "hidden_act": "silu", [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "hidden_size": 4096, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "initializer_range": 0.02, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "intermediate_size": 14336, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "max_position_embeddings": 131072, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "mlp_bias": false, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "model_type": "llama", [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "num_attention_heads": 32, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "num_hidden_layers": 32, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "num_key_value_heads": 8, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "pretraining_tp": 1, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "rms_norm_eps": 1e-05, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "rope_scaling": { [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)     "factor": 8.0, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)     "high_freq_factor": 4.0, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)     "low_freq_factor": 1.0, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)     "original_max_position_embeddings": 8192, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)     "rope_type": "llama3" [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   }, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "rope_theta": 500000.0, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "tie_word_embeddings": false, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "torch_dtype": "bfloat16", [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "transformers_version": "4.47.0", [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "use_cache": true, [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172)   "vocab_size": 128256 [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172) } [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: LlamaAttention -> __init__() : [layer_idx=31] [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: QKVParallelLinear -> __init__: hidden_size=4096 [repeated 220x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: LinearBase -> __init__() [input_size=14336] [repeated 882x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: ColumnParallelLinear -> __init__() [repeated 441x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096] [repeated 882x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: RowParallelLinear -> __init__() [repeated 441x across cluster]
(RayWorkerWrapper pid=1644172) hossein: LlamaMLP -> __init__ : [hidden_size=4096] [repeated 221x across cluster]
(RayWorkerWrapper pid=1644172) hosseins: MergedColumnParallelLinear -> __init__: tp_size=8 [repeated 221x across cluster]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644172) hosseins: LlamaForCausalLM -> load_weights [repeated 6x across cluster]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.50it/s]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
(RayWorkerWrapper pid=1644178) INFO 01-18 04:05:22 weight_utils.py:251] Using model weights format ['*.safetensors'] [repeated 6x across cluster]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
(RayWorkerWrapper pid=1644181) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='lm_head.weight' [repeated 6x across cluster]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.input_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.mlp.down_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.mlp.gate_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.mlp.up_proj.weight'
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.post_attention_layernorm.weight'
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.k_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.o_proj.weight'
hosseins: RowParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.q_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.v_proj.weight'
hosseins: QKVParallelLinear -> weight_loader()
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.33it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.42it/s]

hosseins: LLMEngine -> _initialize_kv_caches()
hosseins: RayTPUExecutor -> determine_num_available_blocks()
hosseins: RayTPUExecutor -> _run_workers() all_args=None
hosseins: RayTPUExecutor -> _run_workers() all_kwargs=None
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644175) hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator all_reduce()
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644181) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.v_proj.weight' [repeated 1915x across cluster]
(RayWorkerWrapper pid=1644181) hosseins: LlamaModel -> load_weights [repeated 6x across cluster]
(RayWorkerWrapper pid=1644181) hosseins: RowParallelLinear -> weight_loader() [repeated 425x across cluster]
(RayWorkerWrapper pid=1644181) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.norm.weight' [repeated 6x across cluster]
(RayWorkerWrapper pid=1644181) hosseins: MergedColumnParallelLinear -> weight_loader [repeated 425x across cluster]
(RayWorkerWrapper pid=1644181) hosseins: QKVParallelLinear -> weight_loader() [repeated 639x across cluster]
(RayWorkerWrapper pid=1644181) hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.embed_tokens.weight' [repeated 6x across cluster]
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
(RayWorkerWrapper pid=1644176) hosseins: LlamaForCausalLM -> compute_logits
(RayWorkerWrapper pid=1644176) hosseins: TpuCommunicator all_gather() dim=-1
hosseins: RayTPUExecutor -> initialize_cache() num_gpu_blocks=106176
hosseins: RayTPUExecutor -> initialize_cache() num_cpu_blocks=65536
INFO 01-18 04:05:28 ray_tpu_executor.py:286] # TPU blocks: 106176, # CPU blocks: 65536
hosseins: RayTPUExecutor -> _run_workers() all_args=None
hosseins: RayTPUExecutor -> _run_workers() all_kwargs=None
INFO 01-18 04:05:30 tpu_model_runner.py:281] Compiling the model with different input shapes...
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:30 tpu_model_runner.py:291] batch_size: 1, seq_len: 16
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:30 tpu_model_runner.py:291] batch_size: 1, seq_len: 32
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:30 tpu_model_runner.py:291] batch_size: 1, seq_len: 64
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:30 tpu_model_runner.py:291] batch_size: 1, seq_len: 128
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:30 tpu_model_runner.py:291] batch_size: 1, seq_len: 256
INFO 01-18 04:05:30 tpu_model_runner.py:298] Compilation for prefill done in 0.78 s.
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:31 tpu_model_runner.py:334] batch_size: 8, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=16]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:32 tpu_model_runner.py:334] batch_size: 16, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=32]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:33 tpu_model_runner.py:334] batch_size: 32, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=48]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:34 tpu_model_runner.py:334] batch_size: 48, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=64]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644180) INFO 01-18 04:05:34 tpu_model_runner.py:281] Compiling the model with different input shapes...
(RayWorkerWrapper pid=1644180) hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1] [repeated 7x across cluster]
(RayWorkerWrapper pid=1644180) hosseins: TpuCommunicator all_reduce() [repeated 486x across cluster]
(RayWorkerWrapper pid=1644180) hosseins: UnquantizedLinearMethod -> apply() [repeated 957x across cluster]
(RayWorkerWrapper pid=1644180) hosseins: LlamaForCausalLM -> compute_logits [repeated 6x across cluster]
(RayWorkerWrapper pid=1644180) hosseins: TpuCommunicator all_gather() dim=-1 [repeated 6x across cluster]
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:34 tpu_model_runner.py:334] batch_size: 64, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=80]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644180) INFO 01-18 04:05:34 tpu_model_runner.py:291] batch_size: 1, seq_len: 16
(RayWorkerWrapper pid=1644180) INFO 01-18 04:05:34 tpu_model_runner.py:291] batch_size: 1, seq_len: 32
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
(RayWorkerWrapper pid=1644181) 
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644180) INFO 01-18 04:05:35 tpu_model_runner.py:298] Compilation for prefill done in 0.76 s.
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:35 tpu_model_runner.py:334] batch_size: 80, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=96]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644171) 
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:36 tpu_model_runner.py:334] batch_size: 96, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=112]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644172) 
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:37 tpu_model_runner.py:334] batch_size: 112, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=128]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644181) 
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:38 tpu_model_runner.py:334] batch_size: 128, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=144]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:38 tpu_model_runner.py:334] batch_size: 144, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=160]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644172) 
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644172) INFO 01-18 04:05:35 tpu_model_runner.py:281] Compiling the model with different input shapes... [repeated 6x across cluster]
(RayWorkerWrapper pid=1644175) hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=80] [repeated 71x across cluster]
(RayWorkerWrapper pid=1644181) hosseins: TpuCommunicator all_reduce() [repeated 4242x across cluster]
(RayWorkerWrapper pid=1644181) hosseins: UnquantizedLinearMethod -> apply() [repeated 8349x across cluster]
(RayWorkerWrapper pid=1644175) hosseins: LlamaForCausalLM -> compute_logits [repeated 65x across cluster]
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator all_gather() dim=-1 [repeated 65x across cluster]
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644175) INFO 01-18 04:05:39 tpu_model_runner.py:334] batch_size: 64, seq_len: 1 [repeated 63x across cluster]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:39 tpu_model_runner.py:334] batch_size: 160, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=176]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644172) INFO 01-18 04:05:36 tpu_model_runner.py:298] Compilation for prefill done in 0.76 s. [repeated 6x across cluster]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:40 tpu_model_runner.py:334] batch_size: 176, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=192]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:41 tpu_model_runner.py:334] batch_size: 192, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=208]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:42 tpu_model_runner.py:334] batch_size: 208, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=224]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644176) 
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:43 tpu_model_runner.py:334] batch_size: 224, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=240]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:44 tpu_model_runner.py:334] batch_size: 240, seq_len: 1
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=256]
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644180) 
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644176) 
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
(RayWorkerWrapper pid=1644176) hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=176] [repeated 46x across cluster]
(RayWorkerWrapper pid=1644175) hosseins: TpuCommunicator all_reduce() [repeated 3015x across cluster]
(RayWorkerWrapper pid=1644175) hosseins: UnquantizedLinearMethod -> apply() [repeated 5936x across cluster]
(RayWorkerWrapper pid=1644176) hosseins: LlamaForCausalLM -> compute_logits [repeated 46x across cluster]
(RayWorkerWrapper pid=1644176) hosseins: TpuCommunicator all_gather() dim=-1 [repeated 46x across cluster]
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: UnquantizedLinearMethod -> apply()
hosseins: TpuCommunicator all_reduce()
hosseins: LlamaForCausalLM -> compute_logits
hosseins: TpuCommunicator all_gather() dim=-1
INFO 01-18 04:05:44 tpu_model_runner.py:334] batch_size: 256, seq_len: 1
INFO 01-18 04:05:44 tpu_model_runner.py:341] Compilation for decode done in 14.04 s.
(RayWorkerWrapper pid=1644176) INFO 01-18 04:05:44 tpu_model_runner.py:334] batch_size: 160, seq_len: 1 [repeated 46x across cluster]
(RayWorkerWrapper pid=1644181) 
(RayWorkerWrapper pid=1644180) INFO 01-18 04:05:48 tpu_model_runner.py:341] Compilation for decode done in 13.18 s.
(RayWorkerWrapper pid=1644172) hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=256] [repeated 36x across cluster]
(RayWorkerWrapper pid=1644171) hosseins: TpuCommunicator all_reduce() [repeated 2635x across cluster]
(RayWorkerWrapper pid=1644171) hosseins: UnquantizedLinearMethod -> apply() [repeated 5195x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: LlamaForCausalLM -> compute_logits [repeated 41x across cluster]
(RayWorkerWrapper pid=1644178) hosseins: TpuCommunicator all_gather() dim=-1 [repeated 41x across cluster]
(RayWorkerWrapper pid=1644178) INFO 01-18 04:05:49 tpu_model_runner.py:334] batch_size: 256, seq_len: 1 [repeated 41x across cluster]
INFO 01-18 04:05:50 llm_engine.py:434] init engine (profile, create kv cache, warmup model) took 22.31 seconds
hosseins: init_app_state
INFO 01-18 04:05:53 api_server.py:697] Using supplied chat template:
INFO 01-18 04:05:53 api_server.py:697] None
INFO 01-18 04:05:53 launcher.py:19] Available routes are:
INFO 01-18 04:05:53 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET
INFO 01-18 04:05:53 launcher.py:27] Route: /docs, Methods: HEAD, GET
INFO 01-18 04:05:53 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 01-18 04:05:53 launcher.py:27] Route: /redoc, Methods: HEAD, GET
INFO 01-18 04:05:53 launcher.py:27] Route: /health, Methods: GET
INFO 01-18 04:05:53 launcher.py:27] Route: /ping, Methods: POST, GET
INFO 01-18 04:05:53 launcher.py:27] Route: /tokenize, Methods: POST
INFO 01-18 04:05:53 launcher.py:27] Route: /detokenize, Methods: POST
INFO 01-18 04:05:53 launcher.py:27] Route: /v1/models, Methods: GET
INFO 01-18 04:05:53 launcher.py:27] Route: /version, Methods: GET
INFO 01-18 04:05:53 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 01-18 04:05:53 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 01-18 04:05:53 launcher.py:27] Route: /v1/embeddings, Methods: POST
INFO 01-18 04:05:53 launcher.py:27] Route: /pooling, Methods: POST
INFO 01-18 04:05:53 launcher.py:27] Route: /score, Methods: POST
INFO 01-18 04:05:53 launcher.py:27] Route: /v1/score, Methods: POST
INFO 01-18 04:05:53 launcher.py:27] Route: /invocations, Methods: POST
INFO:     Started server process [1643394]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
