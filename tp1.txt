nohup: ignoring input
WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
INFO 01-23 01:02:11 api_server.py:773] vLLM API server version 0.1.dev4054+g1ceca5a.d20250122
INFO 01-23 01:02:11 api_server.py:774] args: Namespace(subparser='serve', model_tag='meta-llama/Meta-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=256, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=4, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f45c90e5510>)
hosseins: api_server.py run_server
hosseins: api_server.py build_async_engine_client() args=Namespace(subparser='serve', model_tag='meta-llama/Meta-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=256, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=4, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f45c90e5510>)
INFO 01-23 01:02:11 __init__.py:179] Automatically detected platform tpu.
hosseins: api_server.py build_async_engine_client_from_engine_args() engine_args=AsyncEngineArgs(model='meta-llama/Meta-Llama-3.1-8B', served_model_name=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', task='auto', skip_tokenizer_init=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, seed=0, max_model_len=256, worker_use_ray=False, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, revision=None, code_revision=None, rope_scaling=None, rope_theta=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, fully_sharded_loras=False, lora_extra_vocab_size=256, long_lora_scaling_factors=None, lora_dtype='auto', max_cpu_loras=None, device='auto', num_scheduler_steps=4, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, speculative_model=None, speculative_model_quantization=None, speculative_draft_tensor_parallel_size=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, qlora_adapter_name_or_path=None, disable_logprobs_during_spec_decoding=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto', kv_transfer_config=None, generation_config=None, disable_log_requests=True)
hosseins: build_async_engine_client_from_engine_args() -> else if PROMETHEUS_MULTIPROC_DIR not in
INFO 01-23 01:02:11 api_server.py:199] Started engine process with PID 2520275
INFO 01-23 01:02:14 __init__.py:179] Automatically detected platform tpu.
INFO 01-23 01:02:17 config.py:517] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.
INFO 01-23 01:02:20 config.py:517] This model supports multiple tasks: {'classify', 'generate', 'reward', 'embed', 'score'}. Defaulting to 'generate'.
hosseins: LLMEngine -> _get_executor_cls(): engine_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7aec8d9f6ef0>, cache_config=<vllm.config.CacheConfig object at 0x7aed051ae170>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7aed70fd7f40>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='f5101')
hosseins: LLMEngine -> _get_executor_cls(): executor_class=<class 'vllm.executor.tpu_executor.TPUExecutor'>
hosseins: LLMEngine -> __init__() vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7aec8d9f6ef0>, cache_config=<vllm.config.CacheConfig object at 0x7aed051ae170>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7aed70fd7f40>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='f5101')
INFO 01-23 01:02:20 llm_engine.py:239] Initializing an LLM engine (v0.1.dev4054+g1ceca5a.d20250122) with config: model='meta-llama/Meta-Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir='/dev/shm', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B, num_scheduler_steps=4, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
hosseins: LLMEngine -> _init_tokenizer()
hosseins: LLMEngine -> get_tokenizer_group() group_type=<class 'vllm.transformers_utils.tokenizer_group.base_tokenizer_group.BaseTokenizerGroup'>
hosseins: linear.py is called
hosseins: TPUModelRunner -> __init__()
INFO 01-23 01:02:20 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.
INFO 01-23 01:02:20 selector.py:163] Using Pallas backend.
hosseins: parallel_state.py -> init_distributed_environment rank=0
hosseins: parallel_state.py -> init_distributed_environment distributed_init_method='tcp://10.130.0.54:56913'
hosseins: parallel_state.py -> init_distributed_environment local_rank=0
hosseins: parallel_state.py -> init_distributed_environment backend='gloo'
hosseins: parallel_state.py -> init_world_group ranks=[0]
hosseins: parallel_state.py -> init_world_group backend='gloo'
hosseins: parallel_state.py -> init_world_group local_rank=0
hosseins: GroupCoordinator __init__() group_ranks=[[0]]
hosseins: GroupCoordinator __init__() local_rank=0
hosseins: GroupCoordinator -> __init__() self.ranks=[0]
hosseins: GroupCoordinator -> __init__() self.world_size=1
hosseins: GroupCoordinator -> __init__() self.rank_in_group=0
hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7aec8d245a70>
hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7aec8d245870>
hosseins: parallel_state.py -> ensure_model_parallel_initialized tensor_model_parallel_size=1
hosseins: parallel_state.py -> ensure_model_parallel_initialized pipeline_model_parallel_size=1
hosseins: parallel_state.py -> ensure_model_parallel_initialized backend=None
hosseins: parallel_state.py -> initialize_model_parallel tensor_model_parallel_size=1
hosseins: parallel_state.py -> initialize_model_parallel pipeline_model_parallel_size=1
hosseins: parallel_state.py -> initialize_model_parallel backend='gloo'
hosseins: parallel_state.py -> init_model_parallel_group group_ranks=[[0]]
hosseins: parallel_state.py -> init_model_parallel_group local_rank=0
hosseins: parallel_state.py -> init_model_parallel_group backend='gloo'
hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=None
hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=True
hosseins: parallel_state.py -> init_model_parallel_group group_name='tp'
hosseins: GroupCoordinator __init__() group_ranks=[[0]]
hosseins: GroupCoordinator __init__() local_rank=0
hosseins: GroupCoordinator -> __init__() self.ranks=[0]
hosseins: GroupCoordinator -> __init__() self.world_size=1
hosseins: GroupCoordinator -> __init__() self.rank_in_group=0
hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7aec8d804e70>
hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7aec8d804cf0>
hosseins: parallel_state.py -> init_model_parallel_group group_ranks=[[0]]
hosseins: parallel_state.py -> init_model_parallel_group local_rank=0
hosseins: parallel_state.py -> init_model_parallel_group backend='gloo'
hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=False
hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=False
hosseins: parallel_state.py -> init_model_parallel_group group_name='pp'
hosseins: GroupCoordinator __init__() group_ranks=[[0]]
hosseins: GroupCoordinator __init__() local_rank=0
hosseins: GroupCoordinator -> __init__() self.ranks=[0]
hosseins: GroupCoordinator -> __init__() self.world_size=1
hosseins: GroupCoordinator -> __init__() self.rank_in_group=0
hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7aec8d805030>
hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7aec8d8057b0>
hosseins: TPUModelRunner -> load_model()
hosseins: DefaultModelLoader __init__()
hosseins: DefaultModelLoader load_model()
hosseins: loader.py _initialize_model()
hosseins: LlamaForCausalLM -> __init__() vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7aec8d9f6ef0>, cache_config=<vllm.config.CacheConfig object at 0x7aed051ae170>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7aed70fd7f40>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='f5101')
hosseins: LlamaForCausalLM -> _init_model - vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7aec8d9f6ef0>, cache_config=<vllm.config.CacheConfig object at 0x7aed051ae170>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7aed70fd7f40>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='f5101')
/home/hosseins/miniconda3/envs/vllm/lib/python3.10/site-packages/torch_xla/runtime.py:242: UserWarning: Replicating tensors already initialized on non-virtual XLA device for SPMD to force SPMD mode. This is one-time overhead to setup, and to minimize such, please set SPMD mode before initializting tensors (i.e., call use_spmd() in the beginning of the program).
  warnings.warn(
hosseins: LlamaModel -> __init__ : [vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7aec8d9f6ef0>, cache_config=<vllm.config.CacheConfig object at 0x7aed051ae170>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7aed70fd7f40>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='f5101')]
hosseins: VocabParallelEmbedding -> __init__()
hosseins: llama.py creating mesh
hosseins: mesh_shape: [mesh_shape=(8, 1)]
hosseins: VocabParallelEmbedding -> _get_indices()
hosseins: UnquantizedEmbeddingMethod -> create_weights()
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: utils.py -> make_layers() [num_hidden_layers=32]
hosseins: utils.py -> make_layers() [layer_fn=<function LlamaModel.__init__.<locals>.<lambda> at 0x7aec2e3ff910>]
hosseins: utils.py -> make_layers() [prefix='model.layers']
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=0]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=1]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=2]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=3]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=4]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=5]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=6]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=7]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=8]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=9]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=10]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=11]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=12]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=13]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=14]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=15]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=16]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=17]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=18]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=19]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=20]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=21]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=22]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=23]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=24]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=25]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=26]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=27]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=28]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=29]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=30]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

hosseins: LlamaAttention -> __init__() : [layer_idx=31]
hosseins: QKVParallelLinear -> __init__: hidden_size=4096
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
hosseins: LinearBase -> __init__() [input_size=4096]
hosseins: llama.py returning mesh
hosseins: ColumnParallelLinear -> __init__()
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: RowParallelLinear -> __init__()
hosseins: LinearBase -> __init__() [input_size=14336]
hosseins: llama.py returning mesh
hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])]
hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}] [cpu_mem_util=3.1]
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: utils.py -> make_layers() [modules=ModuleList(
  (0-31): 32 x LlamaDecoderLayer(
    (self_attn): LlamaAttention(
      (qkv_proj): QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
      (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
      (rotary_emb): Llama3RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=131072, base=500000.0, is_neox_style=True)
      (attn): Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=PallasAttentionBackendImpl)
    )
    (mlp): LlamaMLP(
      (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=28672, bias=False, tp_size=1, gather_output=False)
      (down_proj): RowParallelLinear(input_features=14336, output_features=4096, bias=False, tp_size=1, reduce_results=True)
      (act_fn): SiluAndMul()
    )
    (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
    (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
  )
)]
hosseins: LlamaModel -> __init__ [self.start_layer=0]
hosseins: LlamaModel -> __init__ [self.end_layer=32]
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
hosseins: ColumnParallelLinear -> extra_repr()
hosseins: RowParallelLinear -> extra_repr()
hosseins: LlamaModel -> __init__ [self.layers=ModuleList(
  (0-31): 32 x LlamaDecoderLayer(
    (self_attn): LlamaAttention(
      (qkv_proj): QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
      (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
      (rotary_emb): Llama3RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=131072, base=500000.0, is_neox_style=True)
      (attn): Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=PallasAttentionBackendImpl)
    )
    (mlp): LlamaMLP(
      (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=28672, bias=False, tp_size=1, gather_output=False)
      (down_proj): RowParallelLinear(input_features=14336, output_features=4096, bias=False, tp_size=1, reduce_results=True)
      (act_fn): SiluAndMul()
    )
    (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
    (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
  )
)]
hosseins: LlamaForCausalLM -> __init__() get_pp_group().is_last_rank=True
hosseins: VocabParallelEmbedding -> __init__()
hosseins: llama.py returning mesh
hosseins: VocabParallelEmbedding -> _get_indices()
hosseins: UnquantizedEmbeddingMethod -> create_weights()
hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: ParallelLMHead -> __init__()
hosseins: DefaultModelLoader load_model() 1 [{'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: DefaultModelLoader load_model() 1 [3.1]
hosseins: LlamaForCausalLM -> load_weights()
hosseins: AutoWeightsLoader -> __init__()
hosseins: AutoWeightsLoader -> load_weights()
hosseins: DefaultModelLoader _get_all_weights()
hosseins: DefaultModelLoader _get_weights_iterator()
hosseins: DefaultModelLoader _prepare_weights()
hosseins: DefaultModelLoader _maybe_download_from_modelscope()
INFO 01-23 01:02:56 weight_utils.py:256] Using model weights format ['*.safetensors']
hosseins: weight_utils.py -> safetensors_weights_iterator()
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
hosseins: weight_utils.py -> safetensors_weights_iterator() [{'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: weight_utils.py -> safetensors_weights_iterator() [3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='lm_head.weight'
hosseins: VocabParallelEmbedding -> weight_loader()
hosseins: llama.py returning mesh
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5bcd0>
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.input_layernorm.weight'
hosseins: LlamaModel -> load_weights()
hosseins: LlamaModel -> load_weights() [name='layers.31.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.31.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.71it/s]
<rich.table.Table object at 0x7aec2cf597b0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e3490>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e3490>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.31.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.norm.weight'
hosseins: LlamaModel -> load_weights() [name='norm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='norm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: weight_utils.py -> safetensors_weights_iterator() [{'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: weight_utils.py -> safetensors_weights_iterator() [3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.20.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.20.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5a3e0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e08b0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e08b0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.20.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bca0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1bd0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1bd0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.20.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.21.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.21.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5b3d0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1e10>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1e10>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.21.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5aa10>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1a20>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1a20>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.21.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf58e20>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1a20>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1a20>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.21.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.21.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf58e20>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0af0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0af0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.21.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5b340>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1480>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1480>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.21.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf58e20>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0af0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0af0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.21.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5aa10>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0af0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0af0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.22.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.22.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf58310>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0dc0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0dc0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.22.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5aa10>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2050>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2050>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.22.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bd60>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2050>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2050>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.22.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.22.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bd60>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942290>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942290>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.22.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf59720>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1b40>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1b40>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.22.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bd60>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942290>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942290>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.22.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a380>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942290>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942290>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.23.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.23.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf597b0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2290>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2290>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.23.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf58b50>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1ea0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1ea0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.23.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bc10>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1ea0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1ea0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.23.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.23.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b5e0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0f70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0f70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.23.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf58e20>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1900>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1900>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.23.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b5e0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0f70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0f70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.23.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf58b50>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0f70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0f70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.24.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.24.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5be50>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0ca0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0ca0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.24.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a3e0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e24d0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e24d0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.24.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bca0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e24d0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e24d0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.24.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.24.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b1c0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e17e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e17e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.24.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5b6d0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1fc0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1fc0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.24.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b1c0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e17e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e17e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.24.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a3e0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e17e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e17e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.25.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.25.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf590f0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2710>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2710>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.25.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a980>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2320>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2320>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.25.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3eb0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2320>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2320>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.25.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.25.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3eb0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e13f0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e13f0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.25.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc36d0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1d80>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1d80>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.25.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3ee0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e13f0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e13f0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.25.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc39d0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e13f0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e13f0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.26.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.26.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc15a0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0e50>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0e50>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.26.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc2dd0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2950>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2950>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.26.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3cd0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2950>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2950>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.26.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.26.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3cd0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1c60>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1c60>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.2]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.26.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc3fd0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2440>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2440>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.26.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc2320>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1c60>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1c60>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.26.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc1f30>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1c60>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1c60>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.27.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.27.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc2980>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2b90>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2b90>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.27.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc0f70>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e27a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e27a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.27.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3fa0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e27a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e27a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.27.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.27.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3fa0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1870>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1870>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.27.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc2fe0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2200>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2200>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.27.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3eb0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1870>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1870>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.27.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc37c0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1870>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1870>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.28.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.28.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc2fb0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1360>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1360>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.28.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc34c0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2dd0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2dd0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.28.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc39d0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2dd0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2dd0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.28.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.28.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc39d0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e20e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e20e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.28.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc3a60>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e28c0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e28c0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.28.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3cd0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e20e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e20e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.28.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3a90>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e20e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e20e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.29.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.29.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc3c10>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e3010>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e3010>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.29.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3f40>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2c20>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2c20>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.29.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc1f30>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2c20>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2c20>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.29.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.29.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc1f30>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1cf0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1cf0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.29.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc15a0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2680>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2680>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.29.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3fa0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1cf0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1cf0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.29.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc34f0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1cf0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1cf0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.30.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.30.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc3ee0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1630>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1630>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.30.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc24a0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e3250>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e3250>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.30.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc37c0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e3250>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e3250>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.30.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.30.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc37c0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2560>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2560>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.30.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc2980>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2d40>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2d40>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.30.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc39d0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2560>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2560>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.30.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3b80>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2560>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2560>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.31.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3eb0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e30a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e30a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.31.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc1840>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e30a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e30a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.31.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf59720>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2170>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2170>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.31.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930bf40>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2b00>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2b00>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.31.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bf70>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2170>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2170>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.31.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.38s/it]
<rich.table.Table object at 0x7aebf930b820>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2170>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e2170>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.1]
hosseins: weight_utils.py -> safetensors_weights_iterator() [{'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: weight_utils.py -> safetensors_weights_iterator() [3.1]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.embed_tokens.weight'
hosseins: LlamaModel -> load_weights() [name='embed_tokens.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([128256, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='embed_tokens.weight'] [loaded_weight.shape=torch.Size([128256, 4096])] [param.shape=torch.Size([128256, 4096])]
hosseins: VocabParallelEmbedding -> weight_loader()
hosseins: llama.py returning mesh
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930aec0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2e3ff880>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([128256, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2e3ff880>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.0.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.0.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930bf40>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05900>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05900>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.0.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b6a0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da055a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da055a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.0.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bcd0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da055a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da055a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.0.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.0.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bfa0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d941750>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d941750>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.0.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf59ea0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942170>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942170>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.0.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bbe0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d941750>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d941750>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.0.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf590f0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d941750>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d941750>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.1.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.1.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5bf70>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9420e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9420e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.1.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930baf0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da07520>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da07520>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.1.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf59720>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da07520>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da07520>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.1.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.1.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a380>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ea70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ea70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.1.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930b820>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2e3ffc70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2e3ffc70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.1.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b6d0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ea70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ea70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.1.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930aec0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ea70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ea70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.2.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.2.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930bbe0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e830>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e830>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.2.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5ba90>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e9e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e9e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.2.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930baf0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e9e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e9e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.2.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.2.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930be20>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ed40>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ed40>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.2.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf590f0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eef0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eef0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.2.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bd60>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ed40>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ed40>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.2.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a260>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ed40>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ed40>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.3.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.3.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf598a0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942200>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d942200>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.3.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930be50>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05750>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05750>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.3.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b0d0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05750>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05750>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.3.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.3.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a830>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f1c0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f1c0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.3.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930bf40>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f370>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f370>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.3.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5ae60>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f1c0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f1c0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.3.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bdf0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f1c0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f1c0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.4.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.4.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930bd60>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eb90>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eb90>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.4.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a0b0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ecb0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ecb0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.4.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930be50>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ecb0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ecb0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.4.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.4.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bfd0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eb00>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eb00>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.4.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5a260>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f130>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f130>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.4.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bf70>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eb00>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eb00>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.4.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b1c0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eb00>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98eb00>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.5.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.5.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5b6d0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2e3ffd00>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2e3ffd00>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.5.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bca0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f880>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f880>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.5.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc2fb0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f880>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f880>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.5.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.5.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a260>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f520>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f520>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.5.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930b3a0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f6d0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f6d0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.5.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf59ea0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f520>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f520>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.5.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bb20>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f520>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f520>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.6.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.6.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930bcd0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f0a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f0a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.6.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3c70>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ef80>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ef80>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 9.42348018111639e-05}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.6.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf931bdc0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ef80>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ef80>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.6.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.6.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3c70>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f2e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f2e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.6.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5b1c0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e710>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e710>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.6.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3f40>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f2e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f2e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.6.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930b820>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f2e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f2e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.7.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.7.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930bdf0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fd90>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fd90>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.7.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b5e0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fbe0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fbe0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.7.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3c70>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fbe0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fbe0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.7.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.7.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf931bfd0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f7f0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f7f0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.7.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5b880>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fa30>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fa30>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.7.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf931bf70>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f7f0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f7f0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.7.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930b3a0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f7f0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f7f0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.8.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.8.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930b880>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f640>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f640>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.8.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc29e0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98feb0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98feb0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.8.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5ba90>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98feb0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98feb0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.8.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.8.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc2fb0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05870>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05870>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.8.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930aec0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98edd0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98edd0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.8.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5baf0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05870>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05870>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.8.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.46s/it]
<rich.table.Table object at 0x7aec2cfc3fa0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05870>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec8da05870>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: weight_utils.py -> safetensors_weights_iterator() [{'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: weight_utils.py -> safetensors_weights_iterator() [3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.10.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.10.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5a830>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f5b0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f5b0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.10.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930be20>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e8c0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e8c0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.10.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc2170>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e8c0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e8c0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.10.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.10.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b1c0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f250>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f250>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.10.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930bdf0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ff40>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ff40>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.10.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3c70>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f250>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f250>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.10.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a260>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f250>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f250>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.11.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.11.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc2fb0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0700>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0700>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.11.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bdf0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e01f0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e01f0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.11.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b1c0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e01f0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e01f0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.11.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.11.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930b850>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0310>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0310>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.11.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc2170>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0430>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0430>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.11.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bd60>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0310>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0310>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.11.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a830>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0310>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0310>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.12.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.12.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5a0b0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f490>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f490>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.12.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf931bd90>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fac0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fac0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.12.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b880>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fac0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fac0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.12.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.12.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc1e10>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ee60>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ee60>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.12.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf93235e0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ec20>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ec20>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.12.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bf10>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ee60>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ee60>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.12.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930be20>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ee60>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98ee60>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.13.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.13.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf9323e50>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0b80>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0b80>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.13.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930b700>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0790>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0790>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.13.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3c70>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0790>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0790>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.13.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.13.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bdf0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0820>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0820>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.13.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf590f0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0550>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0550>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.13.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930b880>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0820>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0820>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.13.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf9323dc0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0820>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0820>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.14.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.14.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cf5a830>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0d30>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0d30>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.14.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3cd0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fd00>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fd00>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.14.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930b850>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fd00>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fd00>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.14.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.14.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc2320>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f010>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f010>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.14.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf9323ac0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f9a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f9a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.14.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3c70>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f010>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f010>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.14.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5ba90>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f010>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f010>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.15.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.15.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc34f0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1090>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1090>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.15.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3fa0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0ee0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0ee0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.15.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a260>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0ee0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0ee0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.15.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.15.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc29e0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e03a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e03a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.15.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf9323820>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0a60>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0a60>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.15.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a260>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e03a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e03a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.15.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930be20>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e03a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e03a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.16.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.16.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf9323f10>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e11b0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e11b0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.16.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a380>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1240>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1240>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.16.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930baf0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1240>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1240>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.16.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.16.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b0d0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e950>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e950>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.16.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc3c10>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fe20>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fe20>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.16.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bfa0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e950>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e950>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.16.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bf10>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e950>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98e950>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.17.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.17.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930b820>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1510>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1510>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.17.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc39d0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1120>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1120>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.17.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b6a0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1120>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1120>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.17.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.17.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc29e0>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0280>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0280>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.17.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf930b9d0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0c10>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0c10>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.17.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5bcd0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0280>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0280>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.17.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf9323fd0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0280>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0280>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.18.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.18.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc3c70>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0940>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0940>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.18.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc1e10>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1750>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1750>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.18.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bdf0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1750>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1750>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.18.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.18.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc3a90>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fc70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fc70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.18.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf932b9a0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e12d0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e12d0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.18.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf9323df0>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fc70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fc70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.18.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf932b9a0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fc70>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fc70>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.19.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.19.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf931bfa0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1990>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1990>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.19.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bdf0>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e15a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e15a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.19.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf9323df0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e15a0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e15a0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.19.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.19.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5b880>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e05e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e05e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.19.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aec2cfc2fb0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1000>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1000>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.19.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf9323820>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e05e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e05e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.19.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc34f0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e05e0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e05e0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.20.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930b820>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1bd0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e1bd0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.20.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a260>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f760>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f760>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.20.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf93238b0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e16c0>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e16c0>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.20.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930b700>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f760>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f760>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.20.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf932beb0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f760>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f760>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.input_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.9.input_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.mlp.down_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.9.mlp.down_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf932be80>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0160>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0160>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.mlp.gate_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.9.mlp.gate_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cfc1e10>
hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0040>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0040>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.mlp.up_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.9.mlp.up_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bfa0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0040>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d9e0040>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.post_attention_layernorm.weight'
hosseins: LlamaModel -> load_weights() [name='layers.9.post_attention_layernorm.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader()
hosseins: weight_utils.py -> default_weight_loader() [torch.Size([4096])]
hosseins: weight_utils.py -> default_weight_loader() [xla:0]
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7aec8d264670>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.k_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.9.self_attn.k_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aec2cf5a830>
hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f910>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f910>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.o_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.9.self_attn.o_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader()
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
<rich.table.Table object at 0x7aebf9323fa0>
hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fb50>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98fb50>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.q_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.9.self_attn.q_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
<rich.table.Table object at 0x7aebf930bd30>
hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f910>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f910>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.v_proj.weight'
hosseins: LlamaModel -> load_weights() [name='layers.9.self_attn.v_proj.weight']
hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
hosseins: QKVParallelLinear -> weight_loader()
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.78s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.56s/it]

<rich.table.Table object at 0x7aebf932b9a0>
hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f910>]
hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7aec2d98f910>]
hosseins: LlamaModel -> load_weights() [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.3]
hosseins: LlamaModel -> load_weights() [loaded_params={'layers.1.post_attention_layernorm.weight', 'layers.14.self_attn.qkv_proj.weight', 'layers.14.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.3.input_layernorm.weight', 'layers.1.self_attn.o_proj.weight', 'layers.13.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.12.self_attn.qkv_proj.weight', 'layers.17.mlp.gate_up_proj.weight', 'layers.23.mlp.gate_up_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.25.self_attn.qkv_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.5.input_layernorm.weight', 'layers.9.mlp.gate_up_proj.weight', 'layers.11.self_attn.qkv_proj.weight', 'layers.10.mlp.gate_up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.30.input_layernorm.weight', 'layers.20.self_attn.o_proj.weight', 'layers.15.input_layernorm.weight', 'layers.16.self_attn.o_proj.weight', 'layers.31.self_attn.qkv_proj.weight', 'layers.13.mlp.gate_up_proj.weight', 'layers.2.mlp.gate_up_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.9.mlp.down_proj.weight', 'layers.21.mlp.down_proj.weight', 'layers.9.input_layernorm.weight', 'layers.21.self_attn.qkv_proj.weight', 'layers.28.mlp.down_proj.weight', 'layers.26.self_attn.qkv_proj.weight', 'layers.18.mlp.gate_up_proj.weight', 'layers.0.mlp.gate_up_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.5.self_attn.qkv_proj.weight', 'layers.28.mlp.gate_up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.7.self_attn.qkv_proj.weight', 'layers.12.mlp.gate_up_proj.weight', 'layers.12.input_layernorm.weight', 'layers.7.self_attn.o_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.6.input_layernorm.weight', 'layers.2.self_attn.qkv_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.1.input_layernorm.weight', 'layers.19.self_attn.qkv_proj.weight', 'layers.20.input_layernorm.weight', 'layers.24.self_attn.o_proj.weight', 'layers.29.input_layernorm.weight', 'layers.0.post_attention_layernorm.weight', 'layers.28.post_attention_layernorm.weight', 'layers.12.post_attention_layernorm.weight', 'layers.29.self_attn.o_proj.weight', 'layers.11.mlp.gate_up_proj.weight', 'layers.24.mlp.down_proj.weight', 'layers.17.mlp.down_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.22.mlp.down_proj.weight', 'layers.18.self_attn.qkv_proj.weight', 'layers.6.mlp.gate_up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.4.self_attn.qkv_proj.weight', 'layers.8.mlp.down_proj.weight', 'layers.22.self_attn.qkv_proj.weight', 'layers.16.mlp.down_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.30.mlp.down_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.17.self_attn.qkv_proj.weight', 'layers.18.input_layernorm.weight', 'layers.19.input_layernorm.weight', 'layers.14.self_attn.o_proj.weight', 'layers.7.input_layernorm.weight', 'layers.27.self_attn.qkv_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.30.self_attn.qkv_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.9.self_attn.o_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.5.post_attention_layernorm.weight', 'norm.weight', 'layers.15.mlp.gate_up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.9.self_attn.qkv_proj.weight', 'layers.26.mlp.gate_up_proj.weight', 'layers.24.self_attn.qkv_proj.weight', 'layers.29.self_attn.qkv_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.mlp.down_proj.weight', 'layers.23.mlp.down_proj.weight', 'layers.1.mlp.gate_up_proj.weight', 'layers.20.mlp.gate_up_proj.weight', 'layers.4.mlp.down_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.22.input_layernorm.weight', 'layers.20.self_attn.qkv_proj.weight', 'layers.16.mlp.gate_up_proj.weight', 'layers.27.mlp.down_proj.weight', 'layers.29.mlp.down_proj.weight', 'layers.12.mlp.down_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.8.mlp.gate_up_proj.weight', 'layers.13.mlp.down_proj.weight', 'layers.0.self_attn.qkv_proj.weight', 'layers.1.self_attn.qkv_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.25.input_layernorm.weight', 'layers.9.post_attention_layernorm.weight', 'layers.25.mlp.gate_up_proj.weight', 'layers.28.self_attn.qkv_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.23.post_attention_layernorm.weight', 'layers.3.self_attn.o_proj.weight', 'layers.13.self_attn.qkv_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.27.mlp.gate_up_proj.weight', 'layers.19.mlp.gate_up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.4.mlp.gate_up_proj.weight', 'layers.18.mlp.down_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.27.self_attn.o_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.16.self_attn.qkv_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.15.self_attn.qkv_proj.weight', 'layers.26.input_layernorm.weight', 'layers.2.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.24.post_attention_layernorm.weight', 'layers.29.mlp.gate_up_proj.weight', 'layers.31.input_layernorm.weight', 'layers.14.mlp.gate_up_proj.weight', 'layers.8.input_layernorm.weight', 'layers.10.self_attn.qkv_proj.weight', 'layers.3.mlp.down_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.25.mlp.down_proj.weight', 'layers.6.self_attn.qkv_proj.weight', 'layers.8.self_attn.qkv_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.22.mlp.gate_up_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.28.input_layernorm.weight', 'layers.23.self_attn.qkv_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.10.input_layernorm.weight', 'layers.27.input_layernorm.weight', 'layers.24.input_layernorm.weight', 'layers.10.post_attention_layernorm.weight', 'layers.16.input_layernorm.weight', 'layers.3.mlp.gate_up_proj.weight', 'layers.6.mlp.down_proj.weight', 'layers.14.mlp.down_proj.weight', 'embed_tokens.weight', 'layers.30.mlp.gate_up_proj.weight', 'layers.21.mlp.gate_up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.5.mlp.gate_up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.23.input_layernorm.weight', 'layers.12.self_attn.o_proj.weight', 'layers.5.mlp.down_proj.weight', 'layers.1.mlp.down_proj.weight', 'layers.2.mlp.down_proj.weight', 'layers.21.input_layernorm.weight', 'layers.24.mlp.gate_up_proj.weight', 'layers.31.mlp.gate_up_proj.weight', 'layers.7.mlp.gate_up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.0.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.17.input_layernorm.weight', 'layers.11.input_layernorm.weight', 'layers.5.self_attn.o_proj.weight', 'layers.3.self_attn.qkv_proj.weight', 'layers.4.input_layernorm.weight', 'layers.18.post_attention_layernorm.weight'}]
hosseins: DefaultModelLoader load_model() 1 [195 weights loaded]
hosseins: DefaultModelLoader load_model() 2 [{'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 22.459165351601566}]
hosseins: DefaultModelLoader load_model() 2 [3.3]
hosseins: LLMEngine -> _initialize_kv_caches()
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-23 01:03:03 tpu_executor.py:81] # TPU blocks: 8752, # CPU blocks: 8192
INFO 01-23 01:03:04 tpu_model_runner.py:286] Compiling the model with different input shapes...
hosseins: TPUModelRunner -> warmup_model() 1 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.267045328297876}]
hosseins: TPUModelRunner -> warmup_model() 1 [cpu_mem_util=4.3]
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-23 01:03:04 tpu_model_runner.py:301] batch_size: 1, seq_len: 16
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-23 01:03:05 tpu_model_runner.py:301] batch_size: 1, seq_len: 32
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-23 01:03:05 tpu_model_runner.py:301] batch_size: 1, seq_len: 64
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-23 01:03:06 tpu_model_runner.py:301] batch_size: 1, seq_len: 128
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-23 01:03:07 tpu_model_runner.py:301] batch_size: 1, seq_len: 256
hosseins: TPUModelRunner -> warmup_model() 2 [cpu_mem_util=4.2]
hosseins: TPUModelRunner -> warmup_model() 2 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26861718005278}]
INFO 01-23 01:03:07 tpu_model_runner.py:313] Compilation for prefill done in 2.82 s.
hosseins: TPUModelRunner -> warmup_model() 3 [cpu_mem_util=4.2]
hosseins: TPUModelRunner -> warmup_model() 3 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26861718005278}]
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26871332244168}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:08 tpu_model_runner.py:359] batch_size: 8, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=16]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([16, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26871332244168}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:09 tpu_model_runner.py:359] batch_size: 16, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=32]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([32, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26882625159688}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:10 tpu_model_runner.py:359] batch_size: 32, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=48]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([48, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26882625159688}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:12 tpu_model_runner.py:359] batch_size: 48, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=64]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([64, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26882625159688}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:13 tpu_model_runner.py:359] batch_size: 64, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=80]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([80, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.269094839857914}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:14 tpu_model_runner.py:359] batch_size: 80, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=96]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([96, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.269213873291775}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:15 tpu_model_runner.py:359] batch_size: 96, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=112]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([112, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.269213873291775}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:16 tpu_model_runner.py:359] batch_size: 112, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=128]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([128, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.269213873291775}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:17 tpu_model_runner.py:359] batch_size: 128, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=144]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([144, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26947635727415}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:19 tpu_model_runner.py:359] batch_size: 144, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=160]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([160, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26962591210132}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:20 tpu_model_runner.py:359] batch_size: 160, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=176]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([176, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26962591210132}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:21 tpu_model_runner.py:359] batch_size: 176, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=192]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([192, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26962591210132}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:22 tpu_model_runner.py:359] batch_size: 192, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=208]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([208, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.269851770411734}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:24 tpu_model_runner.py:359] batch_size: 208, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=224]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([224, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26999827309957}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:25 tpu_model_runner.py:359] batch_size: 224, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=240]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([240, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26999827309957}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:26 tpu_model_runner.py:359] batch_size: 240, seq_len: 1
hossein: LlamaForCausalLM -> forward
hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=256]
hosseins: LlamaForCausalLM -> compute_logits
hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([256, 128256])]
hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'TPU Utilization (%) / Device 0': 0.0, 'TPU Memory (%) / Device 0': 35.26999827309957}]
hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.3]
INFO 01-23 01:03:27 tpu_model_runner.py:359] batch_size: 256, seq_len: 1
INFO 01-23 01:03:27 tpu_model_runner.py:366] Compilation for decode done in 20.68 s.
INFO 01-23 01:03:27 llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 24.97 seconds
hosseins: init_app_state
INFO 01-23 01:03:28 api_server.py:697] Using supplied chat template:
INFO 01-23 01:03:28 api_server.py:697] None
INFO 01-23 01:03:28 launcher.py:19] Available routes are:
INFO 01-23 01:03:28 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD
INFO 01-23 01:03:28 launcher.py:27] Route: /docs, Methods: GET, HEAD
INFO 01-23 01:03:28 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 01-23 01:03:28 launcher.py:27] Route: /redoc, Methods: GET, HEAD
INFO 01-23 01:03:28 launcher.py:27] Route: /health, Methods: GET
INFO 01-23 01:03:28 launcher.py:27] Route: /ping, Methods: POST, GET
INFO 01-23 01:03:28 launcher.py:27] Route: /tokenize, Methods: POST
INFO 01-23 01:03:28 launcher.py:27] Route: /detokenize, Methods: POST
INFO 01-23 01:03:28 launcher.py:27] Route: /v1/models, Methods: GET
INFO 01-23 01:03:28 launcher.py:27] Route: /version, Methods: GET
INFO 01-23 01:03:28 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 01-23 01:03:28 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 01-23 01:03:28 launcher.py:27] Route: /v1/embeddings, Methods: POST
INFO 01-23 01:03:28 launcher.py:27] Route: /pooling, Methods: POST
INFO 01-23 01:03:28 launcher.py:27] Route: /score, Methods: POST
INFO 01-23 01:03:28 launcher.py:27] Route: /v1/score, Methods: POST
INFO 01-23 01:03:28 launcher.py:27] Route: /invocations, Methods: POST
INFO:     Started server process [2520060]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
ERROR 01-23 01:09:38 client.py:282] RuntimeError('Engine process (pid 2520275) died.')
ERROR 01-23 01:09:38 client.py:282] NoneType: None
