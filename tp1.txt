nohup: ignoring input
WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
INFO 01-25 02:22:36 api_server.py:773] vLLM API server version 0.1.dev4058+gd873b9d.d20250125
INFO 01-25 02:22:36 api_server.py:774] args: Namespace(subparser='serve', model_tag='meta-llama/Meta-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=256, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=4, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7c32089df370>)
INFO 01-25 02:22:36 api_server.py:775] hosseins: api_server.py run_server
INFO 01-25 02:22:36 api_server.py:119] hosseins: api_server.py build_async_engine_client() args=Namespace(subparser='serve', model_tag='meta-llama/Meta-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Meta-Llama-3.1-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=256, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=4, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7c32089df370>)
INFO 01-25 02:22:36 __init__.py:179] Automatically detected platform tpu.
INFO 01-25 02:22:36 api_server.py:144] hosseins: api_server.py build_async_engine_client_from_engine_args() engine_args=AsyncEngineArgs(model='meta-llama/Meta-Llama-3.1-8B', served_model_name=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', task='auto', skip_tokenizer_init=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/dev/shm', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, seed=0, max_model_len=256, worker_use_ray=False, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, swap_space=16.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, revision=None, code_revision=None, rope_scaling=None, rope_theta=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, fully_sharded_loras=False, lora_extra_vocab_size=256, long_lora_scaling_factors=None, lora_dtype='auto', max_cpu_loras=None, device='auto', num_scheduler_steps=4, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, speculative_model=None, speculative_model_quantization=None, speculative_draft_tensor_parallel_size=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, qlora_adapter_name_or_path=None, disable_logprobs_during_spec_decoding=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto', kv_transfer_config=None, generation_config=None, disable_log_requests=True)
INFO 01-25 02:22:36 api_server.py:161] hosseins: build_async_engine_client_from_engine_args() -> else if PROMETHEUS_MULTIPROC_DIR not in
INFO 01-25 02:22:36 api_server.py:199] Started engine process with PID 3475184
INFO 01-25 02:22:39 __init__.py:179] Automatically detected platform tpu.
INFO 01-25 02:22:41 config.py:517] This model supports multiple tasks: {'classify', 'score', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 01-25 02:22:42 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:22:45 config.py:517] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 01-25 02:22:45 llm_engine.py:454] hosseins: LLMEngine -> _get_executor_cls(): engine_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7af1ac2eca60>, cache_config=<vllm.config.CacheConfig object at 0x7af22639e140>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7af28f5d7f10>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='9fecb')
INFO 01-25 02:22:45 llm_engine.py:522] hosseins: LLMEngine -> _get_executor_cls(): executor_class=<class 'vllm.executor.tpu_executor.TPUExecutor'>
INFO 01-25 02:22:45 llm_engine.py:222] hosseins: LLMEngine -> __init__() vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7af1ac2eca60>, cache_config=<vllm.config.CacheConfig object at 0x7af22639e140>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7af28f5d7f10>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='9fecb')
INFO 01-25 02:22:45 llm_engine.py:239] Initializing an LLM engine (v0.1.dev4058+gd873b9d.d20250125) with config: model='meta-llama/Meta-Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir='/dev/shm', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B, num_scheduler_steps=4, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
INFO 01-25 02:22:45 llm_engine.py:587] hosseins: LLMEngine -> _init_tokenizer()
INFO 01-25 02:22:45 llm_engine.py:566] hosseins: LLMEngine -> get_tokenizer_group() group_type=<class 'vllm.transformers_utils.tokenizer_group.base_tokenizer_group.BaseTokenizerGroup'>
INFO 01-25 02:22:45 linear.py:44] hosseins: linear.py is called
INFO 01-25 02:22:45 utils.py:256] hosseins: creating mesh
INFO 01-25 02:22:52 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:23:02 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:23:11 utils.py:246] hosseins: mesh_shape: [mesh_shape=(8,)]
INFO 01-25 02:23:11 tpu_model_runner.py:106] hosseins: TPUModelRunner -> __init__()
INFO 01-25 02:23:11 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.
INFO 01-25 02:23:11 selector.py:163] Using Pallas backend.
INFO 01-25 02:23:11 parallel_state.py:971] hosseins: parallel_state.py -> init_distributed_environment rank=0
INFO 01-25 02:23:11 parallel_state.py:972] hosseins: parallel_state.py -> init_distributed_environment distributed_init_method='tcp://10.130.0.54:56127'
INFO 01-25 02:23:11 parallel_state.py:973] hosseins: parallel_state.py -> init_distributed_environment local_rank=0
INFO 01-25 02:23:11 parallel_state.py:974] hosseins: parallel_state.py -> init_distributed_environment backend='gloo'
INFO 01-25 02:23:11 parallel_state.py:854] hosseins: parallel_state.py -> init_world_group ranks=[0]
INFO 01-25 02:23:11 parallel_state.py:855] hosseins: parallel_state.py -> init_world_group backend='gloo'
INFO 01-25 02:23:11 parallel_state.py:856] hosseins: parallel_state.py -> init_world_group local_rank=0
INFO 01-25 02:23:11 parallel_state.py:171] hosseins: GroupCoordinator __init__() group_ranks=[[0]]
INFO 01-25 02:23:11 parallel_state.py:172] hosseins: GroupCoordinator __init__() local_rank=0
INFO 01-25 02:23:11 parallel_state.py:196] hosseins: GroupCoordinator -> __init__() self.ranks=[0]
INFO 01-25 02:23:11 parallel_state.py:197] hosseins: GroupCoordinator -> __init__() self.world_size=1
INFO 01-25 02:23:11 parallel_state.py:198] hosseins: GroupCoordinator -> __init__() self.rank_in_group=0
INFO 01-25 02:23:11 parallel_state.py:199] hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af1ab84da30>
INFO 01-25 02:23:11 parallel_state.py:200] hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af1ab84d930>
INFO 01-25 02:23:11 parallel_state.py:1116] hosseins: parallel_state.py -> ensure_model_parallel_initialized tensor_model_parallel_size=1
INFO 01-25 02:23:11 parallel_state.py:1117] hosseins: parallel_state.py -> ensure_model_parallel_initialized pipeline_model_parallel_size=1
INFO 01-25 02:23:11 parallel_state.py:1118] hosseins: parallel_state.py -> ensure_model_parallel_initialized backend=None
INFO 01-25 02:23:11 parallel_state.py:1036] hosseins: parallel_state.py -> initialize_model_parallel tensor_model_parallel_size=1
INFO 01-25 02:23:11 parallel_state.py:1037] hosseins: parallel_state.py -> initialize_model_parallel pipeline_model_parallel_size=1
INFO 01-25 02:23:11 parallel_state.py:1038] hosseins: parallel_state.py -> initialize_model_parallel backend='gloo'
INFO 01-25 02:23:11 parallel_state.py:878] hosseins: parallel_state.py -> init_model_parallel_group group_ranks=[[0]]
INFO 01-25 02:23:11 parallel_state.py:879] hosseins: parallel_state.py -> init_model_parallel_group local_rank=0
INFO 01-25 02:23:11 parallel_state.py:880] hosseins: parallel_state.py -> init_model_parallel_group backend='gloo'
INFO 01-25 02:23:11 parallel_state.py:881] hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=None
INFO 01-25 02:23:11 parallel_state.py:882] hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=True
INFO 01-25 02:23:11 parallel_state.py:883] hosseins: parallel_state.py -> init_model_parallel_group group_name='tp'
INFO 01-25 02:23:11 parallel_state.py:171] hosseins: GroupCoordinator __init__() group_ranks=[[0]]
INFO 01-25 02:23:11 parallel_state.py:172] hosseins: GroupCoordinator __init__() local_rank=0
INFO 01-25 02:23:11 parallel_state.py:196] hosseins: GroupCoordinator -> __init__() self.ranks=[0]
INFO 01-25 02:23:11 parallel_state.py:197] hosseins: GroupCoordinator -> __init__() self.world_size=1
INFO 01-25 02:23:11 parallel_state.py:198] hosseins: GroupCoordinator -> __init__() self.rank_in_group=0
INFO 01-25 02:23:11 parallel_state.py:199] hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af1ac104f30>
INFO 01-25 02:23:11 parallel_state.py:200] hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af1ac104d70>
INFO 01-25 02:23:11 parallel_state.py:878] hosseins: parallel_state.py -> init_model_parallel_group group_ranks=[[0]]
INFO 01-25 02:23:11 parallel_state.py:879] hosseins: parallel_state.py -> init_model_parallel_group local_rank=0
INFO 01-25 02:23:11 parallel_state.py:880] hosseins: parallel_state.py -> init_model_parallel_group backend='gloo'
INFO 01-25 02:23:11 parallel_state.py:881] hosseins: parallel_state.py -> init_model_parallel_group use_custom_allreduce=False
INFO 01-25 02:23:11 parallel_state.py:882] hosseins: parallel_state.py -> init_model_parallel_group use_message_queue_broadcaster=False
INFO 01-25 02:23:11 parallel_state.py:883] hosseins: parallel_state.py -> init_model_parallel_group group_name='pp'
INFO 01-25 02:23:11 parallel_state.py:171] hosseins: GroupCoordinator __init__() group_ranks=[[0]]
INFO 01-25 02:23:11 parallel_state.py:172] hosseins: GroupCoordinator __init__() local_rank=0
INFO 01-25 02:23:11 parallel_state.py:196] hosseins: GroupCoordinator -> __init__() self.ranks=[0]
INFO 01-25 02:23:11 parallel_state.py:197] hosseins: GroupCoordinator -> __init__() self.world_size=1
INFO 01-25 02:23:11 parallel_state.py:198] hosseins: GroupCoordinator -> __init__() self.rank_in_group=0
INFO 01-25 02:23:11 parallel_state.py:199] hosseins: GroupCoordinator -> __init__() self.device_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af1ac105370>
INFO 01-25 02:23:11 parallel_state.py:200] hosseins: GroupCoordinator -> __init__() self.cpu_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7af1ac105b30>
INFO 01-25 02:23:11 tpu_model_runner.py:138] hosseins: TPUModelRunner -> load_model()
INFO 01-25 02:23:11 loader.py:189] hosseins: DefaultModelLoader __init__()
INFO 01-25 02:23:11 loader.py:368] hosseins: DefaultModelLoader load_model()
INFO 01-25 02:23:11 loader.py:111] hosseins: loader.py _initialize_model()
INFO 01-25 02:23:11 llama.py:667] hosseins: LlamaForCausalLM -> __init__() vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7af1ac2eca60>, cache_config=<vllm.config.CacheConfig object at 0x7af22639e140>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7af28f5d7f10>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='9fecb')
INFO 01-25 02:23:11 llama.py:715] hosseins: LlamaForCausalLM -> _init_model - vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7af1ac2eca60>, cache_config=<vllm.config.CacheConfig object at 0x7af22639e140>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7af28f5d7f10>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='9fecb')
INFO 01-25 02:23:11 llama.py:355] hosseins: LlamaModel -> __init__ : [vllm_config=VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7af1ac2eca60>, cache_config=<vllm.config.CacheConfig object at 0x7af22639e140>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=2048, max_num_seqs=256, max_model_len=256, num_lookahead_slots=3, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=4, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7af28f5d7f10>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir='/dev/shm', model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config={"level":2,"backend":"openxla","splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, kv_transfer_config=None, additional_config=None, instance_id='9fecb')]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 vocab_parallel_embedding.py:215] hosseins: VocabParallelEmbedding -> __init__()
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 vocab_parallel_embedding.py:287] hosseins: VocabParallelEmbedding -> _get_indices()
INFO 01-25 02:23:11 vocab_parallel_embedding.py:37] hosseins: UnquantizedEmbeddingMethod -> create_weights()
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 utils.py:551] hosseins: utils.py -> make_layers() [num_hidden_layers=32]
INFO 01-25 02:23:11 utils.py:552] hosseins: utils.py -> make_layers() [layer_fn=<function LlamaModel.__init__.<locals>.<lambda> at 0x7af1ac06ca60>]
INFO 01-25 02:23:11 utils.py:553] hosseins: utils.py -> make_layers() [prefix='model.layers']
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=0]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=1]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=2]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=3]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=4]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=5]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=6]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=7]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=8]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=9]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=10]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=11]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=12]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=13]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=14]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=15]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=16]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=17]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=18]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=19]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=20]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=21]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=22]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=23]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=24]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=25]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=26]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=27]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=28]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=29]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=30]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 llama.py:276] hosseins: LlamaDecoderLayer -> __init__() config=LlamaConfig {
INFO 01-25 02:23:11 llama.py:276]   "_name_or_path": "meta-llama/Meta-Llama-3.1-8B",
INFO 01-25 02:23:11 llama.py:276]   "architectures": [
INFO 01-25 02:23:11 llama.py:276]     "LlamaForCausalLM"
INFO 01-25 02:23:11 llama.py:276]   ],
INFO 01-25 02:23:11 llama.py:276]   "attention_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "attention_dropout": 0.0,
INFO 01-25 02:23:11 llama.py:276]   "bos_token_id": 128000,
INFO 01-25 02:23:11 llama.py:276]   "eos_token_id": 128001,
INFO 01-25 02:23:11 llama.py:276]   "head_dim": 128,
INFO 01-25 02:23:11 llama.py:276]   "hidden_act": "silu",
INFO 01-25 02:23:11 llama.py:276]   "hidden_size": 4096,
INFO 01-25 02:23:11 llama.py:276]   "initializer_range": 0.02,
INFO 01-25 02:23:11 llama.py:276]   "intermediate_size": 14336,
INFO 01-25 02:23:11 llama.py:276]   "max_position_embeddings": 131072,
INFO 01-25 02:23:11 llama.py:276]   "mlp_bias": false,
INFO 01-25 02:23:11 llama.py:276]   "model_type": "llama",
INFO 01-25 02:23:11 llama.py:276]   "num_attention_heads": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_hidden_layers": 32,
INFO 01-25 02:23:11 llama.py:276]   "num_key_value_heads": 8,
INFO 01-25 02:23:11 llama.py:276]   "pretraining_tp": 1,
INFO 01-25 02:23:11 llama.py:276]   "rms_norm_eps": 1e-05,
INFO 01-25 02:23:11 llama.py:276]   "rope_scaling": {
INFO 01-25 02:23:11 llama.py:276]     "factor": 8.0,
INFO 01-25 02:23:11 llama.py:276]     "high_freq_factor": 4.0,
INFO 01-25 02:23:11 llama.py:276]     "low_freq_factor": 1.0,
INFO 01-25 02:23:11 llama.py:276]     "original_max_position_embeddings": 8192,
INFO 01-25 02:23:11 llama.py:276]     "rope_type": "llama3"
INFO 01-25 02:23:11 llama.py:276]   },
INFO 01-25 02:23:11 llama.py:276]   "rope_theta": 500000.0,
INFO 01-25 02:23:11 llama.py:276]   "tie_word_embeddings": false,
INFO 01-25 02:23:11 llama.py:276]   "torch_dtype": "bfloat16",
INFO 01-25 02:23:11 llama.py:276]   "transformers_version": "4.47.0",
INFO 01-25 02:23:11 llama.py:276]   "use_cache": true,
INFO 01-25 02:23:11 llama.py:276]   "vocab_size": 128256
INFO 01-25 02:23:11 llama.py:276] }
INFO 01-25 02:23:11 llama.py:276] 
INFO 01-25 02:23:11 llama.py:170] hosseins: LlamaAttention -> __init__() : [layer_idx=31]
INFO 01-25 02:23:11 linear.py:775] hosseins: QKVParallelLinear -> __init__: hidden_size=4096
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=6144]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([6144, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
hosseins: Attention -> __init__()
INFO 01-25 02:23:11 llama.py:122] hosseins: LlamaMLP -> __init__ : [hidden_size=4096]
INFO 01-25 02:23:11 linear.py:465] hosseins: MergedColumnParallelLinear -> __init__: tp_size=1
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=4096]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:319] hosseins: ColumnParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=28672]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([28672, 4096])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:1147] hosseins: RowParallelLinear -> __init__()
INFO 01-25 02:23:11 linear.py:182] hosseins: LinearBase -> __init__() [input_size=14336]
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 linear.py:136] hosseins: UnquantizedLinearMethod -> create_weights() [output_size=4096]
INFO 01-25 02:23:11 linear.py:142] hosseins: UnquantizedLinearMethod -> create_weights() [weight.shape=torch.Size([4096, 14336])] [weight.device=device(type='xla', index=0)]
INFO 01-25 02:23:11 linear.py:145] hosseins: UnquantizedLinearMethod -> create_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}] [cpu_mem_util=2.9]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 utils.py:570] hosseins: utils.py -> make_layers() [modules=ModuleList(
INFO 01-25 02:23:11 utils.py:570]   (0-31): 32 x LlamaDecoderLayer(
INFO 01-25 02:23:11 utils.py:570]     (self_attn): LlamaAttention(
INFO 01-25 02:23:11 utils.py:570]       (qkv_proj): QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
INFO 01-25 02:23:11 utils.py:570]       (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 01-25 02:23:11 utils.py:570]       (rotary_emb): Llama3RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=131072, base=500000.0, is_neox_style=True)
INFO 01-25 02:23:11 utils.py:570]       (attn): Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=PallasAttentionBackendImpl)
INFO 01-25 02:23:11 utils.py:570]     )
INFO 01-25 02:23:11 utils.py:570]     (mlp): LlamaMLP(
INFO 01-25 02:23:11 utils.py:570]       (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=28672, bias=False, tp_size=1, gather_output=False)
INFO 01-25 02:23:11 utils.py:570]       (down_proj): RowParallelLinear(input_features=14336, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 01-25 02:23:11 utils.py:570]       (act_fn): SiluAndMul()
INFO 01-25 02:23:11 utils.py:570]     )
INFO 01-25 02:23:11 utils.py:570]     (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 01-25 02:23:11 utils.py:570]     (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 01-25 02:23:11 utils.py:570]   )
INFO 01-25 02:23:11 utils.py:570] )]
INFO 01-25 02:23:11 llama.py:386] hosseins: LlamaModel -> __init__ [self.start_layer=0]
INFO 01-25 02:23:11 llama.py:387] hosseins: LlamaModel -> __init__ [self.end_layer=32]
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
hosseins: Attention -> extra_repr()
INFO 01-25 02:23:11 linear.py:422] hosseins: ColumnParallelLinear -> extra_repr()
INFO 01-25 02:23:11 linear.py:1289] hosseins: RowParallelLinear -> extra_repr()
INFO 01-25 02:23:11 llama.py:388] hosseins: LlamaModel -> __init__ [self.layers=ModuleList(
INFO 01-25 02:23:11 llama.py:388]   (0-31): 32 x LlamaDecoderLayer(
INFO 01-25 02:23:11 llama.py:388]     (self_attn): LlamaAttention(
INFO 01-25 02:23:11 llama.py:388]       (qkv_proj): QKVParallelLinear(in_features=4096, output_features=6144, bias=False, tp_size=1, gather_output=False)
INFO 01-25 02:23:11 llama.py:388]       (o_proj): RowParallelLinear(input_features=4096, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 01-25 02:23:11 llama.py:388]       (rotary_emb): Llama3RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=131072, base=500000.0, is_neox_style=True)
INFO 01-25 02:23:11 llama.py:388]       (attn): Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=PallasAttentionBackendImpl)
INFO 01-25 02:23:11 llama.py:388]     )
INFO 01-25 02:23:11 llama.py:388]     (mlp): LlamaMLP(
INFO 01-25 02:23:11 llama.py:388]       (gate_up_proj): MergedColumnParallelLinear(in_features=4096, output_features=28672, bias=False, tp_size=1, gather_output=False)
INFO 01-25 02:23:11 llama.py:388]       (down_proj): RowParallelLinear(input_features=14336, output_features=4096, bias=False, tp_size=1, reduce_results=True)
INFO 01-25 02:23:11 llama.py:388]       (act_fn): SiluAndMul()
INFO 01-25 02:23:11 llama.py:388]     )
INFO 01-25 02:23:11 llama.py:388]     (input_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 01-25 02:23:11 llama.py:388]     (post_attention_layernorm): RMSNorm(hidden_size=4096, eps=1e-05)
INFO 01-25 02:23:11 llama.py:388]   )
INFO 01-25 02:23:11 llama.py:388] )]
INFO 01-25 02:23:11 llama.py:679] hosseins: LlamaForCausalLM -> __init__() get_pp_group().is_last_rank=True
INFO 01-25 02:23:11 vocab_parallel_embedding.py:215] hosseins: VocabParallelEmbedding -> __init__()
INFO 01-25 02:23:11 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:11 vocab_parallel_embedding.py:287] hosseins: VocabParallelEmbedding -> _get_indices()
INFO 01-25 02:23:11 vocab_parallel_embedding.py:37] hosseins: UnquantizedEmbeddingMethod -> create_weights()
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['input_dim', 'output_dim']]
INFO 01-25 02:23:11 utils.py:19] hosseins: model_executor.utils -> set_weight_attrs() [['weight_loader']]
INFO 01-25 02:23:11 vocab_parallel_embedding.py:480] hosseins: ParallelLMHead -> __init__()
INFO 01-25 02:23:11 loader.py:379] hosseins: DefaultModelLoader load_model() 1 [{'bytes_limit': 33550237696, 'peak_bytes_used': 31616}]
INFO 01-25 02:23:11 loader.py:380] hosseins: DefaultModelLoader load_model() 1 [2.9]
INFO 01-25 02:23:11 llama.py:760] hosseins: LlamaForCausalLM -> load_weights()
INFO 01-25 02:23:11 utils.py:90] hosseins: AutoWeightsLoader -> __init__()
INFO 01-25 02:23:11 utils.py:235] hosseins: AutoWeightsLoader -> load_weights()
INFO 01-25 02:23:11 loader.py:344] hosseins: DefaultModelLoader _get_all_weights()
INFO 01-25 02:23:11 loader.py:305] hosseins: DefaultModelLoader _get_weights_iterator()
INFO 01-25 02:23:11 loader.py:227] hosseins: DefaultModelLoader _prepare_weights()
INFO 01-25 02:23:11 loader.py:197] hosseins: DefaultModelLoader _maybe_download_from_modelscope()
INFO 01-25 02:23:12 weight_utils.py:256] Using model weights format ['*.safetensors']
INFO 01-25 02:23:12 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:23:12 weight_utils.py:412] hosseins: weight_utils.py -> safetensors_weights_iterator()

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
INFO 01-25 02:23:12 weight_utils.py:424] hosseins: weight_utils.py -> safetensors_weights_iterator() [{'bytes_limit': 33550237696, 'peak_bytes_used': 31616}]
INFO 01-25 02:23:12 weight_utils.py:425] hosseins: weight_utils.py -> safetensors_weights_iterator() [2.9]
INFO 01-25 02:23:12 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='lm_head.weight'
INFO 01-25 02:23:12 vocab_parallel_embedding.py:360] hosseins: VocabParallelEmbedding -> weight_loader()
INFO 01-25 02:23:12 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:12 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:12 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:12 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.input_layernorm.weight'
INFO 01-25 02:23:12 llama.py:445] hosseins: LlamaModel -> load_weights()
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='embed_tokens.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='embed_tokens.weight'] [params_dict[key].shape=torch.Size([128256, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='embed_tokens.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='embed_tokens.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([128256, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=525336576] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='embed_tokens.weight'] [param_bytes=1050673152]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.0.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.0.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.0.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.0.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.0.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.0.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.0.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.0.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.0.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.0.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.0.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.0.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.1.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.1.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.1.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.1.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.1.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.1.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.1.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.1.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.1.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.1.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.1.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.1.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.2.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.2.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.2.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.2.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.2.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.2.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.2.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.2.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.2.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.2.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.2.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.2.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.3.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.3.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.3.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.3.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.3.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.3.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.3.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.3.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.3.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.3.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.3.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.3.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.4.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.4.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.4.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.4.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.4.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.4.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.4.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.4.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.4.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.4.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.4.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.4.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.5.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.5.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.5.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.5.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.5.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.5.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.5.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.5.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.5.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.5.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.5.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.5.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.6.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.6.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.6.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.6.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.6.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.6.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.6.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.6.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.6.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.6.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.6.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.6.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.7.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.7.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.7.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.7.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.7.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.7.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.7.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.7.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.7.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.7.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.7.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.7.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.8.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.8.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.8.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.8.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.8.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.8.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.8.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.8.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.8.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.8.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.8.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.8.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.9.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.9.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.9.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.9.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.9.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.9.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.9.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.9.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.9.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.9.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.9.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.9.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.10.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.10.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.10.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.10.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.10.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.10.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.10.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.10.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.10.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.10.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.10.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.10.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.11.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.11.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.11.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.11.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.11.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.11.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.11.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.11.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.11.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.11.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.11.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.11.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.12.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.12.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.12.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.12.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.12.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.12.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.12.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.12.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.12.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.12.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.12.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.12.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.13.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.13.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.13.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.13.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.13.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.13.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.13.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.13.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.13.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.13.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.13.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.13.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.14.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.14.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.14.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.14.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.14.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.14.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.14.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.14.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.14.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.14.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.14.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.14.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.15.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.15.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.15.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.15.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.15.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.15.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.15.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.15.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.15.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.15.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.15.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.15.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.16.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.16.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.16.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.16.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.16.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.16.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.16.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.16.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.16.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.16.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.16.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.16.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.17.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.17.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.17.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.17.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.17.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.17.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.17.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.17.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.17.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.17.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.17.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.17.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.18.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.18.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.18.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.18.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.18.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.18.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.18.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.18.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.18.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.18.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.18.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.18.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.19.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.19.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.19.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.19.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.19.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.19.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.19.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.19.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.19.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.19.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.19.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.19.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.20.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.20.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.20.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.20.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.20.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.20.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.20.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.20.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.20.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.20.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.20.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.20.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.21.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.21.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.21.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.21.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.21.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.21.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.21.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.21.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.21.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.21.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.21.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.21.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.22.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.22.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.22.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.22.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.22.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.22.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.22.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.22.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.22.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.22.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.22.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.22.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.23.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.23.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.23.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.23.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.23.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.23.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.23.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.23.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.23.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.23.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.23.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.23.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.24.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.24.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.24.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.24.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.24.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.24.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.24.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.24.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.24.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.24.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.24.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.24.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.25.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.25.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.25.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.25.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.25.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.25.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.25.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.25.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.25.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.25.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.25.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.25.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.26.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.26.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.26.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.26.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.26.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.26.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.26.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.26.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.26.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.26.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.26.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.26.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.27.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.27.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.27.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.27.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.27.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.27.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.27.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.27.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.27.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.27.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.27.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.27.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.28.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.28.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.28.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.28.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.28.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.28.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.28.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.28.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.28.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.28.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.28.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.28.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.29.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.29.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.29.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.29.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.29.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.29.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.29.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.29.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.29.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.29.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.29.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.29.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.30.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.30.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.30.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.30.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.30.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.30.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.30.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.30.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.30.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.30.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.30.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.30.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.qkv_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.qkv_proj.weight'] [params_dict[key].shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.qkv_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.qkv_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([6144, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=25165824] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.qkv_proj.weight'] [param_bytes=50331648]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.o_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.o_proj.weight'] [params_dict[key].shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.o_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.o_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.31.self_attn.o_proj.weight'] [param_bytes=33554432]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.gate_up_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.gate_up_proj.weight'] [params_dict[key].shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.gate_up_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.gate_up_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([28672, 4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=117440512] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.gate_up_proj.weight'] [param_bytes=234881024]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.down_proj.weight'] [params_dict[key].shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.down_proj.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.down_proj.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.31.mlp.down_proj.weight'] [param_bytes=117440512]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.31.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.31.input_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.31.input_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.31.input_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.31.input_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='layers.31.post_attention_layernorm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='layers.31.post_attention_layernorm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='layers.31.post_attention_layernorm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='layers.31.post_attention_layernorm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='layers.31.post_attention_layernorm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:460] hosseins: LlamaModel -> load_weights() [key='norm.weight']
INFO 01-25 02:23:12 llama.py:461] hosseins: LlamaModel -> load_weights() [key='norm.weight'] [params_dict[key].shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:462] hosseins: LlamaModel -> load_weights() [key='norm.weight'] [params_dict[key].device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:463] hosseins: LlamaModel -> load_weights() [key='norm.weight'] [params_dict[key].dtype=torch.bfloat16]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:465] hosseins: LlamaModel -> load_weights() [key='norm.weight'] [param_bytes=8192]
INFO 01-25 02:23:12 llama.py:469] hosseins: LlamaModel -> load_weights() [total_bytes=15009849344]
INFO 01-25 02:23:12 llama.py:472] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}]
INFO 01-25 02:23:12 llama.py:473] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.6]
INFO 01-25 02:23:12 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.31.input_layernorm.weight']
INFO 01-25 02:23:12 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:12 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:12 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:12 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:12 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:12 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:12 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:12 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:12 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:12 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:12 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:12 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:12 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:12 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:12 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:12 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:12 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:12 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 31616}]
INFO 01-25 02:23:12 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.6]
INFO 01-25 02:23:12 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.down_proj.weight'
INFO 01-25 02:23:12 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.31.mlp.down_proj.weight']
INFO 01-25 02:23:12 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:12 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:12 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:12 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:12 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:12 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:12 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:12 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:12 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:12 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:12 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:12 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:12 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:12 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:12 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:12 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:21 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:21 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:21 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:21 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:21 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:21 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:21 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360a60>]
INFO 01-25 02:23:21 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:21 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:21 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:21 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360a60>]
INFO 01-25 02:23:21 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:21 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:21 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.post_attention_layernorm.weight'
INFO 01-25 02:23:21 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.31.post_attention_layernorm.weight']
INFO 01-25 02:23:21 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:21 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:21 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:21 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:21 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:21 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:21 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:21 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:21 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:21 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:21 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:21 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:21 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:21 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:21 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:21 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:21 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:21 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:21 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.norm.weight'
INFO 01-25 02:23:21 llama.py:478] hosseins: LlamaModel -> load_weights() [name='norm.weight']
INFO 01-25 02:23:21 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:21 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:21 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='norm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:21 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:21 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:21 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:21 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:21 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:21 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:21 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:21 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:21 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:21 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:21 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:21 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:21 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:21 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:21 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:26,  8.89s/it]
INFO 01-25 02:23:21 weight_utils.py:424] hosseins: weight_utils.py -> safetensors_weights_iterator() [{'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:21 weight_utils.py:425] hosseins: weight_utils.py -> safetensors_weights_iterator() [3.7]
INFO 01-25 02:23:21 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.input_layernorm.weight'
INFO 01-25 02:23:21 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.20.input_layernorm.weight']
INFO 01-25 02:23:21 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:21 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:21 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:21 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:21 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:21 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:21 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:21 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:21 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:21 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:21 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:21 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:21 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:21 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:21 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:21 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:21 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:21 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:21 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.down_proj.weight'
INFO 01-25 02:23:21 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.20.mlp.down_proj.weight']
INFO 01-25 02:23:21 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:21 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:21 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:21 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:21 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:21 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:21 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:21 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:21 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:21 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:21 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:21 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:21 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:21 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:21 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:21 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f0a0>]
INFO 01-25 02:23:21 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:21 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:21 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:21 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:21 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f0a0>]
INFO 01-25 02:23:21 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:21 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:21 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.up_proj.weight'
INFO 01-25 02:23:21 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.20.mlp.up_proj.weight']
INFO 01-25 02:23:21 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:21 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:21 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:21 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:21 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:21 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:21 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:21 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:21 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:21 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:21 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:21 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:21 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:21 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:21 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:21 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:21 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:21 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:21 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:21 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:21 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:21 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:21 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:21 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:21 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:21 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:21 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:21 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:21 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:23:22 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:22 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:22 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:22 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:22 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:22 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:22 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:22 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:22 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f010>]
INFO 01-25 02:23:22 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:22 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:22 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:22 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f010>]
INFO 01-25 02:23:22 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:22 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:22 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.post_attention_layernorm.weight'
INFO 01-25 02:23:22 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.20.post_attention_layernorm.weight']
INFO 01-25 02:23:22 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:22 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:22 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:22 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:22 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:22 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:22 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:22 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:22 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:22 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:22 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:22 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:22 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:22 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:22 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:22 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:22 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:22 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:22 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.input_layernorm.weight'
INFO 01-25 02:23:22 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.21.input_layernorm.weight']
INFO 01-25 02:23:22 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:22 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:22 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:22 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:22 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:22 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:22 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:22 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:22 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:22 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:22 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:22 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:22 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:22 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:22 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:22 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:22 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:22 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:22 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.down_proj.weight'
INFO 01-25 02:23:22 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.21.mlp.down_proj.weight']
INFO 01-25 02:23:22 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:22 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:22 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:22 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:22 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:22 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:22 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:22 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:22 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:22 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:22 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:22 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:22 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:22 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:22 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:22 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f370>]
INFO 01-25 02:23:22 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:22 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:22 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:22 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:22 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f370>]
INFO 01-25 02:23:22 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:22 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:22 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.gate_proj.weight'
INFO 01-25 02:23:22 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.21.mlp.gate_proj.weight']
INFO 01-25 02:23:22 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:22 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:22 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:22 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:22 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:22 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:22 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:22 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:22 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:22 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:22 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:22 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:22 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:22 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:22 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:22 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:22 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:22 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:22 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:22 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:22 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:22 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:22 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:22 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:22 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:22 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:22 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:22 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:22 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:22 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:22 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f2e0>]
INFO 01-25 02:23:22 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:22 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:22 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:22 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f2e0>]
INFO 01-25 02:23:22 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:22 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:22 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.mlp.up_proj.weight'
INFO 01-25 02:23:22 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.21.mlp.up_proj.weight']
INFO 01-25 02:23:22 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:22 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:22 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:22 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:22 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:22 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:22 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:22 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:22 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:22 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:22 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:22 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:22 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:22 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:22 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:22 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:22 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:22 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:22 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:22 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:22 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:22 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:22 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:22 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:22 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:22 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:22 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:22 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:22 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:22 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:23 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:23 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:23 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f2e0>]
INFO 01-25 02:23:23 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:23 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:23 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:23 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:23 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f2e0>]
INFO 01-25 02:23:23 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:23 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:23 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.post_attention_layernorm.weight'
INFO 01-25 02:23:23 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.21.post_attention_layernorm.weight']
INFO 01-25 02:23:23 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:23 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:23 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:23 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:23 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:23 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:23 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:23 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:23 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:23 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:23 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:23 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:23 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:23 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:23 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:23 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:23 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:23 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:23 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.k_proj.weight'
INFO 01-25 02:23:23 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.21.self_attn.k_proj.weight']
INFO 01-25 02:23:23 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:23 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:23 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:23 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:23 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:23 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:23 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:23 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:23 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:23 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:23 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:23 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:23 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:23 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:23 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:23 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:23 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:23 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:23 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:23 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:23 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:23 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:23 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:23 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:23 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:23 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb00>]
INFO 01-25 02:23:23 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:23 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:23 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:23 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb00>]
INFO 01-25 02:23:23 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:23 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:23 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.o_proj.weight'
INFO 01-25 02:23:23 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.21.self_attn.o_proj.weight']
INFO 01-25 02:23:23 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:23 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:23 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:23 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:23 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:23 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:23 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:23 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:23 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:23 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:23 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:23 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:23 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:23 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:23 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f130>]
INFO 01-25 02:23:23 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:23 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:23 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:23 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f130>]
INFO 01-25 02:23:23 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:23 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:23 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.q_proj.weight'
INFO 01-25 02:23:23 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.21.self_attn.q_proj.weight']
INFO 01-25 02:23:23 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:23 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:23 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:23 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:23 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:23 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:23 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:23 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:23 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:23 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:23 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:23 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:23 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:23 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:23 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:23 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:23 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:23 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:23 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:23 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:23 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:23 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:23 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb00>]
INFO 01-25 02:23:23 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:23 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:23 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:23 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb00>]
INFO 01-25 02:23:23 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:23 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:23 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.21.self_attn.v_proj.weight'
INFO 01-25 02:23:23 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.21.self_attn.v_proj.weight']
INFO 01-25 02:23:23 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:23 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.21.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:23 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:23 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:23 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:23 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:23 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:23 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:23 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:23 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:23 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:23 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:23 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:23 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:23 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:23 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:23 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:23 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:23 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:23 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:23 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:23 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:23 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:23 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:23 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:23 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb00>]
INFO 01-25 02:23:23 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:23 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:23 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:23 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:23 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb00>]
INFO 01-25 02:23:23 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:23 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:23 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.input_layernorm.weight'
INFO 01-25 02:23:23 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.22.input_layernorm.weight']
INFO 01-25 02:23:23 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:23 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:23 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:23 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:23 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:23 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:23 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:23 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:23 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:23 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:23 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:23 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:23 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:23 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:23 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:23 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:23 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:23 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:23 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.down_proj.weight'
INFO 01-25 02:23:23 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.22.mlp.down_proj.weight']
INFO 01-25 02:23:23 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:23 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:23 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:23 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:23 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:23 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:23 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:23 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:23 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:23 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:23 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:23 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:23 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:23 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:23 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f520>]
INFO 01-25 02:23:23 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:23 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:23 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:23 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:23 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f520>]
INFO 01-25 02:23:23 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:23 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:23 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.gate_proj.weight'
INFO 01-25 02:23:23 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.22.mlp.gate_proj.weight']
INFO 01-25 02:23:23 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:23 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:23 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:23 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:23 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:23 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:23 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:23 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:23 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:23 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:23 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:23 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:23 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:23 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:23 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:23 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:23 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:23 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:23 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:23 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:23 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:23 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:23 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:23 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:23 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:23 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:23 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:23 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:23 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:23 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:23 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:23 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:24 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:24 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f490>]
INFO 01-25 02:23:24 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:24 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:24 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:24 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:24 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f490>]
INFO 01-25 02:23:24 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:24 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:24 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.mlp.up_proj.weight'
INFO 01-25 02:23:24 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.22.mlp.up_proj.weight']
INFO 01-25 02:23:24 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:24 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:24 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:24 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:24 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:24 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:24 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:24 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:24 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:24 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:24 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:24 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:24 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:24 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:24 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:24 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:24 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:24 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:24 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:24 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:24 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:24 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:24 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:24 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:24 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:24 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:24 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:24 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f490>]
INFO 01-25 02:23:24 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:24 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:24 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:24 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:24 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f490>]
INFO 01-25 02:23:24 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:24 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:24 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.post_attention_layernorm.weight'
INFO 01-25 02:23:24 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.22.post_attention_layernorm.weight']
INFO 01-25 02:23:24 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:24 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:24 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:24 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:24 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:24 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:24 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:24 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:24 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:24 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:24 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:24 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:24 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:24 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:24 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:24 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:24 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:24 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:24 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.k_proj.weight'
INFO 01-25 02:23:24 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.22.self_attn.k_proj.weight']
INFO 01-25 02:23:24 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:24 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:24 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:24 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:24 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:24 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:24 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:24 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:24 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:24 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:24 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:24 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:24 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:24 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:24 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:24 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:24 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:24 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:24 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:24 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:24 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:24 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:24 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:24 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f640>]
INFO 01-25 02:23:24 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:24 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:24 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:24 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f640>]
INFO 01-25 02:23:24 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:24 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:24 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.o_proj.weight'
INFO 01-25 02:23:24 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.22.self_attn.o_proj.weight']
INFO 01-25 02:23:24 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:24 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:24 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:24 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:24 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:24 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:24 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:24 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:24 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:24 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:24 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:24 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:24 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:24 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:24 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f6d0>]
INFO 01-25 02:23:24 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:24 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:24 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:24 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f6d0>]
INFO 01-25 02:23:24 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:24 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:24 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.q_proj.weight'
INFO 01-25 02:23:24 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.22.self_attn.q_proj.weight']
INFO 01-25 02:23:24 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:24 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:24 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:24 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:24 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:24 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:24 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:24 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:24 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:24 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:24 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:24 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:24 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:24 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:24 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:24 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:24 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:24 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:24 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:24 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:24 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f640>]
INFO 01-25 02:23:24 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:24 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:24 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:24 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f640>]
INFO 01-25 02:23:24 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:24 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:24 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.22.self_attn.v_proj.weight'
INFO 01-25 02:23:24 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.22.self_attn.v_proj.weight']
INFO 01-25 02:23:24 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:24 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.22.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:24 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:24 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:24 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:24 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:24 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:24 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:24 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:24 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:24 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:24 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:24 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:24 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:24 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:24 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:24 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:24 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:24 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:24 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:24 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:24 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:24 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:24 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:24 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:24 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f640>]
INFO 01-25 02:23:24 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:24 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:24 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:24 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:24 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f640>]
INFO 01-25 02:23:24 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:24 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:24 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.input_layernorm.weight'
INFO 01-25 02:23:24 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.23.input_layernorm.weight']
INFO 01-25 02:23:24 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:24 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:24 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:24 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:24 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:24 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:24 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:24 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:24 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:24 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:24 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:24 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:24 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:24 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:24 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:24 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:24 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:24 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:24 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.down_proj.weight'
INFO 01-25 02:23:24 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.23.mlp.down_proj.weight']
INFO 01-25 02:23:24 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:24 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:24 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:24 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:24 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:24 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:24 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:24 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:24 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:24 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:24 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:24 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:24 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:24 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:24 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:24 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:24 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:24 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:24 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:25 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:25 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:25 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:25 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:25 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:25 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f7f0>]
INFO 01-25 02:23:25 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:25 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:25 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:25 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:25 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f7f0>]
INFO 01-25 02:23:25 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:25 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:25 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.gate_proj.weight'
INFO 01-25 02:23:25 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.23.mlp.gate_proj.weight']
INFO 01-25 02:23:25 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:25 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:25 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:25 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:25 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:25 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:25 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:25 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:25 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:25 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:25 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:25 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:25 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:25 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:25 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:25 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:25 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:25 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:25 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:25 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:25 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:25 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:25 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:25 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:25 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:25 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:25 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f760>]
INFO 01-25 02:23:25 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:25 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:25 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:25 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f760>]
INFO 01-25 02:23:25 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:25 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:25 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.mlp.up_proj.weight'
INFO 01-25 02:23:25 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.23.mlp.up_proj.weight']
INFO 01-25 02:23:25 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:25 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:25 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:25 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:25 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:25 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:25 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:25 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:25 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:25 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:25 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:25 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:25 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:25 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:25 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:25 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:25 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:25 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:25 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:25 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:25 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:25 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:25 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:25 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:25 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:25 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:25 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f760>]
INFO 01-25 02:23:25 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:25 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:25 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:25 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:25 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f760>]
INFO 01-25 02:23:25 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:25 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:25 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.post_attention_layernorm.weight'
INFO 01-25 02:23:25 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.23.post_attention_layernorm.weight']
INFO 01-25 02:23:25 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:25 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:25 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:25 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:25 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:25 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:25 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:25 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:25 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:25 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:25 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:25 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:25 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:25 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:25 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:25 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:25 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:25 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:25 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.k_proj.weight'
INFO 01-25 02:23:25 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.23.self_attn.k_proj.weight']
INFO 01-25 02:23:25 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:25 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:25 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:25 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:25 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:25 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:25 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:25 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:25 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:25 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:25 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:25 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:25 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:25 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:25 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:25 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:25 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:25 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:25 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:25 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:25 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:25 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:25 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:25 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:25 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:25 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:25 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:25 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:25 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ef80>]
INFO 01-25 02:23:25 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:25 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:25 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:25 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:25 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ef80>]
INFO 01-25 02:23:25 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:25 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:25 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.o_proj.weight'
INFO 01-25 02:23:25 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.23.self_attn.o_proj.weight']
INFO 01-25 02:23:25 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:25 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:25 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:25 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:25 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:25 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:25 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:25 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:25 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:25 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:25 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:25 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:25 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:25 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:25 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:25 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f5b0>]
INFO 01-25 02:23:25 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:25 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:25 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:25 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f5b0>]
INFO 01-25 02:23:25 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:25 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:25 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.q_proj.weight'
INFO 01-25 02:23:25 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.23.self_attn.q_proj.weight']
INFO 01-25 02:23:25 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:25 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:25 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:25 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:25 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:25 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:25 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:25 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:25 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:25 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:25 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:25 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:25 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:25 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:25 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:25 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:25 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:25 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:25 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:25 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:26 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:26 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:26 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:26 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:26 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:26 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:26 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ef80>]
INFO 01-25 02:23:26 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:26 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:26 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:26 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ef80>]
INFO 01-25 02:23:26 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 15992799744}]
INFO 01-25 02:23:26 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:26 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.23.self_attn.v_proj.weight'
INFO 01-25 02:23:26 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.23.self_attn.v_proj.weight']
INFO 01-25 02:23:26 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:26 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.23.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:26 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:26 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:26 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:26 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:26 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:26 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:26 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:26 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:26 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:26 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:26 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:26 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:26 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:26 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:26 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:26 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:26 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:26 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:26 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:26 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:26 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:26 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ef80>]
INFO 01-25 02:23:26 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:26 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:26 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:26 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ef80>]
INFO 01-25 02:23:26 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:26 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:26 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.input_layernorm.weight'
INFO 01-25 02:23:26 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.24.input_layernorm.weight']
INFO 01-25 02:23:26 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:26 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:26 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:26 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:26 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:26 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:26 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:26 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:26 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:26 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:26 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:26 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:26 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:26 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:26 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:26 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:26 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:26 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:26 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.down_proj.weight'
INFO 01-25 02:23:26 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.24.mlp.down_proj.weight']
INFO 01-25 02:23:26 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:26 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:26 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:26 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:26 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:26 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:26 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:26 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:26 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:26 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:26 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:26 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:26 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:26 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:26 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f9a0>]
INFO 01-25 02:23:26 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:26 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:26 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:26 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:26 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f9a0>]
INFO 01-25 02:23:26 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:26 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:26 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.gate_proj.weight'
INFO 01-25 02:23:26 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.24.mlp.gate_proj.weight']
INFO 01-25 02:23:26 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:26 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:26 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:26 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:26 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:26 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:26 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:26 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:26 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:26 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:26 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:26 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:26 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:26 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:26 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:26 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:26 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:26 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:26 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:26 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:26 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:26 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:26 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:26 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f910>]
INFO 01-25 02:23:26 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:26 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:26 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:26 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f910>]
INFO 01-25 02:23:26 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:26 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:26 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.mlp.up_proj.weight'
INFO 01-25 02:23:26 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.24.mlp.up_proj.weight']
INFO 01-25 02:23:26 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:26 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:26 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:26 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:26 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:26 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:26 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:26 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:26 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:26 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:26 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:26 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:26 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:26 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:26 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:26 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:26 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:26 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:26 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:26 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:26 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:26 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:26 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:26 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:26 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:26 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f910>]
INFO 01-25 02:23:26 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:26 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:26 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:26 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:26 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f910>]
INFO 01-25 02:23:26 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:26 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:26 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.post_attention_layernorm.weight'
INFO 01-25 02:23:26 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.24.post_attention_layernorm.weight']
INFO 01-25 02:23:26 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:26 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:26 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:26 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:26 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:26 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:26 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:26 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:26 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:26 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:26 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:26 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:26 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:26 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:26 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:26 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:26 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:26 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:26 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.k_proj.weight'
INFO 01-25 02:23:26 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.24.self_attn.k_proj.weight']
INFO 01-25 02:23:26 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:26 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:26 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:26 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:26 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:26 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:26 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:26 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:26 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:26 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:26 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:26 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:26 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:26 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:26 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:26 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:26 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:26 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:26 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:26 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:26 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:26 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:26 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:27 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:27 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:27 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:27 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:27 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:27 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fac0>]
INFO 01-25 02:23:27 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:27 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:27 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:27 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fac0>]
INFO 01-25 02:23:27 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:27 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:27 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.o_proj.weight'
INFO 01-25 02:23:27 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.24.self_attn.o_proj.weight']
INFO 01-25 02:23:27 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:27 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:27 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:27 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:27 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:27 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:27 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:27 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:27 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:27 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:27 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:27 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:27 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:27 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:27 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fb50>]
INFO 01-25 02:23:27 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:27 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:27 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:27 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fb50>]
INFO 01-25 02:23:27 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:27 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:27 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.q_proj.weight'
INFO 01-25 02:23:27 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.24.self_attn.q_proj.weight']
INFO 01-25 02:23:27 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:27 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:27 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:27 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:27 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:27 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:27 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:27 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:27 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:27 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:27 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:27 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:27 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:27 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:27 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:27 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:27 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:27 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:27 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:27 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:27 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:27 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:27 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fac0>]
INFO 01-25 02:23:27 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:27 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:27 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:27 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fac0>]
INFO 01-25 02:23:27 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:27 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:27 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.24.self_attn.v_proj.weight'
INFO 01-25 02:23:27 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.24.self_attn.v_proj.weight']
INFO 01-25 02:23:27 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:27 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.24.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:27 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:27 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:27 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:27 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:27 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:27 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:27 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:27 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:27 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:27 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:27 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:27 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:27 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:27 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:27 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:27 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:27 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:27 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:27 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:27 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:27 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:27 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:27 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:27 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fac0>]
INFO 01-25 02:23:27 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:27 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:27 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:27 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:27 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fac0>]
INFO 01-25 02:23:27 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:27 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:27 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.input_layernorm.weight'
INFO 01-25 02:23:27 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.25.input_layernorm.weight']
INFO 01-25 02:23:27 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:27 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:27 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:27 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:27 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:27 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:27 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:27 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:27 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:27 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:27 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:27 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:27 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:27 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:27 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:27 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:27 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:27 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:27 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.down_proj.weight'
INFO 01-25 02:23:27 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.25.mlp.down_proj.weight']
INFO 01-25 02:23:27 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:27 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:27 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:27 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:27 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:27 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:27 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:27 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:27 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:27 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:27 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:27 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:27 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:27 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:27 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fc70>]
INFO 01-25 02:23:27 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:27 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:27 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:27 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:27 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fc70>]
INFO 01-25 02:23:27 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:27 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:27 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.gate_proj.weight'
INFO 01-25 02:23:27 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.25.mlp.gate_proj.weight']
INFO 01-25 02:23:27 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:27 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:27 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:27 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:27 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:27 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:27 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:27 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:27 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:27 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:27 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:27 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:27 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:27 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:27 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:27 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:27 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:27 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:27 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:27 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:27 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:27 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:27 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:27 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:27 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:27 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:27 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:27 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:27 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:27 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:27 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:27 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:28 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:28 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fbe0>]
INFO 01-25 02:23:28 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:28 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:28 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:28 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fbe0>]
INFO 01-25 02:23:28 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:28 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:28 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.mlp.up_proj.weight'
INFO 01-25 02:23:28 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.25.mlp.up_proj.weight']
INFO 01-25 02:23:28 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:28 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:28 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:28 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:28 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:28 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:28 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:28 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:28 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:28 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:28 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:28 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:28 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:28 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:28 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:28 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:28 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:28 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:28 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:28 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fbe0>]
INFO 01-25 02:23:28 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:28 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:28 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:28 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fbe0>]
INFO 01-25 02:23:28 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:28 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:28 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.post_attention_layernorm.weight'
INFO 01-25 02:23:28 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.25.post_attention_layernorm.weight']
INFO 01-25 02:23:28 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:28 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:28 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:28 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:28 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:28 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:28 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:28 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:28 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:28 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:28 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:28 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:28 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:28 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:28 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:28 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:28 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:28 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:28 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.k_proj.weight'
INFO 01-25 02:23:28 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.25.self_attn.k_proj.weight']
INFO 01-25 02:23:28 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:28 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:28 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:28 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:28 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:28 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:28 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:28 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:28 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:28 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:28 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:28 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:28 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:28 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:28 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:28 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:28 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:28 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:28 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:28 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f400>]
INFO 01-25 02:23:28 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:28 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:28 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:28 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f400>]
INFO 01-25 02:23:28 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:28 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:28 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.o_proj.weight'
INFO 01-25 02:23:28 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.25.self_attn.o_proj.weight']
INFO 01-25 02:23:28 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:28 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:28 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:28 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:28 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:28 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:28 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:28 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:28 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:28 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:28 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:28 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:28 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:28 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:28 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fa30>]
INFO 01-25 02:23:28 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:28 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:28 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:28 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fa30>]
INFO 01-25 02:23:28 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:28 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:28 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.q_proj.weight'
INFO 01-25 02:23:28 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.25.self_attn.q_proj.weight']
INFO 01-25 02:23:28 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:28 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:28 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:28 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:28 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:28 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:28 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:28 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:28 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:28 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:28 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:28 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:28 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:28 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:28 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:28 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:28 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f400>]
INFO 01-25 02:23:28 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:28 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:28 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:28 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f400>]
INFO 01-25 02:23:28 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:28 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:28 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.25.self_attn.v_proj.weight'
INFO 01-25 02:23:28 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.25.self_attn.v_proj.weight']
INFO 01-25 02:23:28 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:28 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.25.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:28 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:28 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:28 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:28 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:28 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:28 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:28 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:28 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:28 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:28 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:28 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:28 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:28 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:28 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:28 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:28 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:28 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:28 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f400>]
INFO 01-25 02:23:28 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:28 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:28 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:28 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:28 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f400>]
INFO 01-25 02:23:28 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:28 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:28 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.input_layernorm.weight'
INFO 01-25 02:23:28 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.26.input_layernorm.weight']
INFO 01-25 02:23:28 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:28 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:28 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:28 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:28 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:28 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:28 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:28 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:28 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:28 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:28 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:28 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:28 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:28 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:28 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:28 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:28 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:28 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:28 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.down_proj.weight'
INFO 01-25 02:23:28 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.26.mlp.down_proj.weight']
INFO 01-25 02:23:28 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:28 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:28 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:28 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:28 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:28 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:28 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:28 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:28 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:28 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:28 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:28 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:28 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:28 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:28 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fe20>]
INFO 01-25 02:23:28 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:28 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:28 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:28 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:28 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fe20>]
INFO 01-25 02:23:28 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:28 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:28 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.gate_proj.weight'
INFO 01-25 02:23:28 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.26.mlp.gate_proj.weight']
INFO 01-25 02:23:28 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:28 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:28 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:28 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:28 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:28 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:28 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:28 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:28 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:28 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:28 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:28 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:28 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:28 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:28 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:28 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:28 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:28 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:28 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:28 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:28 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:28 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:28 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:28 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:29 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:29 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:29 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:29 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:29 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:29 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:29 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:29 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:29 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fd90>]
INFO 01-25 02:23:29 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:29 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:29 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:29 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:29 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fd90>]
INFO 01-25 02:23:29 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:29 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:29 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.mlp.up_proj.weight'
INFO 01-25 02:23:29 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.26.mlp.up_proj.weight']
INFO 01-25 02:23:29 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:29 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:29 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:29 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:29 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:29 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:29 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:29 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:29 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:29 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:29 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:29 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:29 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:29 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:29 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:29 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:29 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:29 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:29 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:29 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:29 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:29 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:29 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:29 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:29 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:29 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:29 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:29 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:29 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:29 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:29 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:29 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:29 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:29 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:29 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:29 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fd90>]
INFO 01-25 02:23:29 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:29 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:29 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:29 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:29 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fd90>]
INFO 01-25 02:23:29 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:29 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:29 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.post_attention_layernorm.weight'
INFO 01-25 02:23:29 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.26.post_attention_layernorm.weight']
INFO 01-25 02:23:29 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:29 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:29 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:29 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:29 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:29 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:29 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:29 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:29 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:29 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:29 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:29 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:29 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:29 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:29 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:29 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:29 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:29 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:29 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.k_proj.weight'
INFO 01-25 02:23:29 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.26.self_attn.k_proj.weight']
INFO 01-25 02:23:29 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:29 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:29 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:29 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:29 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:29 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:29 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:29 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:29 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:29 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:29 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:29 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:29 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:29 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:29 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:29 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:29 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:29 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:29 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:29 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:29 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:29 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:29 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:29 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:29 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:29 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:29 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:29 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:29 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:29 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:29 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:29 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ff40>]
INFO 01-25 02:23:29 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:29 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:29 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:29 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:29 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ff40>]
INFO 01-25 02:23:29 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:29 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:29 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.o_proj.weight'
INFO 01-25 02:23:29 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.26.self_attn.o_proj.weight']
INFO 01-25 02:23:29 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:29 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:29 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:29 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:29 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:29 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:29 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:29 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:29 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:29 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:29 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:29 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:29 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:29 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:29 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:29 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8feb0>]
INFO 01-25 02:23:29 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:29 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:29 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:29 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8feb0>]
INFO 01-25 02:23:29 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:29 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:29 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.q_proj.weight'
INFO 01-25 02:23:29 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.26.self_attn.q_proj.weight']
INFO 01-25 02:23:29 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:29 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:29 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:29 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:29 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:29 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:29 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:29 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:29 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:29 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:29 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:29 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:29 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:29 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:29 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:29 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:29 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:29 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:29 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:29 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:30 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:30 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:30 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:30 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:30 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:30 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:30 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ff40>]
INFO 01-25 02:23:30 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:30 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:30 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:30 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ff40>]
INFO 01-25 02:23:30 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:30 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:30 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.26.self_attn.v_proj.weight'
INFO 01-25 02:23:30 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.26.self_attn.v_proj.weight']
INFO 01-25 02:23:30 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:30 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.26.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:30 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:30 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:30 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:30 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:30 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:30 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:30 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:30 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:30 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:30 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:30 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:30 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:30 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:30 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:30 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:30 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:30 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:30 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:30 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:30 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:30 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:30 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ff40>]
INFO 01-25 02:23:30 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:30 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:30 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:30 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ff40>]
INFO 01-25 02:23:30 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:30 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:30 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.input_layernorm.weight'
INFO 01-25 02:23:30 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.27.input_layernorm.weight']
INFO 01-25 02:23:30 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:30 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:30 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:30 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:30 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:30 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:30 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:30 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:30 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:30 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:30 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:30 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:30 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:30 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:30 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:30 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:30 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:30 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:30 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.down_proj.weight'
INFO 01-25 02:23:30 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.27.mlp.down_proj.weight']
INFO 01-25 02:23:30 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:30 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:30 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:30 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:30 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:30 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:30 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:30 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:30 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:30 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:30 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:30 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:30 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:30 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:30 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360160>]
INFO 01-25 02:23:30 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:30 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:30 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:30 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:30 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360160>]
INFO 01-25 02:23:30 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:30 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:30 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.gate_proj.weight'
INFO 01-25 02:23:30 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.27.mlp.gate_proj.weight']
INFO 01-25 02:23:30 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:30 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:30 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:30 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:30 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:30 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:30 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:30 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:30 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:30 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:30 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:30 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:30 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:30 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:30 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:30 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:30 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:30 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:30 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:30 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:30 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:30 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:30 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:30 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360040>]
INFO 01-25 02:23:30 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:30 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:30 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:30 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360040>]
INFO 01-25 02:23:30 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:30 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:30 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.mlp.up_proj.weight'
INFO 01-25 02:23:30 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.27.mlp.up_proj.weight']
INFO 01-25 02:23:30 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:30 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:30 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:30 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:30 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:30 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:30 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:30 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:30 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:30 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:30 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:30 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:30 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:30 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:30 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:30 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:30 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:30 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:30 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:30 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:30 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:30 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:30 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:30 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:30 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:30 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360040>]
INFO 01-25 02:23:30 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:30 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:30 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:30 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:30 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360040>]
INFO 01-25 02:23:30 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:30 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:30 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.post_attention_layernorm.weight'
INFO 01-25 02:23:30 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.27.post_attention_layernorm.weight']
INFO 01-25 02:23:30 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:30 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:30 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:30 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:30 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:30 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:30 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:30 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:30 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:30 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:30 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:30 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:30 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:30 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:30 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:30 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:30 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 14540795904}]
INFO 01-25 02:23:30 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:30 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.k_proj.weight'
INFO 01-25 02:23:30 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.27.self_attn.k_proj.weight']
INFO 01-25 02:23:30 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:30 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:30 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:30 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:30 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:30 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:30 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:30 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:30 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:30 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:30 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:30 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:30 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:30 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:30 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:30 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:30 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:30 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:30 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:30 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:30 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:30 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:30 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:31 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:31 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:31 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:31 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:31 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:31 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:31 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:31 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f880>]
INFO 01-25 02:23:31 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:31 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:31 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:31 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f880>]
INFO 01-25 02:23:31 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:31 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:31 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.o_proj.weight'
INFO 01-25 02:23:31 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.27.self_attn.o_proj.weight']
INFO 01-25 02:23:31 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:31 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:31 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:31 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:31 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:31 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:31 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:31 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:31 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:31 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:31 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:31 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:31 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:31 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:31 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:31 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dbd0>]
INFO 01-25 02:23:31 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:31 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:31 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:31 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dbd0>]
INFO 01-25 02:23:31 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:31 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:31 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.q_proj.weight'
INFO 01-25 02:23:31 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.27.self_attn.q_proj.weight']
INFO 01-25 02:23:31 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:31 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:31 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:31 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:31 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:31 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:31 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:31 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:31 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:31 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:31 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:31 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:31 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:31 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:31 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:31 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:31 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:31 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:31 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:31 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:31 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:31 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:31 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:31 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:31 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f880>]
INFO 01-25 02:23:31 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:31 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:31 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:31 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f880>]
INFO 01-25 02:23:31 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:31 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:31 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.27.self_attn.v_proj.weight'
INFO 01-25 02:23:31 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.27.self_attn.v_proj.weight']
INFO 01-25 02:23:31 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:31 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.27.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:31 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:31 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:31 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:31 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:31 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:31 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:31 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:31 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:31 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:31 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:31 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:31 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:31 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:31 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:31 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:31 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:31 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:31 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:31 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:31 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:31 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:31 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:31 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:31 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:31 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:31 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f880>]
INFO 01-25 02:23:31 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:31 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:31 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:31 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:31 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f880>]
INFO 01-25 02:23:31 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:31 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:31 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.input_layernorm.weight'
INFO 01-25 02:23:31 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.28.input_layernorm.weight']
INFO 01-25 02:23:31 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:31 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:31 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:31 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:31 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:31 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:31 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:31 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:31 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:31 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:31 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:31 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:31 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:31 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:31 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:31 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:31 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:31 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:31 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.down_proj.weight'
INFO 01-25 02:23:31 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.28.mlp.down_proj.weight']
INFO 01-25 02:23:31 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:31 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:31 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:31 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:31 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:31 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:31 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:31 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:31 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:31 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:31 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:31 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:31 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:31 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:31 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:31 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cee0>]
INFO 01-25 02:23:31 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:31 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:31 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:31 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:31 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cee0>]
INFO 01-25 02:23:31 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:31 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:31 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.gate_proj.weight'
INFO 01-25 02:23:31 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.28.mlp.gate_proj.weight']
INFO 01-25 02:23:31 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:31 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:31 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:31 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:31 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:31 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:31 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:31 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:31 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:31 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:31 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:31 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:31 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:31 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:31 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:31 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:31 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:31 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:31 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:31 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:31 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:31 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:31 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:31 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:31 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:31 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:31 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:31 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:31 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:32 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:32 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:32 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:32 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:32 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:23:32 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:32 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:32 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:32 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:32 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e320>]
INFO 01-25 02:23:32 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:32 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:32 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:32 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:32 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e320>]
INFO 01-25 02:23:32 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:32 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:32 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.mlp.up_proj.weight'
INFO 01-25 02:23:32 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.28.mlp.up_proj.weight']
INFO 01-25 02:23:32 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:32 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:32 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:32 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:32 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:32 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:32 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:32 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:32 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:32 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:32 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:32 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:32 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:32 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:32 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:32 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:32 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:32 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:32 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:32 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:32 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:32 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:32 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:32 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:32 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:32 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:32 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:32 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:32 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:32 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:32 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:32 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:32 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:32 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:32 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:32 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e320>]
INFO 01-25 02:23:32 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:32 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:32 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:32 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:32 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e320>]
INFO 01-25 02:23:32 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:32 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:32 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.post_attention_layernorm.weight'
INFO 01-25 02:23:32 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.28.post_attention_layernorm.weight']
INFO 01-25 02:23:32 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:32 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:32 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:32 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:32 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:32 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:32 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:32 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:32 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:32 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:32 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:32 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:32 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:32 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:32 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:32 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:32 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:32 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:32 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.k_proj.weight'
INFO 01-25 02:23:32 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.28.self_attn.k_proj.weight']
INFO 01-25 02:23:32 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:32 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:32 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:32 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:32 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:32 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:32 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:32 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:32 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:32 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:32 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:32 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:32 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:32 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:32 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:32 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:32 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:32 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:32 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:32 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:32 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:32 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:32 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:32 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:32 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:32 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:32 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:32 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:32 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:32 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:32 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:32 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360430>]
INFO 01-25 02:23:32 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:32 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:32 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:32 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:32 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360430>]
INFO 01-25 02:23:32 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:32 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:32 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.o_proj.weight'
INFO 01-25 02:23:32 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.28.self_attn.o_proj.weight']
INFO 01-25 02:23:32 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:32 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:32 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:32 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:32 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:32 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:32 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:32 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:32 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:32 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:32 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:32 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:32 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:32 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:32 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:32 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3604c0>]
INFO 01-25 02:23:32 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:32 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:32 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:32 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3604c0>]
INFO 01-25 02:23:32 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:32 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:32 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.q_proj.weight'
INFO 01-25 02:23:32 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.28.self_attn.q_proj.weight']
INFO 01-25 02:23:32 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:32 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:32 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:32 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:32 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:32 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:32 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:32 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:32 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:32 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:32 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:32 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:32 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:32 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:32 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:32 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:32 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:32 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:32 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:32 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:33 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:33 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:33 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:33 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:33 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:33 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:33 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360430>]
INFO 01-25 02:23:33 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:33 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:33 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:33 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360430>]
INFO 01-25 02:23:33 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:33 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:33 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.28.self_attn.v_proj.weight'
INFO 01-25 02:23:33 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.28.self_attn.v_proj.weight']
INFO 01-25 02:23:33 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:33 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.28.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:33 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:33 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:33 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:33 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:33 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:33 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:33 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:33 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:33 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:33 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:33 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:33 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:33 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:33 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:33 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:33 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:33 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:33 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:33 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:33 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:33 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:33 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360430>]
INFO 01-25 02:23:33 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:33 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:33 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:33 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360430>]
INFO 01-25 02:23:33 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:33 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:33 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.input_layernorm.weight'
INFO 01-25 02:23:33 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.29.input_layernorm.weight']
INFO 01-25 02:23:33 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:33 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:33 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:33 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:33 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:33 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:33 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:33 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:33 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:33 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:33 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:33 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:33 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:33 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:33 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:33 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:33 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:33 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:33 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.down_proj.weight'
INFO 01-25 02:23:33 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.29.mlp.down_proj.weight']
INFO 01-25 02:23:33 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:33 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:33 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:33 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:33 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:33 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:33 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:33 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:33 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:33 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:33 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:33 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:33 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:33 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:33 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3605e0>]
INFO 01-25 02:23:33 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:33 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:33 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:33 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:33 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3605e0>]
INFO 01-25 02:23:33 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:33 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:33 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.gate_proj.weight'
INFO 01-25 02:23:33 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.29.mlp.gate_proj.weight']
INFO 01-25 02:23:33 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:33 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:33 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:33 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:33 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:33 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:33 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:33 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:33 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:33 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:33 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:33 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:33 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:33 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:33 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:33 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:33 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:33 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:33 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:33 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:33 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:33 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:33 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:33 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360550>]
INFO 01-25 02:23:33 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:33 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:33 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:33 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360550>]
INFO 01-25 02:23:33 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:33 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:33 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.mlp.up_proj.weight'
INFO 01-25 02:23:33 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.29.mlp.up_proj.weight']
INFO 01-25 02:23:33 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:33 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:33 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:33 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:33 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:33 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:33 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:33 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:33 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:33 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:33 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:33 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:33 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:33 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:33 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:33 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:33 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:33 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:33 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:33 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:33 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:33 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:33 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:33 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:33 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:33 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360550>]
INFO 01-25 02:23:33 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:33 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:33 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:33 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:33 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360550>]
INFO 01-25 02:23:33 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:33 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:33 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.post_attention_layernorm.weight'
INFO 01-25 02:23:33 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.29.post_attention_layernorm.weight']
INFO 01-25 02:23:33 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:33 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:33 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:33 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:33 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:33 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:33 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:33 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:33 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:33 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:33 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:33 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:33 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:33 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:33 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:33 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:33 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:33 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:33 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.k_proj.weight'
INFO 01-25 02:23:33 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.29.self_attn.k_proj.weight']
INFO 01-25 02:23:33 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:33 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:33 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:33 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:33 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:33 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:33 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:33 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:33 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:33 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:33 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:33 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:33 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:33 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:33 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:33 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:33 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:33 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:33 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:33 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:33 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:33 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:33 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:34 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:34 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:34 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:34 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:34 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:34 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3600d0>]
INFO 01-25 02:23:34 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:34 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:34 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:34 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3600d0>]
INFO 01-25 02:23:34 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:34 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:34 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.o_proj.weight'
INFO 01-25 02:23:34 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.29.self_attn.o_proj.weight']
INFO 01-25 02:23:34 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:34 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:34 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:34 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:34 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:34 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:34 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:34 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:34 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:34 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:34 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:34 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:34 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:34 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:34 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3603a0>]
INFO 01-25 02:23:34 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:34 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:34 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:34 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3603a0>]
INFO 01-25 02:23:34 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:34 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:34 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.q_proj.weight'
INFO 01-25 02:23:34 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.29.self_attn.q_proj.weight']
INFO 01-25 02:23:34 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:34 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:34 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:34 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:34 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:34 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:34 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:34 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:34 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:34 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:34 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:34 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:34 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:34 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:34 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:34 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:34 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:34 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:34 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:34 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:34 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:34 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:34 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3600d0>]
INFO 01-25 02:23:34 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:34 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:34 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:34 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3600d0>]
INFO 01-25 02:23:34 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:34 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:34 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.29.self_attn.v_proj.weight'
INFO 01-25 02:23:34 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.29.self_attn.v_proj.weight']
INFO 01-25 02:23:34 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:34 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.29.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:34 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:34 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:34 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:34 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:34 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:34 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:34 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:34 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:34 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:34 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:34 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:34 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:34 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:34 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:34 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:34 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:34 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:34 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:34 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:34 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:34 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:34 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:34 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:34 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3600d0>]
INFO 01-25 02:23:34 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:34 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:34 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:34 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:34 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3600d0>]
INFO 01-25 02:23:34 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:34 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:34 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.input_layernorm.weight'
INFO 01-25 02:23:34 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.30.input_layernorm.weight']
INFO 01-25 02:23:34 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:34 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:34 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:34 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:34 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:34 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:34 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:34 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:34 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:34 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:34 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:34 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:34 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:34 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:34 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:34 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:34 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:34 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:34 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.down_proj.weight'
INFO 01-25 02:23:34 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.30.mlp.down_proj.weight']
INFO 01-25 02:23:34 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:34 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:34 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:34 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:34 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:34 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:34 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:34 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:34 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:34 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:34 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:34 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:34 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:34 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:34 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fd00>]
INFO 01-25 02:23:34 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:34 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:34 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:34 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:34 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8fd00>]
INFO 01-25 02:23:34 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:34 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:34 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.gate_proj.weight'
INFO 01-25 02:23:34 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.30.mlp.gate_proj.weight']
INFO 01-25 02:23:34 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:34 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:34 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:34 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:34 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:34 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:34 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:34 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:34 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:34 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:34 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:34 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:34 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:34 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:34 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:34 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:34 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:34 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:34 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:34 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:34 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:34 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:34 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:34 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:34 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:34 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:34 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:34 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:34 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:34 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:34 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:34 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:35 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:35 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c940>]
INFO 01-25 02:23:35 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:35 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:35 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:35 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c940>]
INFO 01-25 02:23:35 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:35 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:35 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.mlp.up_proj.weight'
INFO 01-25 02:23:35 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.30.mlp.up_proj.weight']
INFO 01-25 02:23:35 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:35 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:35 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:35 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:35 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:35 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:35 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:35 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:35 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:35 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:35 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:35 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:35 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:35 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:35 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:35 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:35 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:35 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:35 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:35 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c940>]
INFO 01-25 02:23:35 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:35 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:35 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:35 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c940>]
INFO 01-25 02:23:35 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:35 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:35 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.post_attention_layernorm.weight'
INFO 01-25 02:23:35 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.30.post_attention_layernorm.weight']
INFO 01-25 02:23:35 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:35 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:35 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:35 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:35 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:35 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:35 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:35 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:35 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:35 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:35 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:35 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:35 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:35 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:35 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:35 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:35 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:35 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:35 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.k_proj.weight'
INFO 01-25 02:23:35 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.30.self_attn.k_proj.weight']
INFO 01-25 02:23:35 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:35 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:35 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:35 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:35 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:35 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:35 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:35 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:35 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:35 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:35 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:35 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:35 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:35 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:35 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:35 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:35 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:35 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:35 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:35 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3608b0>]
INFO 01-25 02:23:35 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:35 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:35 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:35 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3608b0>]
INFO 01-25 02:23:35 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:35 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:35 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.o_proj.weight'
INFO 01-25 02:23:35 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.30.self_attn.o_proj.weight']
INFO 01-25 02:23:35 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:35 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:35 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:35 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:35 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:35 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:35 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:35 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:35 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:35 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:35 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:35 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:35 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:35 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:35 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360940>]
INFO 01-25 02:23:35 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:35 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:35 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:35 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360940>]
INFO 01-25 02:23:35 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:35 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:35 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.q_proj.weight'
INFO 01-25 02:23:35 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.30.self_attn.q_proj.weight']
INFO 01-25 02:23:35 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:35 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:35 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:35 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:35 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:35 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:35 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:35 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:35 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:35 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:35 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:35 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:35 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:35 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:35 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:35 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:35 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3608b0>]
INFO 01-25 02:23:35 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:35 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:35 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:35 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3608b0>]
INFO 01-25 02:23:35 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:35 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:35 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.30.self_attn.v_proj.weight'
INFO 01-25 02:23:35 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.30.self_attn.v_proj.weight']
INFO 01-25 02:23:35 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:35 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.30.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:35 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:35 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:35 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:35 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:35 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:35 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:35 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:35 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:35 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:35 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:35 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:35 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:35 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:35 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:35 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:35 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:35 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:35 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3608b0>]
INFO 01-25 02:23:35 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:35 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:35 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:35 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:35 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3608b0>]
INFO 01-25 02:23:35 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13087469568}]
INFO 01-25 02:23:35 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:35 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.gate_proj.weight'
INFO 01-25 02:23:35 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.31.mlp.gate_proj.weight']
INFO 01-25 02:23:35 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:35 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:35 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:35 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:35 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:35 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:35 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:35 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:35 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:35 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:35 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:35 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:35 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:35 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:35 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:35 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:35 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:35 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:35 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:35 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:35 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:36 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:36 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:36 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:36 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:36 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:36 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:36 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:36 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:36 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:36 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:36 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:36 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:36 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3609d0>]
INFO 01-25 02:23:36 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:36 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:36 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:36 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:36 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:36 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3609d0>]
INFO 01-25 02:23:36 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11986464768}]
INFO 01-25 02:23:36 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:36 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.mlp.up_proj.weight'
INFO 01-25 02:23:36 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.31.mlp.up_proj.weight']
INFO 01-25 02:23:36 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:36 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:36 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:36 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:36 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:36 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:36 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:36 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:36 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:36 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:36 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:36 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:36 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:36 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:36 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:36 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:36 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:36 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:36 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:36 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:36 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:36 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:36 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:36 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:36 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:36 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:36 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:36 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:36 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:36 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:36 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:36 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:36 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:36 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:36 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:36 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:36 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:36 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3609d0>]
INFO 01-25 02:23:36 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:36 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:36 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:36 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:36 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:36 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d3609d0>]
INFO 01-25 02:23:36 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11986464768}]
INFO 01-25 02:23:36 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:36 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.k_proj.weight'
INFO 01-25 02:23:36 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.31.self_attn.k_proj.weight']
INFO 01-25 02:23:36 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:36 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:36 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:36 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:36 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:36 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:36 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:36 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:36 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:36 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:36 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:36 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:36 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:36 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:36 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:36 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:36 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:36 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:36 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:36 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:36 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:36 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:36 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:36 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:36 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:36 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:36 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:36 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:36 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:36 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:36 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:36 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:36 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:36 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360280>]
INFO 01-25 02:23:36 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:36 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:36 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:36 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:36 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:36 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360280>]
INFO 01-25 02:23:36 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11986464768}]
INFO 01-25 02:23:36 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:36 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.o_proj.weight'
INFO 01-25 02:23:36 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.31.self_attn.o_proj.weight']
INFO 01-25 02:23:36 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:36 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:36 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:36 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:36 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:36 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:36 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:36 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:36 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:36 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:36 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:36 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:36 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:36 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:36 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:36 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:36 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:36 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:36 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:36 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:37 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:37 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:37 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:37 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:37 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:37 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:37 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:37 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360820>]
INFO 01-25 02:23:37 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:37 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:37 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:37 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:37 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:37 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360820>]
INFO 01-25 02:23:37 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11986464768}]
INFO 01-25 02:23:37 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:37 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.q_proj.weight'
INFO 01-25 02:23:37 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.31.self_attn.q_proj.weight']
INFO 01-25 02:23:37 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:37 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:37 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:37 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:37 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:37 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:37 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:37 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:37 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:37 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:37 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:37 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:37 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:37 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:37 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:37 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:37 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:37 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:37 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:37 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:37 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:37 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:37 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:37 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:37 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:37 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:37 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:37 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:37 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:37 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360280>]
INFO 01-25 02:23:37 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:37 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:37 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:37 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:37 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360280>]
INFO 01-25 02:23:37 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11986464768}]
INFO 01-25 02:23:37 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]
INFO 01-25 02:23:37 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.31.self_attn.v_proj.weight'
INFO 01-25 02:23:37 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.31.self_attn.v_proj.weight']
INFO 01-25 02:23:37 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:37 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:37 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.31.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:37 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:37 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:37 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:37 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:37 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:37 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:37 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:37 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:37 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:37 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:37 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:37 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:37 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:37 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:37 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:37 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:37 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:37 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:37 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:37 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:37 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:37 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:37 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:37 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:37 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:37 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:37 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360280>]
INFO 01-25 02:23:37 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:37 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:37 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:37 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:37 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:37 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:37 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14d360280>]
INFO 01-25 02:23:37 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11986464768}]
INFO 01-25 02:23:37 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=3.7]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:24<00:25, 12.98s/it]
INFO 01-25 02:23:37 weight_utils.py:424] hosseins: weight_utils.py -> safetensors_weights_iterator() [{'bytes_limit': 33550237696, 'peak_bytes_used': 11986464768}]
INFO 01-25 02:23:37 weight_utils.py:425] hosseins: weight_utils.py -> safetensors_weights_iterator() [3.7]
INFO 01-25 02:23:37 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.embed_tokens.weight'
INFO 01-25 02:23:37 llama.py:478] hosseins: LlamaModel -> load_weights() [name='embed_tokens.weight']
INFO 01-25 02:23:37 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([128256, 4096])]
INFO 01-25 02:23:37 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:37 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:37 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='embed_tokens.weight'] [loaded_weight.shape=torch.Size([128256, 4096])] [param.shape=torch.Size([128256, 4096])]
INFO 01-25 02:23:37 vocab_parallel_embedding.py:360] hosseins: VocabParallelEmbedding -> weight_loader()
INFO 01-25 02:23:37 utils.py:259] hosseins: returning mesh
INFO 01-25 02:23:38 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:38 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:38 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([128256, 4096])] - dtype='uint16'
INFO 01-25 02:23:38 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=525336576] - dtype_size=2
INFO 01-25 02:23:38 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:38 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:38 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:38 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac06c700>]
INFO 01-25 02:23:38 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:38 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:38 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:38 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:38 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([128256, 4096])]
INFO 01-25 02:23:38 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:38 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac06c700>]
INFO 01-25 02:23:38 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11986464768}]
INFO 01-25 02:23:38 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:38 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.input_layernorm.weight'
INFO 01-25 02:23:38 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.0.input_layernorm.weight']
INFO 01-25 02:23:38 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:38 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:38 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:38 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:38 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:38 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:38 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:38 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:38 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:38 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:38 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:38 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:38 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:38 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:38 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:38 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:38 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:38 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:38 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:38 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:38 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11986464768}]
INFO 01-25 02:23:38 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:38 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.mlp.down_proj.weight'
INFO 01-25 02:23:38 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.0.mlp.down_proj.weight']
INFO 01-25 02:23:38 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:38 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:38 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:38 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:38 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:38 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:38 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:38 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:38 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:38 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:38 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:38 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:38 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:38 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:38 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:38 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:38 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:38 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:38 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:38 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:38 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:38 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:38 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:42 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:23:44 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:44 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:44 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:44 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:44 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:44 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:44 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:44 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff2e0>]
INFO 01-25 02:23:44 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:44 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:44 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:44 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:44 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:44 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:44 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff2e0>]
INFO 01-25 02:23:44 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13588688896}]
INFO 01-25 02:23:44 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:44 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.mlp.gate_proj.weight'
INFO 01-25 02:23:44 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.0.mlp.gate_proj.weight']
INFO 01-25 02:23:44 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:44 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:44 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:44 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:44 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:44 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:44 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:44 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:44 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:44 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:44 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:44 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:44 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:44 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:44 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:44 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:44 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:44 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:44 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:44 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:44 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:44 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:44 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:44 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:44 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:44 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:44 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:44 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:44 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:44 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:44 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:44 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:45 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:45 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:45 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:45 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:45 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:45 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:45 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:45 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:45 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff130>]
INFO 01-25 02:23:45 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:45 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:45 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:45 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:45 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff130>]
INFO 01-25 02:23:45 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13588688896}]
INFO 01-25 02:23:45 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:45 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.mlp.up_proj.weight'
INFO 01-25 02:23:45 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.0.mlp.up_proj.weight']
INFO 01-25 02:23:45 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:45 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:45 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:45 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:45 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:45 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:45 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:45 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:45 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:45 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:45 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:45 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:45 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:45 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:45 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:45 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:45 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:45 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:45 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:45 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:45 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:45 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:45 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:45 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:45 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:45 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:45 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:45 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:45 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:45 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:45 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:45 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:45 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:45 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:45 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:45 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff130>]
INFO 01-25 02:23:45 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:45 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:45 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:45 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:45 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff130>]
INFO 01-25 02:23:45 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13588688896}]
INFO 01-25 02:23:45 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:45 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.post_attention_layernorm.weight'
INFO 01-25 02:23:45 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.0.post_attention_layernorm.weight']
INFO 01-25 02:23:45 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:45 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:45 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:45 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:45 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:45 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:45 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:45 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:45 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:45 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:45 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:45 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:45 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:45 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:45 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:45 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:45 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13588688896}]
INFO 01-25 02:23:45 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:45 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.k_proj.weight'
INFO 01-25 02:23:45 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.0.self_attn.k_proj.weight']
INFO 01-25 02:23:45 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:45 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:45 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:45 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:45 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:45 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:45 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:45 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:45 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:45 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:45 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:45 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:45 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:45 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:45 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:45 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:45 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:45 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:45 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:45 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:45 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:45 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:45 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:45 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:45 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:45 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:45 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:45 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:45 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:45 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:45 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:45 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd19bd0>]
INFO 01-25 02:23:45 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:45 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:45 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:45 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:45 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd19bd0>]
INFO 01-25 02:23:45 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13588688896}]
INFO 01-25 02:23:45 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:45 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.o_proj.weight'
INFO 01-25 02:23:45 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.0.self_attn.o_proj.weight']
INFO 01-25 02:23:45 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:45 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:45 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:45 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:45 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:45 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:45 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:45 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:45 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:45 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:45 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:45 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:45 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:45 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:45 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:45 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:45 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:45 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:45 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:46 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:46 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:46 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:46 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:46 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:46 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd3ff40>]
INFO 01-25 02:23:46 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:46 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:46 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:46 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:46 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd3ff40>]
INFO 01-25 02:23:46 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 13588688896}]
INFO 01-25 02:23:46 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:46 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.q_proj.weight'
INFO 01-25 02:23:46 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.0.self_attn.q_proj.weight']
INFO 01-25 02:23:46 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:46 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:46 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:46 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:46 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:46 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:46 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:46 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:46 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:46 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:46 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:46 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:46 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:46 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:46 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:46 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:46 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:46 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:46 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:46 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:46 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:46 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:46 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:46 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:46 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:46 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd19bd0>]
INFO 01-25 02:23:46 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:46 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:46 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:46 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd19bd0>]
INFO 01-25 02:23:46 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:46 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:46 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.0.self_attn.v_proj.weight'
INFO 01-25 02:23:46 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.0.self_attn.v_proj.weight']
INFO 01-25 02:23:46 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:46 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.0.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:46 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:46 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:46 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:46 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:46 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:46 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:46 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:46 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:46 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:46 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:46 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:46 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:46 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:46 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:46 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:46 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:46 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:46 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:46 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:46 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:46 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:46 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:46 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:46 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd19bd0>]
INFO 01-25 02:23:46 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:46 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:46 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:46 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:46 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd19bd0>]
INFO 01-25 02:23:46 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:46 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:46 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.input_layernorm.weight'
INFO 01-25 02:23:46 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.1.input_layernorm.weight']
INFO 01-25 02:23:46 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:46 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:46 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:46 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:46 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:46 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:46 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:46 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:46 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:46 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:46 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:46 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:46 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:46 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:46 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:46 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:46 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:46 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:46 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.mlp.down_proj.weight'
INFO 01-25 02:23:46 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.1.mlp.down_proj.weight']
INFO 01-25 02:23:46 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:46 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:46 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:46 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:46 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:46 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:46 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:46 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:46 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:46 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:46 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:46 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:46 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:46 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:46 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c700>]
INFO 01-25 02:23:46 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:46 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:46 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:46 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:46 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c700>]
INFO 01-25 02:23:46 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:46 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:46 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.mlp.gate_proj.weight'
INFO 01-25 02:23:46 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.1.mlp.gate_proj.weight']
INFO 01-25 02:23:46 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:46 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:46 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:46 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:46 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:46 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:46 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:46 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:46 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:46 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:46 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:46 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:46 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:46 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:46 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:46 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:46 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:46 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:46 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:46 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:46 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:46 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:46 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:46 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:46 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:46 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:46 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c670>]
INFO 01-25 02:23:46 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:46 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:46 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:46 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c670>]
INFO 01-25 02:23:46 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:46 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:46 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.mlp.up_proj.weight'
INFO 01-25 02:23:46 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.1.mlp.up_proj.weight']
INFO 01-25 02:23:46 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:46 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:46 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:46 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:46 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:46 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:46 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:46 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:46 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:46 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:46 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:46 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:46 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:46 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:46 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:46 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:46 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:46 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:46 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:46 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:46 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:46 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:46 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:46 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:46 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:46 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:46 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:46 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:47 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:47 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:47 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c670>]
INFO 01-25 02:23:47 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:47 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:47 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:47 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:47 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c670>]
INFO 01-25 02:23:47 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:47 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:47 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.post_attention_layernorm.weight'
INFO 01-25 02:23:47 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.1.post_attention_layernorm.weight']
INFO 01-25 02:23:47 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:47 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:47 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:47 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:47 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:47 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:47 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:47 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:47 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:47 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:47 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:47 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:47 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:47 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:47 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:47 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:47 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:47 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:47 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.k_proj.weight'
INFO 01-25 02:23:47 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.1.self_attn.k_proj.weight']
INFO 01-25 02:23:47 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:47 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:47 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:47 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:47 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:47 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:47 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:47 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:47 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:47 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:47 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:47 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:47 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:47 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:47 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:47 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:47 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:47 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:47 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:47 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:47 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:47 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:47 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:47 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:47 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:47 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c550>]
INFO 01-25 02:23:47 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:47 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:47 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:47 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c550>]
INFO 01-25 02:23:47 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:47 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:47 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.o_proj.weight'
INFO 01-25 02:23:47 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.1.self_attn.o_proj.weight']
INFO 01-25 02:23:47 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:47 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:47 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:47 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:47 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:47 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:47 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:47 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:47 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:47 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:47 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:47 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:47 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:47 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:47 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c5e0>]
INFO 01-25 02:23:47 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:47 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:47 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:47 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c5e0>]
INFO 01-25 02:23:47 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:47 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:47 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.q_proj.weight'
INFO 01-25 02:23:47 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.1.self_attn.q_proj.weight']
INFO 01-25 02:23:47 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:47 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:47 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:47 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:47 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:47 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:47 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:47 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:47 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:47 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:47 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:47 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:47 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:47 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:47 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:47 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:47 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:47 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:47 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:47 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:47 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:47 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:47 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c550>]
INFO 01-25 02:23:47 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:47 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:47 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:47 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c550>]
INFO 01-25 02:23:47 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:47 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:47 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.1.self_attn.v_proj.weight'
INFO 01-25 02:23:47 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.1.self_attn.v_proj.weight']
INFO 01-25 02:23:47 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:47 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.1.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:47 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:47 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:47 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:47 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:47 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:47 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:47 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:47 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:47 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:47 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:47 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:47 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:47 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:47 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:47 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:47 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:47 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:47 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:47 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:47 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:47 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:47 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:47 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:47 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c550>]
INFO 01-25 02:23:47 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:47 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:47 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:47 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:47 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c550>]
INFO 01-25 02:23:47 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:47 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:47 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.input_layernorm.weight'
INFO 01-25 02:23:47 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.2.input_layernorm.weight']
INFO 01-25 02:23:47 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:47 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:47 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:47 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:47 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:47 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:47 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:47 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:47 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:47 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:47 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:47 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:47 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:47 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:47 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:47 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:47 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:47 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:47 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.mlp.down_proj.weight'
INFO 01-25 02:23:47 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.2.mlp.down_proj.weight']
INFO 01-25 02:23:47 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:47 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:47 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:47 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:47 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:47 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:47 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:47 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:47 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:47 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:47 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:47 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:47 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:47 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:47 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff490>]
INFO 01-25 02:23:47 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:47 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:47 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:47 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:47 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff490>]
INFO 01-25 02:23:47 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:47 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:47 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.mlp.gate_proj.weight'
INFO 01-25 02:23:47 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.2.mlp.gate_proj.weight']
INFO 01-25 02:23:47 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:47 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:47 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:47 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:47 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:47 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:47 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:47 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:47 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:47 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:47 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:47 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:47 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:47 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:47 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:47 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:47 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:47 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:47 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:47 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:47 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:47 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:47 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:47 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:47 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:47 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:47 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:47 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:47 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:47 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:47 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:47 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:48 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:48 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd3f5b0>]
INFO 01-25 02:23:48 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:48 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:48 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:48 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd3f5b0>]
INFO 01-25 02:23:48 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:48 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:48 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.mlp.up_proj.weight'
INFO 01-25 02:23:48 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.2.mlp.up_proj.weight']
INFO 01-25 02:23:48 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:48 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:48 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:48 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:48 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:48 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:48 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:48 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:48 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:48 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:48 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:48 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:48 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:48 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:48 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:48 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:48 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:48 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:48 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:48 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd3f5b0>]
INFO 01-25 02:23:48 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:48 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:48 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:48 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd3f5b0>]
INFO 01-25 02:23:48 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:48 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:48 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.post_attention_layernorm.weight'
INFO 01-25 02:23:48 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.2.post_attention_layernorm.weight']
INFO 01-25 02:23:48 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:48 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:48 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:48 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:48 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:48 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:48 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:48 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:48 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:48 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:48 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:48 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:48 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:48 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:48 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:48 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:48 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:48 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:48 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.k_proj.weight'
INFO 01-25 02:23:48 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.2.self_attn.k_proj.weight']
INFO 01-25 02:23:48 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:48 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:48 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:48 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:48 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:48 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:48 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:48 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:48 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:48 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:48 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:48 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:48 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:48 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:48 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:48 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:48 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:48 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:48 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:48 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c9d0>]
INFO 01-25 02:23:48 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:48 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:48 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:48 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c9d0>]
INFO 01-25 02:23:48 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:48 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:48 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.o_proj.weight'
INFO 01-25 02:23:48 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.2.self_attn.o_proj.weight']
INFO 01-25 02:23:48 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:48 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:48 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:48 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:48 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:48 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:48 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:48 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:48 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:48 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:48 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:48 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:48 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:48 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:48 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ca60>]
INFO 01-25 02:23:48 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:48 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:48 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:48 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ca60>]
INFO 01-25 02:23:48 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:48 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:48 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.q_proj.weight'
INFO 01-25 02:23:48 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.2.self_attn.q_proj.weight']
INFO 01-25 02:23:48 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:48 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:48 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:48 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:48 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:48 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:48 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:48 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:48 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:48 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:48 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:48 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:48 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:48 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:48 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:48 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:48 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c9d0>]
INFO 01-25 02:23:48 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:48 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:48 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:48 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c9d0>]
INFO 01-25 02:23:48 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:48 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:48 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.2.self_attn.v_proj.weight'
INFO 01-25 02:23:48 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.2.self_attn.v_proj.weight']
INFO 01-25 02:23:48 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:48 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.2.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:48 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:48 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:48 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:48 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:48 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:48 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:48 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:48 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:48 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:48 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:48 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:48 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:48 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:48 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:48 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:48 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:48 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:48 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c9d0>]
INFO 01-25 02:23:48 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:48 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:48 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:48 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:48 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c9d0>]
INFO 01-25 02:23:48 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:48 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:48 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.input_layernorm.weight'
INFO 01-25 02:23:48 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.3.input_layernorm.weight']
INFO 01-25 02:23:48 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:48 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:48 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:48 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:48 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:48 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:48 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:48 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:48 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:48 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:48 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:48 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:48 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:48 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:48 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:48 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:48 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:48 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:48 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.mlp.down_proj.weight'
INFO 01-25 02:23:48 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.3.mlp.down_proj.weight']
INFO 01-25 02:23:48 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:48 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:48 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:48 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:48 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:48 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:48 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:48 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:48 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:48 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:48 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:48 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:48 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:48 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:48 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c8b0>]
INFO 01-25 02:23:48 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:48 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:48 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:48 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:48 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c8b0>]
INFO 01-25 02:23:48 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:48 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:48 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.mlp.gate_proj.weight'
INFO 01-25 02:23:48 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.3.mlp.gate_proj.weight']
INFO 01-25 02:23:48 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:48 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:48 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:48 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:48 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:48 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:48 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:48 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:48 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:48 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:48 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:48 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:48 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:48 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:48 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:48 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:48 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:48 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:48 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:48 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:48 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:48 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:48 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:48 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:49 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:49 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:49 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:49 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c820>]
INFO 01-25 02:23:49 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:49 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:49 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:49 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c820>]
INFO 01-25 02:23:49 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:49 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:49 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.mlp.up_proj.weight'
INFO 01-25 02:23:49 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.3.mlp.up_proj.weight']
INFO 01-25 02:23:49 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:49 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:49 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:49 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:49 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:49 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:49 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:49 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:49 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:49 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:49 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:49 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:49 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:49 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:49 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:49 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:49 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:49 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:49 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:49 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c820>]
INFO 01-25 02:23:49 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:49 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:49 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:49 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c820>]
INFO 01-25 02:23:49 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:49 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:49 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.post_attention_layernorm.weight'
INFO 01-25 02:23:49 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.3.post_attention_layernorm.weight']
INFO 01-25 02:23:49 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:49 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:49 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:49 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:49 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:49 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:49 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:49 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:49 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:49 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:49 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:49 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:49 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:49 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:49 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:49 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:49 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:49 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:49 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.k_proj.weight'
INFO 01-25 02:23:49 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.3.self_attn.k_proj.weight']
INFO 01-25 02:23:49 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:49 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:49 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:49 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:49 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:49 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:49 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:49 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:49 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:49 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:49 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:49 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:49 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:49 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:49 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:49 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:49 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:49 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:49 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:49 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045bd0>]
INFO 01-25 02:23:49 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:49 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:49 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:49 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045bd0>]
INFO 01-25 02:23:49 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:49 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:49 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.o_proj.weight'
INFO 01-25 02:23:49 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.3.self_attn.o_proj.weight']
INFO 01-25 02:23:49 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:49 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:49 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:49 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:49 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:49 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:49 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:49 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:49 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:49 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:49 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:49 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:49 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:49 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:49 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045870>]
INFO 01-25 02:23:49 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:49 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:49 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:49 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045870>]
INFO 01-25 02:23:49 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:49 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:49 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.q_proj.weight'
INFO 01-25 02:23:49 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.3.self_attn.q_proj.weight']
INFO 01-25 02:23:49 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:49 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:49 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:49 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:49 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:49 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:49 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:49 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:49 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:49 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:49 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:49 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:49 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:49 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:49 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:49 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:49 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045bd0>]
INFO 01-25 02:23:49 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:49 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:49 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:49 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045bd0>]
INFO 01-25 02:23:49 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:49 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:49 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.3.self_attn.v_proj.weight'
INFO 01-25 02:23:49 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.3.self_attn.v_proj.weight']
INFO 01-25 02:23:49 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:49 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.3.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:49 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:49 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:49 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:49 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:49 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:49 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:49 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:49 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:49 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:49 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:49 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:49 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:49 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:49 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:49 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:49 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:49 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:49 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045bd0>]
INFO 01-25 02:23:49 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:49 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:49 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:49 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:49 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045bd0>]
INFO 01-25 02:23:49 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:49 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:49 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.input_layernorm.weight'
INFO 01-25 02:23:49 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.4.input_layernorm.weight']
INFO 01-25 02:23:49 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:49 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:49 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:49 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:49 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:49 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:49 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:49 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:49 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:49 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:49 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:49 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:49 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:49 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:49 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:49 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:49 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:49 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:49 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.mlp.down_proj.weight'
INFO 01-25 02:23:49 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.4.mlp.down_proj.weight']
INFO 01-25 02:23:49 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:49 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:49 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:49 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:49 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:49 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:49 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:49 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:49 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:49 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:49 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:49 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:49 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:49 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:49 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045cf0>]
INFO 01-25 02:23:49 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:49 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:49 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:49 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:49 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045cf0>]
INFO 01-25 02:23:49 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:49 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:49 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.mlp.gate_proj.weight'
INFO 01-25 02:23:49 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.4.mlp.gate_proj.weight']
INFO 01-25 02:23:49 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:49 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:49 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:49 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:49 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:49 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:49 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:49 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:49 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:49 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:49 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:49 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:49 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:49 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:49 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:49 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:49 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:49 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:49 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:49 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:49 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:49 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:49 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:49 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:50 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:50 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:50 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:50 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045c60>]
INFO 01-25 02:23:50 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:50 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:50 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:50 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:50 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045c60>]
INFO 01-25 02:23:50 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:50 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:50 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.mlp.up_proj.weight'
INFO 01-25 02:23:50 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.4.mlp.up_proj.weight']
INFO 01-25 02:23:50 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:50 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:50 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:50 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:50 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:50 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:50 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:50 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:50 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:50 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:50 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:50 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:50 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:50 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:50 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:50 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:50 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:50 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:50 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:50 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:50 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:50 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:50 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:50 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:50 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:50 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:50 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:50 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045c60>]
INFO 01-25 02:23:50 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:50 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:50 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:50 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:50 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac045c60>]
INFO 01-25 02:23:50 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:50 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:50 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.post_attention_layernorm.weight'
INFO 01-25 02:23:50 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.4.post_attention_layernorm.weight']
INFO 01-25 02:23:50 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:50 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:50 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:50 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:50 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:50 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:50 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:50 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:50 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:50 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:50 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:50 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:50 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:50 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:50 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:50 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:50 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:50 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:50 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.k_proj.weight'
INFO 01-25 02:23:50 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.4.self_attn.k_proj.weight']
INFO 01-25 02:23:50 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:50 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:50 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:50 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:50 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:50 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:50 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:50 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:50 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:50 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:50 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:50 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:50 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:50 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:50 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:50 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:50 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:50 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:50 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:50 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:50 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:50 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:50 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:50 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cc10>]
INFO 01-25 02:23:50 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:50 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:50 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:50 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cc10>]
INFO 01-25 02:23:50 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:50 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:50 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.o_proj.weight'
INFO 01-25 02:23:50 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.4.self_attn.o_proj.weight']
INFO 01-25 02:23:50 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:50 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:50 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:50 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:50 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:50 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:50 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:50 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:50 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:50 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:50 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:50 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:50 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:50 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:50 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cca0>]
INFO 01-25 02:23:50 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:50 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:50 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:50 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cca0>]
INFO 01-25 02:23:50 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:50 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:50 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.q_proj.weight'
INFO 01-25 02:23:50 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.4.self_attn.q_proj.weight']
INFO 01-25 02:23:50 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:50 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:50 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:50 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:50 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:50 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:50 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:50 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:50 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:50 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:50 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:50 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:50 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:50 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:50 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:50 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:50 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:50 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:50 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:50 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:50 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cc10>]
INFO 01-25 02:23:50 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:50 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:50 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:50 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cc10>]
INFO 01-25 02:23:50 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:50 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:50 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.4.self_attn.v_proj.weight'
INFO 01-25 02:23:50 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.4.self_attn.v_proj.weight']
INFO 01-25 02:23:50 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:50 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.4.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:50 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:50 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:50 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:50 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:50 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:50 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:50 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:50 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:50 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:50 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:50 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:50 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:50 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:50 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:50 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:50 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:50 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:50 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:50 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:50 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:50 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:50 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:50 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cc10>]
INFO 01-25 02:23:50 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:50 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:50 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:50 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:50 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cc10>]
INFO 01-25 02:23:50 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:50 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:50 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.input_layernorm.weight'
INFO 01-25 02:23:50 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.5.input_layernorm.weight']
INFO 01-25 02:23:50 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:50 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:50 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:50 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:50 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:50 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:50 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:50 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:50 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:50 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:50 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:50 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:50 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:50 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:50 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:50 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:50 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:50 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:50 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.mlp.down_proj.weight'
INFO 01-25 02:23:50 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.5.mlp.down_proj.weight']
INFO 01-25 02:23:50 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:50 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:50 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:50 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:50 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:50 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:50 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:50 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:50 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:50 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:50 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:50 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:50 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:50 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:50 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:50 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cdc0>]
INFO 01-25 02:23:50 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:50 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:50 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:50 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:50 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:50 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:50 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cdc0>]
INFO 01-25 02:23:51 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 11208473600}]
INFO 01-25 02:23:51 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:51 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.mlp.gate_proj.weight'
INFO 01-25 02:23:51 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.5.mlp.gate_proj.weight']
INFO 01-25 02:23:51 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:51 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:51 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:51 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:51 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:51 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:51 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:51 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:51 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:51 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:51 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:51 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:51 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:51 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:51 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:51 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:51 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:51 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:51 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:51 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:51 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:51 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:51 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:51 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:51 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:51 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:51 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:51 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:51 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:51 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:51 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cd30>]
INFO 01-25 02:23:51 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:51 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:51 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:51 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cd30>]
INFO 01-25 02:23:51 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:51 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:51 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.mlp.up_proj.weight'
INFO 01-25 02:23:51 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.5.mlp.up_proj.weight']
INFO 01-25 02:23:51 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:51 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:51 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:51 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:51 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:51 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:51 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:51 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:51 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:51 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:51 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:51 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:51 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:51 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:51 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:51 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:51 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:51 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:51 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:51 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:51 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:51 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:51 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:51 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:51 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:51 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:51 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:51 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:51 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:51 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:51 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cd30>]
INFO 01-25 02:23:51 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:51 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:51 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:51 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:51 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8cd30>]
INFO 01-25 02:23:51 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:51 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:51 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.post_attention_layernorm.weight'
INFO 01-25 02:23:51 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.5.post_attention_layernorm.weight']
INFO 01-25 02:23:51 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:51 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:51 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:51 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:51 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:51 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:51 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:51 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:51 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:51 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:51 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:51 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:51 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:51 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:51 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:51 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:51 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:51 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:51 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.k_proj.weight'
INFO 01-25 02:23:51 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.5.self_attn.k_proj.weight']
INFO 01-25 02:23:51 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:51 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:51 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:51 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:51 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:51 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:51 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:51 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:51 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:51 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:51 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:51 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:51 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:51 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:51 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:51 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:51 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:51 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:51 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:51 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:51 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:51 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:51 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:51 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:51 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:51 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:51 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:51 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:51 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:51 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:51 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:51 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8caf0>]
INFO 01-25 02:23:51 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:51 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:51 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:51 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:51 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8caf0>]
INFO 01-25 02:23:51 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:51 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:51 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.o_proj.weight'
INFO 01-25 02:23:51 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.5.self_attn.o_proj.weight']
INFO 01-25 02:23:51 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:51 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:51 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:51 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:51 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:51 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:51 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:51 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:51 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:51 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:51 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:51 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:51 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:51 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:51 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:51 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:51 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:51 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:51 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:52 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:52 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:52 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:52 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:52 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:52 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c790>]
INFO 01-25 02:23:52 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:52 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:52 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:52 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:52 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c790>]
INFO 01-25 02:23:52 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:52 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:52 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.q_proj.weight'
INFO 01-25 02:23:52 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.5.self_attn.q_proj.weight']
INFO 01-25 02:23:52 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:52 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:52 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:52 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:52 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:52 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:52 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:52 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:52 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:52 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:52 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:52 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:52 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:52 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:52 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:52 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:52 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:52 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:52 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:52 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:52 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:52 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:52 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:52 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:52 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:52 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8caf0>]
INFO 01-25 02:23:52 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:52 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:52 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:52 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8caf0>]
INFO 01-25 02:23:52 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:52 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:52 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.5.self_attn.v_proj.weight'
INFO 01-25 02:23:52 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.5.self_attn.v_proj.weight']
INFO 01-25 02:23:52 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:52 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.5.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:52 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:52 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:52 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:52 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:52 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:52 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:52 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:52 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:52 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:52 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:52 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:52 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:52 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:52 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:52 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:52 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:23:52 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:52 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:52 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:52 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:52 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:52 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:52 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:52 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:52 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8caf0>]
INFO 01-25 02:23:52 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:52 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:52 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:52 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:52 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8caf0>]
INFO 01-25 02:23:52 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:52 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:52 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.input_layernorm.weight'
INFO 01-25 02:23:52 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.6.input_layernorm.weight']
INFO 01-25 02:23:52 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:52 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:52 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:52 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:52 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:52 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:52 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:52 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:52 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:52 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:52 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:52 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:52 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:52 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:52 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:52 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:52 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:52 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:52 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.mlp.down_proj.weight'
INFO 01-25 02:23:52 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.6.mlp.down_proj.weight']
INFO 01-25 02:23:52 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:52 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:52 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:52 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:52 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:52 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:52 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:52 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:52 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:52 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:52 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:52 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:52 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:52 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:52 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff400>]
INFO 01-25 02:23:52 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:52 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:52 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:52 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:52 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2ff400>]
INFO 01-25 02:23:52 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:52 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:52 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.mlp.gate_proj.weight'
INFO 01-25 02:23:52 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.6.mlp.gate_proj.weight']
INFO 01-25 02:23:52 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:52 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:52 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:52 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:52 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:52 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:52 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:52 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:52 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:52 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:52 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:52 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:52 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:52 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:52 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:52 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:52 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:52 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:52 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:52 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:52 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:52 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:52 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:52 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:52 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:52 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:52 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:52 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:52 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:52 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:52 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2fe4d0>]
INFO 01-25 02:23:52 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:52 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:52 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:52 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:52 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2fe4d0>]
INFO 01-25 02:23:52 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:52 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:52 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.mlp.up_proj.weight'
INFO 01-25 02:23:52 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.6.mlp.up_proj.weight']
INFO 01-25 02:23:52 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:52 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:52 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:52 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:52 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:52 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:52 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:52 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:52 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:52 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:52 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:52 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:52 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:52 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:52 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:52 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:52 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:52 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:52 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:52 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:52 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:52 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:52 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:52 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:52 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:53 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:53 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:53 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:53 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:53 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:53 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:53 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:53 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:53 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:53 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2fe4d0>]
INFO 01-25 02:23:53 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:53 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:53 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:53 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:53 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af1ac2fe4d0>]
INFO 01-25 02:23:53 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:53 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:53 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.post_attention_layernorm.weight'
INFO 01-25 02:23:53 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.6.post_attention_layernorm.weight']
INFO 01-25 02:23:53 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:53 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:53 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:53 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:53 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:53 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:53 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:53 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:53 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:53 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:53 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:53 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:53 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:53 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:53 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:53 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:53 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:53 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:53 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.k_proj.weight'
INFO 01-25 02:23:53 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.6.self_attn.k_proj.weight']
INFO 01-25 02:23:53 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:53 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:53 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:53 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:53 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:53 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:53 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:53 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:53 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:53 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:53 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:53 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:53 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:53 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:53 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:53 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:53 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:53 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:53 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:53 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:53 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:53 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:53 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:53 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:53 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:53 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d090>]
INFO 01-25 02:23:53 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:53 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:53 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:53 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d090>]
INFO 01-25 02:23:53 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:53 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:53 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.o_proj.weight'
INFO 01-25 02:23:53 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.6.self_attn.o_proj.weight']
INFO 01-25 02:23:53 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:53 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:53 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:53 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:53 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:53 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:53 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:53 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:53 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:53 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:53 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:53 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:53 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:53 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:53 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d120>]
INFO 01-25 02:23:53 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:53 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:53 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:53 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d120>]
INFO 01-25 02:23:53 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:53 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:53 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.q_proj.weight'
INFO 01-25 02:23:53 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.6.self_attn.q_proj.weight']
INFO 01-25 02:23:53 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:53 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:53 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:53 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:53 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:53 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:53 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:53 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:53 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:53 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:53 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:53 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:53 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:53 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:53 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:53 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:53 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:53 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:53 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:53 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:53 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:53 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:53 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d090>]
INFO 01-25 02:23:53 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:53 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:53 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:53 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d090>]
INFO 01-25 02:23:53 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:53 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:53 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.6.self_attn.v_proj.weight'
INFO 01-25 02:23:53 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.6.self_attn.v_proj.weight']
INFO 01-25 02:23:53 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:53 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.6.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:53 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:53 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:53 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:53 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:53 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:53 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:53 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:53 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:53 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:53 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:53 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:53 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:53 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:53 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:53 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:53 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:53 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:53 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:53 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:53 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:53 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:53 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:53 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:53 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d090>]
INFO 01-25 02:23:53 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:53 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:53 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:53 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:53 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d090>]
INFO 01-25 02:23:53 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:53 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:53 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.input_layernorm.weight'
INFO 01-25 02:23:53 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.7.input_layernorm.weight']
INFO 01-25 02:23:53 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:53 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:53 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:53 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:53 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:53 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:53 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:53 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:53 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:53 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:53 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:53 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:53 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:53 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:53 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:53 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:53 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:53 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:53 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.mlp.down_proj.weight'
INFO 01-25 02:23:53 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.7.mlp.down_proj.weight']
INFO 01-25 02:23:53 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:53 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:53 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:53 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:53 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:53 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:53 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:53 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:53 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:53 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:53 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:53 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:53 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:53 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:53 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d240>]
INFO 01-25 02:23:53 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:53 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:53 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:53 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:53 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d240>]
INFO 01-25 02:23:53 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:53 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:53 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.mlp.gate_proj.weight'
INFO 01-25 02:23:53 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.7.mlp.gate_proj.weight']
INFO 01-25 02:23:53 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:53 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:53 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:53 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:53 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:53 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:53 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:53 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:53 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:53 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:53 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:53 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:53 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:53 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:53 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:53 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:53 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:53 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:53 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:53 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:53 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:53 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:53 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:53 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:53 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:53 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:53 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:53 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:53 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:54 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:54 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:54 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:54 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d1b0>]
INFO 01-25 02:23:54 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:54 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:54 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:54 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:54 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d1b0>]
INFO 01-25 02:23:54 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:54 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:54 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.mlp.up_proj.weight'
INFO 01-25 02:23:54 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.7.mlp.up_proj.weight']
INFO 01-25 02:23:54 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:54 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:54 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:54 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:54 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:54 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:54 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:54 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:54 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:54 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:54 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:54 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:54 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:54 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:54 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:54 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:54 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:54 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:54 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:54 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:54 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:54 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:54 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:54 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:54 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:54 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:54 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:54 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d1b0>]
INFO 01-25 02:23:54 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:54 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:54 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:54 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:54 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d1b0>]
INFO 01-25 02:23:54 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:54 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:54 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.post_attention_layernorm.weight'
INFO 01-25 02:23:54 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.7.post_attention_layernorm.weight']
INFO 01-25 02:23:54 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:54 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:54 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:54 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:54 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:54 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:54 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:54 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:54 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:54 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:54 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:54 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:54 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:54 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:54 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:54 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:54 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:54 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:54 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.k_proj.weight'
INFO 01-25 02:23:54 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.7.self_attn.k_proj.weight']
INFO 01-25 02:23:54 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:54 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:54 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:54 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:54 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:54 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:54 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:54 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:54 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:54 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:54 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:54 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:54 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:54 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:54 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:54 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:54 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:54 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:54 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:54 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:54 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:54 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:54 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:54 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c430>]
INFO 01-25 02:23:54 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:54 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:54 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:54 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c430>]
INFO 01-25 02:23:54 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:54 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:54 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.o_proj.weight'
INFO 01-25 02:23:54 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.7.self_attn.o_proj.weight']
INFO 01-25 02:23:54 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:54 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:54 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:54 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:54 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:54 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:54 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:54 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:54 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:54 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:54 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:54 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:54 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:54 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:54 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d000>]
INFO 01-25 02:23:54 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:54 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:54 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:54 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d000>]
INFO 01-25 02:23:54 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:54 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:54 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.q_proj.weight'
INFO 01-25 02:23:54 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.7.self_attn.q_proj.weight']
INFO 01-25 02:23:54 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:54 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:54 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:54 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:54 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:54 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:54 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:54 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:54 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:54 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:54 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:54 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:54 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:54 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:54 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:54 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:54 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:54 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:54 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:54 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:54 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c430>]
INFO 01-25 02:23:54 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:54 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:54 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:54 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c430>]
INFO 01-25 02:23:54 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:54 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:54 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.7.self_attn.v_proj.weight'
INFO 01-25 02:23:54 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.7.self_attn.v_proj.weight']
INFO 01-25 02:23:54 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:54 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.7.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:54 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:54 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:54 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:54 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:54 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:54 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:54 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:54 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:54 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:54 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:54 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:54 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:54 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:54 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:54 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:54 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:54 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:54 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:54 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:54 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:54 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:54 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:54 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:54 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c430>]
INFO 01-25 02:23:54 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:54 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:54 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:54 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:54 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8c430>]
INFO 01-25 02:23:54 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:54 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:54 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.input_layernorm.weight'
INFO 01-25 02:23:54 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.8.input_layernorm.weight']
INFO 01-25 02:23:54 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:54 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:54 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:54 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:54 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:54 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:54 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:54 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:54 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:54 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:54 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:54 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:54 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:54 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:54 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:54 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:54 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:54 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:54 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.mlp.down_proj.weight'
INFO 01-25 02:23:54 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.8.mlp.down_proj.weight']
INFO 01-25 02:23:54 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:54 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:54 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:54 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:54 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:54 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:54 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:54 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:54 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:54 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:54 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:54 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:54 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:54 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:54 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:54 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:54 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:54 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:54 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:55 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:55 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:55 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:55 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:55 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:55 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d3f0>]
INFO 01-25 02:23:55 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:55 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:55 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:55 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:55 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d3f0>]
INFO 01-25 02:23:55 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:55 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:55 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.mlp.gate_proj.weight'
INFO 01-25 02:23:55 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.8.mlp.gate_proj.weight']
INFO 01-25 02:23:55 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:55 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:55 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:55 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:55 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:55 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:55 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:55 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:55 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:55 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:55 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:55 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:55 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:55 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:55 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:55 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:55 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:55 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:55 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:55 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:55 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:55 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:55 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:55 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:55 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:55 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:55 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d360>]
INFO 01-25 02:23:55 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:55 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:55 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:55 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d360>]
INFO 01-25 02:23:55 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:55 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:55 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.mlp.up_proj.weight'
INFO 01-25 02:23:55 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.8.mlp.up_proj.weight']
INFO 01-25 02:23:55 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:55 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:55 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:55 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:55 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:55 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:55 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:55 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:55 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:55 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:55 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:55 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:55 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:55 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:55 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:55 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:55 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:55 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:55 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:55 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:55 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:55 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:55 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:55 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:55 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:55 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:55 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d360>]
INFO 01-25 02:23:55 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:55 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:55 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:55 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:55 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d360>]
INFO 01-25 02:23:55 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:55 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:55 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.post_attention_layernorm.weight'
INFO 01-25 02:23:55 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.8.post_attention_layernorm.weight']
INFO 01-25 02:23:55 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:55 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:55 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:55 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:55 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:55 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:55 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:55 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:55 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:55 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:55 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:55 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:55 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:55 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:55 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:55 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:55 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:55 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:55 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.k_proj.weight'
INFO 01-25 02:23:55 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.8.self_attn.k_proj.weight']
INFO 01-25 02:23:55 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:55 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:55 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:55 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:55 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:55 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:55 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:55 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:55 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:55 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:55 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:55 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:55 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:55 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:55 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:55 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:55 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:55 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:55 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:55 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:55 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:55 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:55 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:55 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:55 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:55 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:55 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:55 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:55 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d510>]
INFO 01-25 02:23:55 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:55 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:55 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:55 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:55 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d510>]
INFO 01-25 02:23:55 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:55 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:55 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.o_proj.weight'
INFO 01-25 02:23:55 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.8.self_attn.o_proj.weight']
INFO 01-25 02:23:55 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:55 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:55 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:55 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:55 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:55 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:55 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:55 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:55 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:55 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:55 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:55 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:55 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:55 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:55 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:55 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d5a0>]
INFO 01-25 02:23:55 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:55 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:55 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:55 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d5a0>]
INFO 01-25 02:23:55 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:55 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:55 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.q_proj.weight'
INFO 01-25 02:23:55 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.8.self_attn.q_proj.weight']
INFO 01-25 02:23:55 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:55 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:55 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:55 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:55 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:55 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:55 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:55 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:55 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:55 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:55 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:55 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:55 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:55 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:55 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:55 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:55 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:55 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:55 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:55 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:56 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:56 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:56 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:56 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:56 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:56 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:56 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:56 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:56 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:56 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d510>]
INFO 01-25 02:23:56 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:56 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:56 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:56 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:56 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d510>]
INFO 01-25 02:23:56 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 9696427008}]
INFO 01-25 02:23:56 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:56 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.8.self_attn.v_proj.weight'
INFO 01-25 02:23:56 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.8.self_attn.v_proj.weight']
INFO 01-25 02:23:56 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:56 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.8.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:56 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:56 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:56 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:56 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:56 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:56 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:56 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:56 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:56 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:56 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:56 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:56 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:56 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:56 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:56 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:56 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:56 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:56 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:56 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:56 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:56 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:56 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:56 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:56 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:56 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:56 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:56 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d510>]
INFO 01-25 02:23:56 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:56 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:56 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:56 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:56 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d510>]
INFO 01-25 02:23:56 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:56 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:43<00:15, 15.71s/it]
INFO 01-25 02:23:56 weight_utils.py:424] hosseins: weight_utils.py -> safetensors_weights_iterator() [{'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:56 weight_utils.py:425] hosseins: weight_utils.py -> safetensors_weights_iterator() [4.0]
INFO 01-25 02:23:56 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.input_layernorm.weight'
INFO 01-25 02:23:56 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.10.input_layernorm.weight']
INFO 01-25 02:23:56 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:56 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:56 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:56 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:56 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:56 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:56 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:56 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:56 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:56 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:56 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:56 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:56 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:56 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:56 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:56 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:56 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:56 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:56 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.mlp.down_proj.weight'
INFO 01-25 02:23:56 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.10.mlp.down_proj.weight']
INFO 01-25 02:23:56 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:56 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:56 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:56 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:56 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:56 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:56 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:56 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:56 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:56 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:56 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:56 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:56 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:56 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:56 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d870>]
INFO 01-25 02:23:56 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:56 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:56 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:56 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:56 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d870>]
INFO 01-25 02:23:56 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:56 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:56 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.mlp.gate_proj.weight'
INFO 01-25 02:23:56 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.10.mlp.gate_proj.weight']
INFO 01-25 02:23:56 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:56 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:56 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:56 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:56 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:56 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:56 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:56 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:56 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:56 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:56 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:56 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:56 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:56 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:56 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:56 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:56 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:56 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:56 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:56 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:56 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:56 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:56 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:56 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:56 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:56 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:56 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d7e0>]
INFO 01-25 02:23:56 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:56 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:56 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:56 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d7e0>]
INFO 01-25 02:23:56 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:56 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:56 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.mlp.up_proj.weight'
INFO 01-25 02:23:56 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.10.mlp.up_proj.weight']
INFO 01-25 02:23:56 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:56 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:56 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:56 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:56 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:56 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:56 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:56 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:56 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:56 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:56 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:56 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:56 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:56 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:56 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:56 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:56 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:56 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:56 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:56 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:56 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:56 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:56 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:56 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:56 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:56 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:56 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:56 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:57 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:57 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:57 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d7e0>]
INFO 01-25 02:23:57 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:57 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:57 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:57 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:57 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d7e0>]
INFO 01-25 02:23:57 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:57 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:57 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.post_attention_layernorm.weight'
INFO 01-25 02:23:57 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.10.post_attention_layernorm.weight']
INFO 01-25 02:23:57 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:57 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:57 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:57 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:57 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:57 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:57 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:57 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:57 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:57 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:57 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:57 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:57 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:57 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:57 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:57 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:57 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:57 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:57 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.k_proj.weight'
INFO 01-25 02:23:57 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.10.self_attn.k_proj.weight']
INFO 01-25 02:23:57 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:57 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:57 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:57 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:57 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:57 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:57 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:57 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:57 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:57 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:57 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:57 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:57 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:57 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:57 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:57 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:57 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:57 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:57 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:57 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:57 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:57 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:57 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:57 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:57 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:57 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d990>]
INFO 01-25 02:23:57 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:57 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:57 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:57 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d990>]
INFO 01-25 02:23:57 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:57 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:57 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.o_proj.weight'
INFO 01-25 02:23:57 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.10.self_attn.o_proj.weight']
INFO 01-25 02:23:57 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:57 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:57 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:57 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:57 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:57 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:57 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:57 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:57 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:57 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:57 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:57 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:57 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:57 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:57 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8da20>]
INFO 01-25 02:23:57 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:57 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:57 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:57 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8da20>]
INFO 01-25 02:23:57 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:57 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:57 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.q_proj.weight'
INFO 01-25 02:23:57 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.10.self_attn.q_proj.weight']
INFO 01-25 02:23:57 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:57 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:57 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:57 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:57 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:57 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:57 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:57 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:57 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:57 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:57 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:57 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:57 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:57 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:57 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:57 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:57 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:57 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:57 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:57 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:57 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:57 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:57 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d990>]
INFO 01-25 02:23:57 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:57 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:57 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:57 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d990>]
INFO 01-25 02:23:57 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:57 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:57 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.10.self_attn.v_proj.weight'
INFO 01-25 02:23:57 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.10.self_attn.v_proj.weight']
INFO 01-25 02:23:57 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:57 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.10.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:57 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:57 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:57 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:57 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:57 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:57 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:57 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:57 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:57 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:57 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:57 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:57 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:57 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:57 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:57 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:57 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:57 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:57 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:57 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:57 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:57 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:57 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:57 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:57 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d990>]
INFO 01-25 02:23:57 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:57 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:57 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:57 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:57 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d990>]
INFO 01-25 02:23:57 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:57 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:57 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.input_layernorm.weight'
INFO 01-25 02:23:57 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.11.input_layernorm.weight']
INFO 01-25 02:23:57 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:57 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:57 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:57 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:57 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:57 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:57 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:57 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:57 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:57 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:57 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:57 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:57 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:57 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:57 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:57 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:57 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:57 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:57 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.mlp.down_proj.weight'
INFO 01-25 02:23:57 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.11.mlp.down_proj.weight']
INFO 01-25 02:23:57 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:57 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:57 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:57 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:57 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:57 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:57 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:57 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:57 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:57 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:57 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:57 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:57 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:57 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:57 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dc60>]
INFO 01-25 02:23:57 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:57 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:57 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:57 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:57 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dc60>]
INFO 01-25 02:23:57 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:57 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:57 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.mlp.gate_proj.weight'
INFO 01-25 02:23:57 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.11.mlp.gate_proj.weight']
INFO 01-25 02:23:57 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:57 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:57 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:57 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:57 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:57 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:57 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:57 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:57 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:57 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:57 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:57 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:57 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:57 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:57 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:57 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:57 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:57 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:57 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:57 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:57 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:57 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:57 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:57 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:57 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:57 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:57 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:57 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:57 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:58 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:58 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:58 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:58 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:58 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:58 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dab0>]
INFO 01-25 02:23:58 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:58 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:58 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:58 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:58 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dab0>]
INFO 01-25 02:23:58 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:58 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:58 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.mlp.up_proj.weight'
INFO 01-25 02:23:58 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.11.mlp.up_proj.weight']
INFO 01-25 02:23:58 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:58 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:58 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:58 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:58 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:58 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:58 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:58 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:58 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:58 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:58 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:58 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:58 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:58 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:58 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:58 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:58 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:58 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:58 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:58 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:58 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:58 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:58 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:58 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:58 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:58 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:58 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:58 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:58 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:58 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:58 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dab0>]
INFO 01-25 02:23:58 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:58 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:58 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:58 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:58 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dab0>]
INFO 01-25 02:23:58 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:58 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:58 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.post_attention_layernorm.weight'
INFO 01-25 02:23:58 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.11.post_attention_layernorm.weight']
INFO 01-25 02:23:58 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:58 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:58 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:58 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:58 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:58 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:58 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:58 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:58 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:58 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:58 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:58 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:58 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:58 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:58 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:58 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:58 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:58 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:58 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.k_proj.weight'
INFO 01-25 02:23:58 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.11.self_attn.k_proj.weight']
INFO 01-25 02:23:58 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:58 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:58 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:58 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:58 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:58 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:58 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:58 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:58 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:58 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:58 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:58 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:58 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:58 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:58 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:58 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:58 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:58 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:58 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:58 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:58 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:58 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:58 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:58 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:23:58 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:58 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d2d0>]
INFO 01-25 02:23:58 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:58 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:58 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:58 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d2d0>]
INFO 01-25 02:23:58 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:58 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:58 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.o_proj.weight'
INFO 01-25 02:23:58 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.11.self_attn.o_proj.weight']
INFO 01-25 02:23:58 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:58 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:58 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:58 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:58 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:58 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:58 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:58 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:58 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:58 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:58 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:58 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:58 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:58 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:58 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d900>]
INFO 01-25 02:23:58 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:58 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:58 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:58 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d900>]
INFO 01-25 02:23:58 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:58 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:58 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.q_proj.weight'
INFO 01-25 02:23:58 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.11.self_attn.q_proj.weight']
INFO 01-25 02:23:58 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:23:58 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:58 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:58 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:58 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:23:58 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:58 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:58 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:58 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:58 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:58 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:58 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:58 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:58 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:58 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:23:58 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:23:58 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:58 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:23:58 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:23:58 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:58 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:58 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:23:58 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:58 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d2d0>]
INFO 01-25 02:23:58 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:58 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:58 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:58 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d2d0>]
INFO 01-25 02:23:58 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:58 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:58 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.11.self_attn.v_proj.weight'
INFO 01-25 02:23:58 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.11.self_attn.v_proj.weight']
INFO 01-25 02:23:58 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:58 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.11.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:23:58 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:58 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:58 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:58 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:58 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:58 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:23:58 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:58 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:58 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:58 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:58 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:58 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:58 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:58 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:58 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:58 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:58 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:58 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:59 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:59 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:59 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:59 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:59 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:59 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:59 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:59 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:23:59 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:59 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d2d0>]
INFO 01-25 02:23:59 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:59 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:59 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:23:59 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:59 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d2d0>]
INFO 01-25 02:23:59 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:59 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:59 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.input_layernorm.weight'
INFO 01-25 02:23:59 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.12.input_layernorm.weight']
INFO 01-25 02:23:59 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:59 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:59 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:59 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:59 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:59 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:59 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:59 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:59 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:59 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:59 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:59 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:59 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:59 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:59 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:59 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:59 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:59 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:59 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.mlp.down_proj.weight'
INFO 01-25 02:23:59 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.12.mlp.down_proj.weight']
INFO 01-25 02:23:59 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:59 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:23:59 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:59 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:59 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:23:59 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:23:59 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:59 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:59 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:59 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:23:59 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:59 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:23:59 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:59 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:59 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8de10>]
INFO 01-25 02:23:59 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:59 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:59 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:59 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:23:59 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8de10>]
INFO 01-25 02:23:59 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:59 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:59 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.mlp.gate_proj.weight'
INFO 01-25 02:23:59 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.12.mlp.gate_proj.weight']
INFO 01-25 02:23:59 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:59 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:23:59 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:59 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:59 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:59 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:59 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:23:59 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:59 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:59 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:59 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:59 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:59 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:59 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:59 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:59 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:59 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:23:59 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:59 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:59 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:59 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:59 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:59 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:59 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:59 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:23:59 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:59 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dd80>]
INFO 01-25 02:23:59 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:59 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:59 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:59 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dd80>]
INFO 01-25 02:23:59 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:59 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:59 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.mlp.up_proj.weight'
INFO 01-25 02:23:59 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.12.mlp.up_proj.weight']
INFO 01-25 02:23:59 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:59 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:23:59 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:23:59 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:59 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:59 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:59 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:23:59 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:23:59 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:59 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:59 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:59 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:59 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:59 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:59 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:59 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:59 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:23:59 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:23:59 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:59 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:23:59 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:59 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:23:59 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:23:59 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:59 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:59 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:59 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dd80>]
INFO 01-25 02:23:59 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:23:59 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:59 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:59 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:23:59 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dd80>]
INFO 01-25 02:23:59 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:59 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:59 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.post_attention_layernorm.weight'
INFO 01-25 02:23:59 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.12.post_attention_layernorm.weight']
INFO 01-25 02:23:59 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:23:59 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:23:59 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:23:59 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:23:59 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:23:59 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:23:59 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:23:59 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:23:59 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:23:59 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:23:59 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:59 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:23:59 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:23:59 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:23:59 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:23:59 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:23:59 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:23:59 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:23:59 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.k_proj.weight'
INFO 01-25 02:23:59 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.12.self_attn.k_proj.weight']
INFO 01-25 02:23:59 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:59 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:23:59 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:23:59 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:23:59 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:23:59 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:23:59 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:59 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:23:59 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:23:59 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:59 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:59 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:59 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:59 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:23:59 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:23:59 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:23:59 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:23:59 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:23:59 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:23:59 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:23:59 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:23:59 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:59 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:59 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:23:59 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:23:59 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:23:59 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:23:59 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:23:59 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:23:59 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:00 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:00 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:00 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:00 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8df30>]
INFO 01-25 02:24:00 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:00 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:00 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:00 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8df30>]
INFO 01-25 02:24:00 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:24:00 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:00 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.o_proj.weight'
INFO 01-25 02:24:00 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.12.self_attn.o_proj.weight']
INFO 01-25 02:24:00 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:00 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:00 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:00 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:00 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:00 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:00 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:00 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:00 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:00 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:00 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:00 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:00 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:00 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:00 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:00 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dfc0>]
INFO 01-25 02:24:00 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:00 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:00 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:00 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dfc0>]
INFO 01-25 02:24:00 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:24:00 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:00 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.q_proj.weight'
INFO 01-25 02:24:00 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.12.self_attn.q_proj.weight']
INFO 01-25 02:24:00 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:00 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:00 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:00 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:00 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:00 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:00 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:00 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:00 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:00 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:00 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:00 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:00 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:00 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:00 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:00 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:00 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:00 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:00 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:00 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:00 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:00 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:00 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:00 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:00 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8df30>]
INFO 01-25 02:24:00 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:00 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:00 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:00 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8df30>]
INFO 01-25 02:24:00 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:24:00 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:00 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.12.self_attn.v_proj.weight'
INFO 01-25 02:24:00 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.12.self_attn.v_proj.weight']
INFO 01-25 02:24:00 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:00 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.12.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:00 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:00 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:00 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:00 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:00 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:00 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:00 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:00 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:00 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:00 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:00 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:00 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:00 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:00 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:00 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:00 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:00 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:00 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:00 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:00 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:00 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:00 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:00 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:00 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:00 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:00 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8df30>]
INFO 01-25 02:24:00 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:00 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:00 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:00 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:00 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8df30>]
INFO 01-25 02:24:00 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:24:00 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:00 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.input_layernorm.weight'
INFO 01-25 02:24:00 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.13.input_layernorm.weight']
INFO 01-25 02:24:00 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:00 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:00 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:00 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:00 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:00 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:00 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:00 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:00 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:00 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:00 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:00 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:00 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:00 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:00 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:00 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:00 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:24:00 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:00 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.mlp.down_proj.weight'
INFO 01-25 02:24:00 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.13.mlp.down_proj.weight']
INFO 01-25 02:24:00 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:00 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:00 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:00 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:00 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:00 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:00 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:00 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:00 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:00 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:00 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:24:00 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:00 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:00 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:00 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:00 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e0e0>]
INFO 01-25 02:24:00 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:00 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:00 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:00 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:00 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e0e0>]
INFO 01-25 02:24:00 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 8155020288}]
INFO 01-25 02:24:00 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:00 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.mlp.gate_proj.weight'
INFO 01-25 02:24:00 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.13.mlp.gate_proj.weight']
INFO 01-25 02:24:00 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:00 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:24:00 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:00 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:00 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:00 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:00 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:00 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:24:00 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:00 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:00 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:00 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:00 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:00 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:00 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:00 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:00 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:00 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:00 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:00 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:00 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:00 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:00 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:24:00 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:00 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:00 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:00 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:00 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:00 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:00 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:00 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:00 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:00 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:00 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:01 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:01 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:24:01 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:01 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e050>]
INFO 01-25 02:24:01 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:01 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:01 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:01 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:01 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e050>]
INFO 01-25 02:24:01 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:01 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:01 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.mlp.up_proj.weight'
INFO 01-25 02:24:01 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.13.mlp.up_proj.weight']
INFO 01-25 02:24:01 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:01 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:24:01 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:01 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:01 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:01 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:01 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:24:01 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:01 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:01 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:01 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:01 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:01 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:01 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:01 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:01 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:01 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:01 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:01 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:01 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:01 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:01 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:24:01 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:01 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:01 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:01 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:01 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:01 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:01 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:01 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:01 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:01 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:01 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:01 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:01 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:01 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e050>]
INFO 01-25 02:24:01 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:01 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:01 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:01 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:01 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e050>]
INFO 01-25 02:24:01 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:01 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:01 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.post_attention_layernorm.weight'
INFO 01-25 02:24:01 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.13.post_attention_layernorm.weight']
INFO 01-25 02:24:01 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:01 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:01 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:01 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:01 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:01 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:01 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:01 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:01 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:01 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:01 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:01 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:01 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:01 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:01 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:01 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:01 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:01 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:01 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.k_proj.weight'
INFO 01-25 02:24:01 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.13.self_attn.k_proj.weight']
INFO 01-25 02:24:01 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:01 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:24:01 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:01 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:01 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:01 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:01 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:24:01 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:01 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:01 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:01 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:01 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:01 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:01 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:01 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:01 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:01 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:01 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:01 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:01 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:01 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:01 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:01 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:01 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:01 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:01 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:01 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:01 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d750>]
INFO 01-25 02:24:01 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:01 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:01 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:01 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d750>]
INFO 01-25 02:24:01 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:01 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:01 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.o_proj.weight'
INFO 01-25 02:24:01 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.13.self_attn.o_proj.weight']
INFO 01-25 02:24:01 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:01 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:01 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:01 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:01 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:01 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:01 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:01 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:01 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:01 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:01 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:01 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:01 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:01 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:01 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:01 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dea0>]
INFO 01-25 02:24:01 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:01 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:01 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:01 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dea0>]
INFO 01-25 02:24:01 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:01 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:01 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.q_proj.weight'
INFO 01-25 02:24:01 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.13.self_attn.q_proj.weight']
INFO 01-25 02:24:01 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:01 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:01 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:01 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:01 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:01 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:01 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:01 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:01 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:01 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:01 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:01 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:01 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:01 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:01 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:01 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:01 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:01 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:01 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:01 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:01 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:01 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:01 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:01 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:01 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d750>]
INFO 01-25 02:24:01 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:01 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:01 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:01 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d750>]
INFO 01-25 02:24:01 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:01 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:01 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.13.self_attn.v_proj.weight'
INFO 01-25 02:24:01 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.13.self_attn.v_proj.weight']
INFO 01-25 02:24:01 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:01 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.13.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:01 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:01 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:01 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:01 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:01 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:01 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:01 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:01 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:01 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:01 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:01 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:01 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:01 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:01 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:01 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:01 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:01 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:01 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:01 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:01 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:01 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:02 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:02 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:02 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:02 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:02 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:02 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d750>]
INFO 01-25 02:24:02 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:02 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:02 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:02 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:02 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d750>]
INFO 01-25 02:24:02 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:02 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:02 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.input_layernorm.weight'
INFO 01-25 02:24:02 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.14.input_layernorm.weight']
INFO 01-25 02:24:02 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:02 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:02 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:02 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:02 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:02 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:02 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:02 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:02 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:02 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:02 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:02 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:02 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:02 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:02 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:02 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:02 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:02 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:02 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.mlp.down_proj.weight'
INFO 01-25 02:24:02 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.14.mlp.down_proj.weight']
INFO 01-25 02:24:02 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:02 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:02 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:02 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:02 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:02 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:02 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:02 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:02 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:02 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:24:02 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:02 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:02 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:02 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:02 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e290>]
INFO 01-25 02:24:02 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:02 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:02 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:02 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:02 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e290>]
INFO 01-25 02:24:02 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:02 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:02 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.mlp.gate_proj.weight'
INFO 01-25 02:24:02 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.14.mlp.gate_proj.weight']
INFO 01-25 02:24:02 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:02 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:24:02 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:02 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:02 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:02 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:02 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:24:02 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:02 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:02 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:02 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:02 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:02 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:02 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:02 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:02 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:02 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:24:02 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:02 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:02 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:02 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:02 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:02 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:02 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:02 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:24:02 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:02 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e200>]
INFO 01-25 02:24:02 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:02 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:02 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:02 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e200>]
INFO 01-25 02:24:02 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:02 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:02 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.mlp.up_proj.weight'
INFO 01-25 02:24:02 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.14.mlp.up_proj.weight']
INFO 01-25 02:24:02 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:02 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:24:02 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:02 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:02 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:02 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:02 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:24:02 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:02 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:02 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:02 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:02 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:02 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:02 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:02 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:02 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:02 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:24:02 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:02 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:02 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:02 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:02 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:02 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:02 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:02 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:02 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:02 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e200>]
INFO 01-25 02:24:02 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:02 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:02 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:02 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:02 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e200>]
INFO 01-25 02:24:02 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:02 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:02 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.post_attention_layernorm.weight'
INFO 01-25 02:24:02 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.14.post_attention_layernorm.weight']
INFO 01-25 02:24:02 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:02 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:02 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:02 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:02 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:02 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:02 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:02 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:02 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:02 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:02 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:02 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:02 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:02 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:02 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:02 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:02 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:02 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:02 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.k_proj.weight'
INFO 01-25 02:24:02 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.14.self_attn.k_proj.weight']
INFO 01-25 02:24:02 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:02 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:24:02 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:02 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:02 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:02 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:02 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:02 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:24:02 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:02 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:02 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:02 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:02 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:02 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:02 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:02 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:02 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:02 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:02 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:02 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:02 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:02 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:02 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:02 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:02 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:02 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:02 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:02 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:02 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:02 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:03 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:03 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:03 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:03 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e3b0>]
INFO 01-25 02:24:03 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:03 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:03 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:03 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e3b0>]
INFO 01-25 02:24:03 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:03 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:03 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.o_proj.weight'
INFO 01-25 02:24:03 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.14.self_attn.o_proj.weight']
INFO 01-25 02:24:03 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:03 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:03 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:03 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:03 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:03 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:03 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:03 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:03 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:03 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:03 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:03 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:03 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:03 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:03 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:03 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e440>]
INFO 01-25 02:24:03 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:03 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:03 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:03 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e440>]
INFO 01-25 02:24:03 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:03 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:03 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.q_proj.weight'
INFO 01-25 02:24:03 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.14.self_attn.q_proj.weight']
INFO 01-25 02:24:03 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:03 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:03 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:03 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:03 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:03 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:03 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:03 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:03 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:03 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:03 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:03 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:03 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:03 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:03 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:03 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:03 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:03 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:03 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:03 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:03 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:03 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:03 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:03 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:03 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e3b0>]
INFO 01-25 02:24:03 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:03 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:03 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:03 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e3b0>]
INFO 01-25 02:24:03 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:03 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:03 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.14.self_attn.v_proj.weight'
INFO 01-25 02:24:03 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.14.self_attn.v_proj.weight']
INFO 01-25 02:24:03 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:03 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.14.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:03 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:03 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:03 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:03 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:03 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:03 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:03 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:03 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:03 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:03 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:03 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:03 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:03 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:03 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:03 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:03 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:03 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:03 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:03 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:03 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:03 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:03 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:03 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:03 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:03 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:03 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e3b0>]
INFO 01-25 02:24:03 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:03 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:03 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:03 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:03 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e3b0>]
INFO 01-25 02:24:03 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:03 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:03 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.input_layernorm.weight'
INFO 01-25 02:24:03 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.15.input_layernorm.weight']
INFO 01-25 02:24:03 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:03 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:03 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:03 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:03 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:03 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:03 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:03 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:03 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:03 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:03 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:03 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:03 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:03 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:03 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:03 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:03 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:03 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:03 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.mlp.down_proj.weight'
INFO 01-25 02:24:03 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.15.mlp.down_proj.weight']
INFO 01-25 02:24:03 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:03 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:03 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:03 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:03 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:03 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:03 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:03 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:03 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:03 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:03 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:24:03 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:03 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:03 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:03 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:03 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e5f0>]
INFO 01-25 02:24:03 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:03 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:03 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:03 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:03 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e5f0>]
INFO 01-25 02:24:03 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:03 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:03 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.mlp.gate_proj.weight'
INFO 01-25 02:24:03 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.15.mlp.gate_proj.weight']
INFO 01-25 02:24:03 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:03 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:24:03 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:03 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:03 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:03 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:03 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:03 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:24:03 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:03 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:03 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:03 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:03 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:03 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:03 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:03 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:03 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:03 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:03 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:03 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:03 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:03 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:03 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:24:03 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:03 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:03 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:03 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:03 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:03 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:03 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:03 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:03 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:03 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:03 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:24:04 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:04 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e560>]
INFO 01-25 02:24:04 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:04 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:04 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:04 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:04 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e560>]
INFO 01-25 02:24:04 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:04 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:04 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.mlp.up_proj.weight'
INFO 01-25 02:24:04 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.15.mlp.up_proj.weight']
INFO 01-25 02:24:04 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:04 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:24:04 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:04 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:04 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:04 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:04 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:24:04 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:04 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:04 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:04 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:04 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:04 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:04 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:04 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:04 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:04 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:04 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:04 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:24:04 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:04 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:04 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:04 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:04 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:04 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:04 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:04 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:04 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e560>]
INFO 01-25 02:24:04 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:04 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:04 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:04 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:04 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e560>]
INFO 01-25 02:24:04 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:04 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:04 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.post_attention_layernorm.weight'
INFO 01-25 02:24:04 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.15.post_attention_layernorm.weight']
INFO 01-25 02:24:04 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:04 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:04 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:04 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:04 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:04 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:04 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:04 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:04 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:04 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:04 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:04 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:04 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:04 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:04 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:04 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:04 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:04 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:04 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.k_proj.weight'
INFO 01-25 02:24:04 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.15.self_attn.k_proj.weight']
INFO 01-25 02:24:04 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:04 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:24:04 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:04 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:04 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:04 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:04 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:24:04 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:04 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:04 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:04 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:04 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:04 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:04 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:04 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:04 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:04 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:04 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:04 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:04 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:04 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:04 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:04 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:04 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dcf0>]
INFO 01-25 02:24:04 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:04 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:04 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:04 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dcf0>]
INFO 01-25 02:24:04 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:04 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:04 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.o_proj.weight'
INFO 01-25 02:24:04 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.15.self_attn.o_proj.weight']
INFO 01-25 02:24:04 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:04 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:04 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:04 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:04 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:04 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:04 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:04 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:04 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:04 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:04 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:04 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:04 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:04 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:04 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e4d0>]
INFO 01-25 02:24:04 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:04 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:04 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:04 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e4d0>]
INFO 01-25 02:24:04 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:04 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:04 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.q_proj.weight'
INFO 01-25 02:24:04 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.15.self_attn.q_proj.weight']
INFO 01-25 02:24:04 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:04 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:04 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:04 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:04 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:04 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:04 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:04 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:04 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:04 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:04 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:04 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:04 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:04 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:04 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:04 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:04 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:04 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:04 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:04 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:04 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dcf0>]
INFO 01-25 02:24:04 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:04 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:04 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:04 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dcf0>]
INFO 01-25 02:24:04 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:04 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:04 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.15.self_attn.v_proj.weight'
INFO 01-25 02:24:04 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.15.self_attn.v_proj.weight']
INFO 01-25 02:24:04 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:04 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.15.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:04 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:04 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:04 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:04 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:04 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:04 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:04 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:04 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:04 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:04 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:04 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:04 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:04 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:04 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:04 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:04 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:04 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:04 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:04 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:04 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:04 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:04 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:04 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:04 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dcf0>]
INFO 01-25 02:24:04 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:04 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:04 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:04 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:04 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8dcf0>]
INFO 01-25 02:24:04 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:04 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:04 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.input_layernorm.weight'
INFO 01-25 02:24:04 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.16.input_layernorm.weight']
INFO 01-25 02:24:04 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:04 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:04 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:04 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:04 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:04 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:04 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:04 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:04 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:04 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:04 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:04 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:04 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:04 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:04 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:04 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:04 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:04 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:04 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.mlp.down_proj.weight'
INFO 01-25 02:24:04 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.16.mlp.down_proj.weight']
INFO 01-25 02:24:04 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:04 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:04 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:04 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:04 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:04 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:04 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:04 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:04 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:04 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:04 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:04 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:04 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:04 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:04 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:04 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:04 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:04 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:04 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:05 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:24:05 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:05 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:05 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:05 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:05 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e7a0>]
INFO 01-25 02:24:05 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:05 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:05 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:05 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:05 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e7a0>]
INFO 01-25 02:24:05 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:05 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:05 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.mlp.gate_proj.weight'
INFO 01-25 02:24:05 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.16.mlp.gate_proj.weight']
INFO 01-25 02:24:05 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:05 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:24:05 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:05 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:05 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:05 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:05 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:24:05 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:05 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:05 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:05 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:05 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:05 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:05 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:05 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:05 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:05 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:24:05 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:05 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:05 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:05 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:05 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:05 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:05 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:05 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:24:05 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:05 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e710>]
INFO 01-25 02:24:05 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:05 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:05 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:05 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e710>]
INFO 01-25 02:24:05 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:05 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:05 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.mlp.up_proj.weight'
INFO 01-25 02:24:05 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.16.mlp.up_proj.weight']
INFO 01-25 02:24:05 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:05 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:24:05 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:05 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:05 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:05 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:05 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:24:05 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:05 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:05 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:05 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:05 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:05 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:05 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:05 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:05 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:05 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:24:05 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:05 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:05 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:05 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:05 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:05 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:05 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:05 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:05 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:05 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e710>]
INFO 01-25 02:24:05 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:05 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:05 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:05 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:05 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e710>]
INFO 01-25 02:24:05 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:05 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:05 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.post_attention_layernorm.weight'
INFO 01-25 02:24:05 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.16.post_attention_layernorm.weight']
INFO 01-25 02:24:05 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:05 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:05 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:05 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:05 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:05 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:05 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:05 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:05 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:05 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:05 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:05 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:05 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:05 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:05 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:05 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:05 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:05 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:05 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.k_proj.weight'
INFO 01-25 02:24:05 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.16.self_attn.k_proj.weight']
INFO 01-25 02:24:05 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:05 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:24:05 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:05 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:05 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:05 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:05 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:24:05 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:05 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:05 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:05 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:05 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:05 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:05 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:05 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:05 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:05 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:05 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:05 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:05 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:05 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:05 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:05 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:05 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:05 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:05 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:05 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:05 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:05 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e8c0>]
INFO 01-25 02:24:05 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:05 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:05 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:05 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:05 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e8c0>]
INFO 01-25 02:24:05 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:05 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:05 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.o_proj.weight'
INFO 01-25 02:24:05 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.16.self_attn.o_proj.weight']
INFO 01-25 02:24:05 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:05 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:05 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:05 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:05 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:05 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:05 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:05 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:05 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:05 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:05 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:05 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:05 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:05 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:05 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:05 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e950>]
INFO 01-25 02:24:05 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:05 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:05 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:05 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e950>]
INFO 01-25 02:24:05 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:05 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:05 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.q_proj.weight'
INFO 01-25 02:24:05 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.16.self_attn.q_proj.weight']
INFO 01-25 02:24:05 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:05 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:05 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:05 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:05 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:05 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:05 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:05 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:05 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:05 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:05 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:05 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:05 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:05 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:05 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:05 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:05 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:05 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:05 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:05 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:06 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:06 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:06 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:06 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:06 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:06 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:06 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e8c0>]
INFO 01-25 02:24:06 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:06 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:06 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:06 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e8c0>]
INFO 01-25 02:24:06 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:06 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:06 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.16.self_attn.v_proj.weight'
INFO 01-25 02:24:06 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.16.self_attn.v_proj.weight']
INFO 01-25 02:24:06 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:06 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.16.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:06 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:06 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:06 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:06 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:06 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:06 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:06 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:06 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:06 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:06 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:06 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:06 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:06 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:06 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:06 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:06 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:06 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:06 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:06 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:06 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:06 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:06 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e8c0>]
INFO 01-25 02:24:06 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:06 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:06 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:06 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e8c0>]
INFO 01-25 02:24:06 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:06 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:06 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.input_layernorm.weight'
INFO 01-25 02:24:06 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.17.input_layernorm.weight']
INFO 01-25 02:24:06 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:06 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:06 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:06 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:06 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:06 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:06 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:06 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:06 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:06 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:06 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:06 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:06 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:06 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:06 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:06 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:06 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 6907214848}]
INFO 01-25 02:24:06 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:06 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.mlp.down_proj.weight'
INFO 01-25 02:24:06 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.17.mlp.down_proj.weight']
INFO 01-25 02:24:06 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:06 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:06 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:06 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:06 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:06 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:06 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:06 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:06 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:06 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:24:06 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:06 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:06 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:06 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:06 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ea70>]
INFO 01-25 02:24:06 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:06 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:06 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:06 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:06 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ea70>]
INFO 01-25 02:24:06 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:06 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:06 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.mlp.gate_proj.weight'
INFO 01-25 02:24:06 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.17.mlp.gate_proj.weight']
INFO 01-25 02:24:06 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:06 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:24:06 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:06 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:06 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:06 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:06 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:24:06 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:06 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:06 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:06 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:06 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:06 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:06 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:06 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:06 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:24:06 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:06 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:06 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:06 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:06 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:06 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:24:06 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:06 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e9e0>]
INFO 01-25 02:24:06 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:06 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:06 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:06 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e9e0>]
INFO 01-25 02:24:06 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:06 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:06 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.mlp.up_proj.weight'
INFO 01-25 02:24:06 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.17.mlp.up_proj.weight']
INFO 01-25 02:24:06 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:06 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:24:06 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:06 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:06 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:06 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:06 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:24:06 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:06 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:06 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:06 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:06 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:06 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:06 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:06 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:06 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:24:06 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:06 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:06 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:06 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:06 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:06 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:06 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:06 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:06 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:06 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e9e0>]
INFO 01-25 02:24:06 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:06 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:06 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:06 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:06 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e9e0>]
INFO 01-25 02:24:06 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:06 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:06 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.post_attention_layernorm.weight'
INFO 01-25 02:24:06 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.17.post_attention_layernorm.weight']
INFO 01-25 02:24:06 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:06 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:06 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:06 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:06 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:06 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:06 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:06 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:06 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:06 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:06 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:06 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:06 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:06 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:06 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:06 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:06 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:06 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:06 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.k_proj.weight'
INFO 01-25 02:24:06 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.17.self_attn.k_proj.weight']
INFO 01-25 02:24:06 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:06 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:24:06 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:06 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:06 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:06 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:06 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:06 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:24:06 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:06 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:06 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:06 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:06 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:06 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:06 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:06 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:06 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:06 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:06 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:06 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:07 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:07 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:07 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:07 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:07 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:07 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:07 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:07 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:07 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:07 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:07 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e170>]
INFO 01-25 02:24:07 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:07 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:07 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:07 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e170>]
INFO 01-25 02:24:07 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:07 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:07 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.o_proj.weight'
INFO 01-25 02:24:07 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.17.self_attn.o_proj.weight']
INFO 01-25 02:24:07 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:07 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:07 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:07 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:07 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:07 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:07 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:07 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:07 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:07 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:07 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:07 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:07 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:07 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:07 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:07 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e830>]
INFO 01-25 02:24:07 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:07 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:07 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:07 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e830>]
INFO 01-25 02:24:07 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:07 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:07 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.q_proj.weight'
INFO 01-25 02:24:07 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.17.self_attn.q_proj.weight']
INFO 01-25 02:24:07 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:07 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:07 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:07 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:07 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:07 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:07 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:07 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:07 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:07 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:07 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:07 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:07 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:07 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:07 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:07 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:07 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:07 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:07 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:07 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:07 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:07 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:07 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:07 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:07 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e170>]
INFO 01-25 02:24:07 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:07 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:07 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:07 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e170>]
INFO 01-25 02:24:07 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:07 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:07 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.17.self_attn.v_proj.weight'
INFO 01-25 02:24:07 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.17.self_attn.v_proj.weight']
INFO 01-25 02:24:07 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:07 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.17.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:07 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:07 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:07 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:07 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:07 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:07 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:07 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:07 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:07 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:07 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:07 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:07 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:07 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:07 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:07 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:07 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:07 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:07 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:07 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:07 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:07 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:07 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:07 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:07 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:07 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:07 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e170>]
INFO 01-25 02:24:07 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:07 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:07 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:07 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:07 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e170>]
INFO 01-25 02:24:07 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:07 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:07 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.input_layernorm.weight'
INFO 01-25 02:24:07 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.18.input_layernorm.weight']
INFO 01-25 02:24:07 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:07 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:07 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:07 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:07 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:07 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:07 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:07 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:07 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:07 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:07 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:07 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:07 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:07 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:07 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:07 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:07 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:07 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:07 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.mlp.down_proj.weight'
INFO 01-25 02:24:07 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.18.mlp.down_proj.weight']
INFO 01-25 02:24:07 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:07 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:07 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:07 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:07 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:07 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:07 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:07 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:07 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:07 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:07 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:24:07 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:07 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:07 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:07 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:07 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ec20>]
INFO 01-25 02:24:07 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:07 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:07 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:07 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:07 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ec20>]
INFO 01-25 02:24:07 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:07 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:07 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.mlp.gate_proj.weight'
INFO 01-25 02:24:07 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.18.mlp.gate_proj.weight']
INFO 01-25 02:24:07 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:07 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:24:07 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:07 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:07 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:07 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:07 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:07 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:24:07 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:07 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:07 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:07 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:07 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:07 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:07 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:07 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:07 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:07 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:07 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:07 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:07 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:07 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:07 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:24:07 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:07 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:07 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:07 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:07 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:07 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:08 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:08 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:08 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:08 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:24:08 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:08 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb90>]
INFO 01-25 02:24:08 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:08 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:08 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:08 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:08 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb90>]
INFO 01-25 02:24:08 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:08 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:08 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.mlp.up_proj.weight'
INFO 01-25 02:24:08 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.18.mlp.up_proj.weight']
INFO 01-25 02:24:08 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:08 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:24:08 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:08 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:08 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:08 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:08 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:24:08 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:08 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:08 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:08 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:08 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:08 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:08 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:08 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:08 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:08 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:08 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:08 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:08 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:24:08 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:08 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:08 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:08 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:08 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:08 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:08 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:08 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:08 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:08 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:08 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb90>]
INFO 01-25 02:24:08 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:08 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:08 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:08 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:08 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eb90>]
INFO 01-25 02:24:08 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:08 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:08 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.post_attention_layernorm.weight'
INFO 01-25 02:24:08 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.18.post_attention_layernorm.weight']
INFO 01-25 02:24:08 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:08 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:08 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:08 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:08 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:08 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:08 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:08 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:08 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:08 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:08 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:08 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:08 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:08 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:08 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:08 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:08 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:08 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:08 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.k_proj.weight'
INFO 01-25 02:24:08 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.18.self_attn.k_proj.weight']
INFO 01-25 02:24:08 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:08 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:24:08 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:08 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:08 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:08 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:08 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:24:08 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:08 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:08 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:08 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:08 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:08 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:08 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:08 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:08 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:08 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:08 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:08 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:08 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:08 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:08 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:08 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:08 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:08 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:08 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ed40>]
INFO 01-25 02:24:08 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:08 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:08 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:08 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ed40>]
INFO 01-25 02:24:08 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:08 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:08 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.o_proj.weight'
INFO 01-25 02:24:08 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.18.self_attn.o_proj.weight']
INFO 01-25 02:24:08 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:08 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:08 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:08 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:08 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:08 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:08 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:08 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:08 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:08 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:08 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:08 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:08 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:08 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:08 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8edd0>]
INFO 01-25 02:24:08 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:08 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:08 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:08 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8edd0>]
INFO 01-25 02:24:08 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:08 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:08 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.q_proj.weight'
INFO 01-25 02:24:08 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.18.self_attn.q_proj.weight']
INFO 01-25 02:24:08 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:08 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:08 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:08 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:08 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:08 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:08 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:08 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:08 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:08 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:08 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:08 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:08 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:08 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:08 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:08 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:08 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:08 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:08 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:08 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:08 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:08 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:08 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:08 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ed40>]
INFO 01-25 02:24:08 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:08 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:08 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:08 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ed40>]
INFO 01-25 02:24:08 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:08 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:08 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.18.self_attn.v_proj.weight'
INFO 01-25 02:24:08 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.18.self_attn.v_proj.weight']
INFO 01-25 02:24:08 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:08 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.18.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:08 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:08 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:08 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:08 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:08 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:08 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:08 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:08 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:08 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:08 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:08 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:08 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:08 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:08 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:08 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:08 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:08 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:08 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:09 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:09 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:09 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:09 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:09 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:09 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:09 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:09 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:09 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:09 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:09 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:09 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:09 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ed40>]
INFO 01-25 02:24:09 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:09 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:09 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:09 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:09 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ed40>]
INFO 01-25 02:24:09 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:09 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:09 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.input_layernorm.weight'
INFO 01-25 02:24:09 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.19.input_layernorm.weight']
INFO 01-25 02:24:09 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:09 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:09 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:09 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:09 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:09 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:09 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:09 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:09 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:09 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:09 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:09 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:09 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:09 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:09 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:09 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:09 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:09 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:09 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.mlp.down_proj.weight'
INFO 01-25 02:24:09 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.19.mlp.down_proj.weight']
INFO 01-25 02:24:09 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:09 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:09 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:09 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:09 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:09 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:09 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:09 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:09 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:09 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:09 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:24:09 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:09 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:09 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:09 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:09 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eef0>]
INFO 01-25 02:24:09 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:09 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:09 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:09 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:09 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8eef0>]
INFO 01-25 02:24:09 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:09 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:09 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.mlp.gate_proj.weight'
INFO 01-25 02:24:09 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.19.mlp.gate_proj.weight']
INFO 01-25 02:24:09 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:09 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:24:09 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:09 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:09 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:09 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:09 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:24:09 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:09 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:09 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:09 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:09 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:09 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:09 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:09 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:09 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:09 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:09 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:09 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:24:09 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:09 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:09 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:09 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:09 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:09 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:09 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:09 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:09 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:09 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:24:09 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:09 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ee60>]
INFO 01-25 02:24:09 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:09 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:09 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:09 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ee60>]
INFO 01-25 02:24:09 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:09 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:09 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.mlp.up_proj.weight'
INFO 01-25 02:24:09 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.19.mlp.up_proj.weight']
INFO 01-25 02:24:09 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:09 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:24:09 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:09 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:09 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:09 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:09 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:24:09 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:09 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:09 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:09 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:09 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:09 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:09 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:09 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:09 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:09 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:09 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:09 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:24:09 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:09 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:09 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:09 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:09 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:09 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:09 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:09 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:09 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:09 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:09 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:09 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ee60>]
INFO 01-25 02:24:09 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:09 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:09 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:09 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:09 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ee60>]
INFO 01-25 02:24:09 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:09 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:09 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.post_attention_layernorm.weight'
INFO 01-25 02:24:09 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.19.post_attention_layernorm.weight']
INFO 01-25 02:24:09 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:09 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:09 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:09 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:09 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:09 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:09 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:09 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:09 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:09 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:09 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:09 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:09 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:09 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:09 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:09 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:09 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:09 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:09 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.k_proj.weight'
INFO 01-25 02:24:09 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.19.self_attn.k_proj.weight']
INFO 01-25 02:24:09 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:09 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:24:09 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:09 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:09 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:09 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:09 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:09 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:24:09 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:09 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:09 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:09 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:09 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:09 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:09 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:09 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:09 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:09 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:09 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:09 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:09 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:09 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:09 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:09 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:09 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:10 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:10 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:10 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:10 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e680>]
INFO 01-25 02:24:10 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:10 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:10 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:10 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e680>]
INFO 01-25 02:24:10 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:10 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:10 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.o_proj.weight'
INFO 01-25 02:24:10 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.19.self_attn.o_proj.weight']
INFO 01-25 02:24:10 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:10 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:10 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:10 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:10 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:10 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:10 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:10 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:10 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:10 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:10 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:10 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:10 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:10 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:10 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ecb0>]
INFO 01-25 02:24:10 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:10 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:10 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:10 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ecb0>]
INFO 01-25 02:24:10 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:10 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:10 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.q_proj.weight'
INFO 01-25 02:24:10 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.19.self_attn.q_proj.weight']
INFO 01-25 02:24:10 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:10 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:10 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:10 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:10 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:10 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:10 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:10 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:10 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:10 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:10 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:10 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:10 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:10 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:10 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:10 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e680>]
INFO 01-25 02:24:10 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:10 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:10 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:10 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e680>]
INFO 01-25 02:24:10 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:10 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:10 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.19.self_attn.v_proj.weight'
INFO 01-25 02:24:10 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.19.self_attn.v_proj.weight']
INFO 01-25 02:24:10 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.19.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:10 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:10 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:10 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:10 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:10 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:10 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:10 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:10 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:10 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:10 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:10 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:10 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:10 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:10 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:10 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e680>]
INFO 01-25 02:24:10 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:10 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:10 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:10 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8e680>]
INFO 01-25 02:24:10 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:10 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:10 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.mlp.gate_proj.weight'
INFO 01-25 02:24:10 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.20.mlp.gate_proj.weight']
INFO 01-25 02:24:10 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:10 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:24:10 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:10 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:10 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:10 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:10 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:24:10 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:10 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:10 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:10 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:10 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:10 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:10 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:10 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:24:10 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:10 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:10 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:10 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:10 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:10 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:10 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:24:10 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:10 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f010>]
INFO 01-25 02:24:10 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:10 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:10 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:10 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:10 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f010>]
INFO 01-25 02:24:10 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:10 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:10 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.k_proj.weight'
INFO 01-25 02:24:10 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.20.self_attn.k_proj.weight']
INFO 01-25 02:24:10 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:24:10 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:10 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:10 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:10 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:24:10 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:10 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:10 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:10 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:10 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:10 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:10 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:10 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:10 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:10 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:10 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f1c0>]
INFO 01-25 02:24:10 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:10 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:10 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:10 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f1c0>]
INFO 01-25 02:24:10 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:10 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:10 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.o_proj.weight'
INFO 01-25 02:24:10 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.20.self_attn.o_proj.weight']
INFO 01-25 02:24:10 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:10 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:10 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:10 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:10 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:10 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:10 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:10 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:10 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:10 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:10 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:10 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:10 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:10 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:10 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f250>]
INFO 01-25 02:24:10 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:10 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:10 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:10 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f250>]
INFO 01-25 02:24:10 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:10 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:10 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.q_proj.weight'
INFO 01-25 02:24:10 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.20.self_attn.q_proj.weight']
INFO 01-25 02:24:10 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:10 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:10 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:10 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:10 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:10 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:10 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:10 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:10 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:10 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:10 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:10 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:10 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:10 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:10 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:10 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:10 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:10 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:10 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f1c0>]
INFO 01-25 02:24:10 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:10 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:10 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:10 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f1c0>]
INFO 01-25 02:24:10 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:10 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:10 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.20.self_attn.v_proj.weight'
INFO 01-25 02:24:10 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.20.self_attn.v_proj.weight']
INFO 01-25 02:24:10 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.20.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:10 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:10 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:10 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:10 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:10 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:10 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:10 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:10 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:10 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:10 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:10 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:10 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:10 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:10 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:10 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:10 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:11 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:11 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:11 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:11 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:11 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:11 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:11 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:11 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:11 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:11 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:11 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:11 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:11 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f1c0>]
INFO 01-25 02:24:11 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:11 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:11 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:11 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:11 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8f1c0>]
INFO 01-25 02:24:11 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:11 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:11 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.input_layernorm.weight'
INFO 01-25 02:24:11 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.9.input_layernorm.weight']
INFO 01-25 02:24:11 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:11 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:11 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.input_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:11 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:11 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:11 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:11 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:11 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:11 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:11 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:11 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:11 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:11 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:11 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:11 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:11 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:11 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 5365799936}]
INFO 01-25 02:24:11 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:11 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.mlp.down_proj.weight'
INFO 01-25 02:24:11 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.9.mlp.down_proj.weight']
INFO 01-25 02:24:11 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:11 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.mlp.down_proj.weight'] [loaded_weight.shape=torch.Size([4096, 14336])] [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:11 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:11 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:11 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:11 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:11 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:11 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:11 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:11 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:11 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 14336])] - dtype='uint16'
INFO 01-25 02:24:11 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:11 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:11 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:11 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:11 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d6c0>]
INFO 01-25 02:24:11 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:11 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:11 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:11 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 14336])]
INFO 01-25 02:24:11 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d6c0>]
INFO 01-25 02:24:11 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:11 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:11 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.mlp.gate_proj.weight'
INFO 01-25 02:24:11 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.9.mlp.gate_proj.weight']
INFO 01-25 02:24:11 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:11 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=0]
INFO 01-25 02:24:11 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:11 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:11 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:11 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:11 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=0]
INFO 01-25 02:24:11 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:11 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:11 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:11 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:11 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:11 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:11 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:11 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:11 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:11 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:11 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:11 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=0]
INFO 01-25 02:24:11 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:11 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:11 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:11 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:11 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:11 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:11 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:11 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:11 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:11 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.gate_proj']
INFO 01-25 02:24:11 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:11 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d630>]
INFO 01-25 02:24:11 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:11 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:11 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:11 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d630>]
INFO 01-25 02:24:11 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:11 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:11 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.mlp.up_proj.weight'
INFO 01-25 02:24:11 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.9.mlp.up_proj.weight']
INFO 01-25 02:24:11 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:11 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.mlp.gate_up_proj.weight'] [loaded_weight.shape=torch.Size([14336, 4096])] [param.shape=torch.Size([28672, 4096])] [shard_id=1]
INFO 01-25 02:24:11 linear.py:481] hosseins: MergedColumnParallelLinear -> weight_loader
INFO 01-25 02:24:11 linear.py:490] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:11 linear.py:491] hosseins: MergedColumnParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:11 linear.py:492] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 linear.py:493] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:11 linear.py:494] hosseins: MergedColumnParallelLinear -> weight_loader() [loaded_shard_id=1]
INFO 01-25 02:24:11 linear.py:495] hosseins: MergedColumnParallelLinear -> weight_loader() [self.output_sizes=[14336, 14336]]
INFO 01-25 02:24:11 linear.py:496] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:497] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 linear.py:498] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 linear.py:534] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:11 linear.py:535] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:11 linear.py:536] hosseins: MergedColumnParallelLinear -> weight_loader() 1 before narrow [param_data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 linear.py:587] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:11 linear.py:588] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:11 linear.py:597] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:11 linear.py:608] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:11 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:11 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:11 linear.py:621] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:11 linear.py:622] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:11 linear.py:623] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_offset=14336]
INFO 01-25 02:24:11 linear.py:624] hosseins: MergedColumnParallelLinear -> weight_loader() 1 [shard_size=14336]
INFO 01-25 02:24:11 linear.py:651] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:652] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:653] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 linear.py:654] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:11 linear.py:655] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([14336, 4096])]
INFO 01-25 02:24:11 linear.py:656] hosseins: MergedColumnParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:11 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:11 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([14336, 4096])] - dtype='uint16'
INFO 01-25 02:24:11 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=58720256] - dtype_size=2
INFO 01-25 02:24:11 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:11 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:11 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:11 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:11 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d630>]
INFO 01-25 02:24:11 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:11 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:11 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:11 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([28672, 4096])]
INFO 01-25 02:24:11 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d630>]
INFO 01-25 02:24:11 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:11 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:11 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.post_attention_layernorm.weight'
INFO 01-25 02:24:11 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.9.post_attention_layernorm.weight']
INFO 01-25 02:24:11 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096])]
INFO 01-25 02:24:11 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:11 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.post_attention_layernorm.weight'] [loaded_weight.shape=torch.Size([4096])] [param.shape=torch.Size([4096])]
INFO 01-25 02:24:11 weight_utils.py:572] hosseins: weight_utils.py -> default_weight_loader()
INFO 01-25 02:24:11 weight_utils.py:590] hosseins: weight_utils.py -> default_weight_loader() [param.data.shape=torch.Size([4096])]
INFO 01-25 02:24:11 weight_utils.py:591] hosseins: weight_utils.py -> default_weight_loader() [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096])] - dtype='uint16'
INFO 01-25 02:24:11 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4096] - dtype_size=2
INFO 01-25 02:24:11 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌──────────────────────────────┐
│                              │
│ TPU [0, 1, 2, 3, 4, 5, 6, 7] │
│                              │
└──────────────────────────────┘
INFO 01-25 02:24:11 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:11 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:11 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:11 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:11 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:11 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:11 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096])]
INFO 01-25 02:24:11 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function default_weight_loader at 0x7af1ab86e170>]
INFO 01-25 02:24:11 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:11 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:11 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.k_proj.weight'
INFO 01-25 02:24:11 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.9.self_attn.k_proj.weight']
INFO 01-25 02:24:11 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:11 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='k']
INFO 01-25 02:24:11 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:11 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:11 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:11 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:11 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:11 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='k']
INFO 01-25 02:24:11 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:11 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:11 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:11 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:11 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:11 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:11 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:11 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:11 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:11 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:11 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:11 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:11 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:11 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:11 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:11 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:11 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:12 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:12 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:12 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:12 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.k_proj']
INFO 01-25 02:24:12 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:12 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ce50>]
INFO 01-25 02:24:12 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:12 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:12 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:12 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ce50>]
INFO 01-25 02:24:12 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:12 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:12 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.o_proj.weight'
INFO 01-25 02:24:12 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.9.self_attn.o_proj.weight']
INFO 01-25 02:24:12 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 weight_utils.py:692] hosseins: weight_utils.py -> maybe_remap_kv_scale_name()
INFO 01-25 02:24:12 llama.py:548] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.self_attn.o_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:1187] hosseins: RowParallelLinear -> weight_loader()
INFO 01-25 02:24:12 linear.py:1199] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:12 linear.py:1200] hosseins: RowParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:12 linear.py:1201] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 linear.py:1202] hosseins: RowParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:1203] hosseins: RowParallelLinear -> weight_loader() [input_dim=1]
INFO 01-25 02:24:12 linear.py:1204] hosseins: RowParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:1205] hosseins: RowParallelLinear -> weight_loader() 1 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:1206] hosseins: RowParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:1216] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:1227] hosseins: RowParallelLinear -> weight_loader() 1 [input_dim=1]
INFO 01-25 02:24:12 linear.py:1228] hosseins: RowParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:12 linear.py:1229] hosseins: RowParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:12 linear.py:1240] hosseins: RowParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:1241] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:1242] hosseins: RowParallelLinear -> weight_loader() 2 [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:1243] hosseins: RowParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:1244] hosseins: RowParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:1245] hosseins: RowParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:24:12 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[8,1]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:12 utils.py:287] hosseins: after sharding param
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:12 llama.py:557] hosseins: LlamaModel -> load_weights() - else:
┌───────┐
│       │
│ TPU 0 │
│       │
├───────┤
│       │
│ TPU 1 │
│       │
├───────┤
│       │
│ TPU 2 │
│       │
├───────┤
│       │
│ TPU 3 │
│       │
├───────┤
│       │
│ TPU 4 │
│       │
├───────┤
│       │
│ TPU 5 │
│       │
├───────┤
│       │
│ TPU 6 │
│       │
├───────┤
│       │
│ TPU 7 │
│       │
└───────┘
INFO 01-25 02:24:12 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.up_proj']
INFO 01-25 02:24:12 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:12 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d480>]
INFO 01-25 02:24:12 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=True]
INFO 01-25 02:24:12 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:12 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.gate_up_proj']
INFO 01-25 02:24:12 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8d480>]
INFO 01-25 02:24:12 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:12 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:12 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.q_proj.weight'
INFO 01-25 02:24:12 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.9.self_attn.q_proj.weight']
INFO 01-25 02:24:12 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([4096, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='q']
INFO 01-25 02:24:12 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:12 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:12 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:12 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='q']
INFO 01-25 02:24:12 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:12 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:12 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:12 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:12 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:12 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:12 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:12 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:12 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:12 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=4096]
INFO 01-25 02:24:12 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([4096, 4096])]
INFO 01-25 02:24:12 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:12 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([4096, 4096])] - dtype='uint16'
INFO 01-25 02:24:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=16777216] - dtype_size=2
INFO 01-25 02:24:12 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:12 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:12 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.q_proj']
INFO 01-25 02:24:12 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:12 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ce50>]
INFO 01-25 02:24:12 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:12 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:12 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:12 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ce50>]
INFO 01-25 02:24:12 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:12 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:12 llama.py:781] hosseins: LlamaForCausalLM -> maybe_remap_mistral name='model.layers.9.self_attn.v_proj.weight'
INFO 01-25 02:24:12 llama.py:478] hosseins: LlamaModel -> load_weights() [name='layers.9.self_attn.v_proj.weight']
INFO 01-25 02:24:12 llama.py:479] hosseins: LlamaModel -> load_weights() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:12 llama.py:480] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 llama.py:522] hosseins: LlamaModel -> load_weights() X calling weight_loader() for [name='layers.9.self_attn.qkv_proj.weight'] [loaded_weight.shape=torch.Size([1024, 4096])] [param.shape=torch.Size([6144, 4096])] [shard_id='v']
INFO 01-25 02:24:12 linear.py:898] hosseins: QKVParallelLinear -> weight_loader()
INFO 01-25 02:24:12 linear.py:905] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight=False]
INFO 01-25 02:24:12 linear.py:906] hosseins: QKVParallelLinear -> weight_loader() [is_gguf_weight_type=False]
INFO 01-25 02:24:12 linear.py:907] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 linear.py:908] hosseins: QKVParallelLinear -> weight_loader() [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:12 linear.py:909] hosseins: QKVParallelLinear -> weight_loader() [loaded_shard_id='v']
INFO 01-25 02:24:12 linear.py:910] hosseins: QKVParallelLinear -> weight_loader() [self.output_sizes=[4096, 1024, 1024]]
INFO 01-25 02:24:12 linear.py:911] hosseins: QKVParallelLinear -> weight_loader() 1 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:912] hosseins: QKVParallelLinear -> weight_loader() 1 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 linear.py:913] hosseins: QKVParallelLinear -> weight_loader() 1 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 linear.py:948] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:12 linear.py:949] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:12 linear.py:1013] hosseins: QKVParallelLinear -> weight_loader() 1 [is_metadata=False]
INFO 01-25 02:24:12 linear.py:1014] hosseins: QKVParallelLinear -> weight_loader() 1 [needs_scalar_to_array=False]
INFO 01-25 02:24:12 linear.py:1032] hosseins: QKVParallelLinear -> weight_loader() 1 [packed_dim=None]
INFO 01-25 02:24:12 linear.py:1044] hosseins: QKVParallelLinear -> weight_loader() 1 [use_bitsandbytes_4bit=False]
INFO 01-25 02:24:12 linear.py:1069] hosseins: QKVParallelLinear -> weight_loader() 1 [output_dim=0]
INFO 01-25 02:24:12 linear.py:1070] hosseins: QKVParallelLinear -> weight_loader() 1 [start_idx=0]
INFO 01-25 02:24:12 linear.py:1071] hosseins: QKVParallelLinear -> weight_loader() 1 [shard_size=1024]
INFO 01-25 02:24:12 linear.py:1101] hosseins: QKVParallelLinear -> weight_loader() 2 [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:1102] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 linear.py:1103] hosseins: QKVParallelLinear -> weight_loader() 2 [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 linear.py:1104] hosseins: QKVParallelLinear -> weight_loader() 2 [loaded_weight.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:12 linear.py:1105] hosseins: QKVParallelLinear -> weight_loader() 2 [param_data.shape=torch.Size([1024, 4096])]
INFO 01-25 02:24:12 linear.py:1106] hosseins: QKVParallelLinear -> weight_loader() 2 [param.data.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:12 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:12 llama.py:92] hosseins: estimate_tensor_memory-> [shape=torch.Size([1024, 4096])] - dtype='uint16'
INFO 01-25 02:24:12 llama.py:99] hosseins: estimate_tensor_memory-> [num_elements=4194304] - dtype_size=2
INFO 01-25 02:24:12 llama.py:529] hosseins: LlamaModel -> load_weights() - param_name, weight_name, shard_id in stacked_params_mapping [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 utils.py:284] hosseins: shard_spmd() -> [sharding='{devices=[1,8]0,1,2,3,4,5,6,7}']
INFO 01-25 02:24:12 utils.py:287] hosseins: after sharding param
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘
INFO 01-25 02:24:12 llama.py:562] hosseins: LlamaModel -> load_weights() [weight_name='.v_proj']
INFO 01-25 02:24:12 llama.py:563] hosseins: LlamaModel -> load_weights() [type(param)=<class 'torch.nn.parameter.Parameter'>]
INFO 01-25 02:24:12 llama.py:564] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ce50>]
INFO 01-25 02:24:12 llama.py:565] hosseins: LlamaModel -> load_weights() [loaded_weight.shape==param.shape=False]
INFO 01-25 02:24:12 llama.py:566] hosseins: LlamaModel -> load_weights() [loaded_weight.device=device(type='cpu')]
INFO 01-25 02:24:12 llama.py:567] hosseins: LlamaModel -> load_weights() [type(loaded_weight)=<class 'torch.Tensor'>]
INFO 01-25 02:24:12 llama.py:568] hosseins: LlamaModel -> load_weights() [param_name='.qkv_proj']
INFO 01-25 02:24:12 llama.py:569] hosseins: LlamaModel -> load_weights() [param.shape=torch.Size([6144, 4096])]
INFO 01-25 02:24:12 llama.py:570] hosseins: LlamaModel -> load_weights() [param.device=device(type='xla', index=0)]
INFO 01-25 02:24:12 llama.py:571] hosseins: LlamaModel -> load_weights() [weight_loader=<function _make_synced_weight_loader.<locals>._synced_weight_loader at 0x7af14dd8ce50>]
INFO 01-25 02:24:12 llama.py:574] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:12 llama.py:575] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:59<00:00, 15.94s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:59<00:00, 14.99s/it]

INFO 01-25 02:24:12 llama.py:577] hosseins: LlamaModel -> load_weights() [len(loaded_params)=194]
INFO 01-25 02:24:12 llama.py:578] hosseins: LlamaModel -> load_weights() [loaded_params={'embed_tokens.weight', 'layers.31.post_attention_layernorm.weight', 'layers.30.post_attention_layernorm.weight', 'layers.5.mlp.gate_up_proj.weight', 'layers.14.self_attn.qkv_proj.weight', 'layers.17.mlp.gate_up_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.22.self_attn.qkv_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.28.input_layernorm.weight', 'layers.1.self_attn.qkv_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.5.self_attn.qkv_proj.weight', 'layers.17.self_attn.qkv_proj.weight', 'layers.25.self_attn.qkv_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.3.self_attn.o_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.19.self_attn.o_proj.weight', 'layers.26.input_layernorm.weight', 'layers.25.self_attn.o_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.18.self_attn.qkv_proj.weight', 'layers.4.self_attn.qkv_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.23.mlp.down_proj.weight', 'layers.25.input_layernorm.weight', 'layers.12.self_attn.qkv_proj.weight', 'layers.11.input_layernorm.weight', 'layers.7.self_attn.o_proj.weight', 'layers.17.input_layernorm.weight', 'layers.29.input_layernorm.weight', 'layers.29.self_attn.o_proj.weight', 'layers.19.self_attn.qkv_proj.weight', 'layers.3.mlp.gate_up_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.27.input_layernorm.weight', 'layers.8.post_attention_layernorm.weight', 'layers.13.mlp.gate_up_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.0.mlp.down_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.post_attention_layernorm.weight', 'layers.5.input_layernorm.weight', 'layers.28.self_attn.qkv_proj.weight', 'layers.7.self_attn.qkv_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.3.self_attn.qkv_proj.weight', 'layers.31.input_layernorm.weight', 'layers.26.post_attention_layernorm.weight', 'layers.30.mlp.gate_up_proj.weight', 'layers.27.mlp.gate_up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.20.mlp.gate_up_proj.weight', 'layers.14.input_layernorm.weight', 'layers.19.input_layernorm.weight', 'layers.20.input_layernorm.weight', 'layers.24.post_attention_layernorm.weight', 'layers.9.mlp.gate_up_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.13.mlp.down_proj.weight', 'layers.14.mlp.gate_up_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.7.input_layernorm.weight', 'layers.25.post_attention_layernorm.weight', 'layers.18.input_layernorm.weight', 'layers.9.self_attn.qkv_proj.weight', 'norm.weight', 'layers.21.input_layernorm.weight', 'layers.11.self_attn.qkv_proj.weight', 'layers.3.mlp.down_proj.weight', 'layers.24.mlp.down_proj.weight', 'layers.1.mlp.down_proj.weight', 'layers.6.mlp.down_proj.weight', 'layers.18.mlp.down_proj.weight', 'layers.29.mlp.down_proj.weight', 'layers.28.mlp.down_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.6.self_attn.o_proj.weight', 'layers.24.input_layernorm.weight', 'layers.7.mlp.gate_up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.8.self_attn.qkv_proj.weight', 'layers.10.self_attn.qkv_proj.weight', 'layers.20.self_attn.qkv_proj.weight', 'layers.28.mlp.gate_up_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.3.input_layernorm.weight', 'layers.8.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.15.self_attn.qkv_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.14.post_attention_layernorm.weight', 'layers.17.self_attn.o_proj.weight', 'layers.18.mlp.gate_up_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.30.input_layernorm.weight', 'layers.22.mlp.gate_up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.30.self_attn.qkv_proj.weight', 'layers.15.mlp.gate_up_proj.weight', 'layers.15.mlp.down_proj.weight', 'layers.2.self_attn.qkv_proj.weight', 'layers.13.self_attn.qkv_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.qkv_proj.weight', 'layers.29.self_attn.qkv_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.5.self_attn.o_proj.weight', 'layers.8.mlp.down_proj.weight', 'layers.1.input_layernorm.weight', 'layers.23.self_attn.qkv_proj.weight', 'layers.6.self_attn.qkv_proj.weight', 'layers.1.mlp.gate_up_proj.weight', 'layers.4.mlp.down_proj.weight', 'layers.27.self_attn.qkv_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.6.post_attention_layernorm.weight', 'layers.22.input_layernorm.weight', 'layers.28.post_attention_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.19.mlp.down_proj.weight', 'layers.23.mlp.gate_up_proj.weight', 'layers.26.mlp.gate_up_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.2.mlp.gate_up_proj.weight', 'layers.12.input_layernorm.weight', 'layers.4.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.7.mlp.down_proj.weight', 'layers.21.mlp.gate_up_proj.weight', 'layers.0.mlp.gate_up_proj.weight', 'layers.31.mlp.gate_up_proj.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_up_proj.weight', 'layers.4.mlp.gate_up_proj.weight', 'layers.12.mlp.down_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.9.post_attention_layernorm.weight', 'layers.31.self_attn.qkv_proj.weight', 'layers.6.mlp.gate_up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.mlp.gate_up_proj.weight', 'layers.16.self_attn.qkv_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.26.self_attn.qkv_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.10.input_layernorm.weight', 'layers.16.self_attn.o_proj.weight', 'layers.20.mlp.down_proj.weight', 'layers.30.mlp.down_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.24.mlp.gate_up_proj.weight', 'layers.8.mlp.gate_up_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.31.mlp.down_proj.weight', 'layers.26.mlp.down_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.9.mlp.down_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.19.mlp.gate_up_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.13.input_layernorm.weight', 'layers.29.mlp.gate_up_proj.weight', 'layers.24.self_attn.qkv_proj.weight', 'layers.23.input_layernorm.weight', 'layers.13.post_attention_layernorm.weight', 'layers.16.mlp.gate_up_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.27.mlp.down_proj.weight', 'layers.10.mlp.gate_up_proj.weight', 'layers.0.input_layernorm.weight', 'layers.16.post_attention_layernorm.weight', 'layers.9.input_layernorm.weight', 'layers.22.self_attn.o_proj.weight', 'layers.5.mlp.down_proj.weight', 'layers.15.input_layernorm.weight', 'layers.6.input_layernorm.weight', 'layers.0.self_attn.qkv_proj.weight', 'layers.25.mlp.gate_up_proj.weight'}]
INFO 01-25 02:24:12 llama.py:579] hosseins: LlamaModel -> load_weights() [len(processed_params)=194]
INFO 01-25 02:24:12 llama.py:580] hosseins: LlamaModel -> load_weights() [processed_params={'embed_tokens.weight', 'layers.31.post_attention_layernorm.weight', 'layers.30.post_attention_layernorm.weight', 'layers.5.mlp.gate_up_proj.weight', 'layers.14.self_attn.qkv_proj.weight', 'layers.17.mlp.gate_up_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.22.self_attn.qkv_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.28.input_layernorm.weight', 'layers.1.self_attn.qkv_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.5.self_attn.qkv_proj.weight', 'layers.17.self_attn.qkv_proj.weight', 'layers.25.self_attn.qkv_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.3.self_attn.o_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.19.self_attn.o_proj.weight', 'layers.26.input_layernorm.weight', 'layers.25.self_attn.o_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.18.self_attn.qkv_proj.weight', 'layers.4.self_attn.qkv_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.23.mlp.down_proj.weight', 'layers.25.input_layernorm.weight', 'layers.12.self_attn.qkv_proj.weight', 'layers.11.input_layernorm.weight', 'layers.7.self_attn.o_proj.weight', 'layers.17.input_layernorm.weight', 'layers.29.input_layernorm.weight', 'layers.29.self_attn.o_proj.weight', 'layers.19.self_attn.qkv_proj.weight', 'layers.3.mlp.gate_up_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.27.input_layernorm.weight', 'layers.8.post_attention_layernorm.weight', 'layers.13.mlp.gate_up_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.0.mlp.down_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.post_attention_layernorm.weight', 'layers.5.input_layernorm.weight', 'layers.28.self_attn.qkv_proj.weight', 'layers.7.self_attn.qkv_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.3.self_attn.qkv_proj.weight', 'layers.31.input_layernorm.weight', 'layers.26.post_attention_layernorm.weight', 'layers.30.mlp.gate_up_proj.weight', 'layers.27.mlp.gate_up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.20.mlp.gate_up_proj.weight', 'layers.14.input_layernorm.weight', 'layers.19.input_layernorm.weight', 'layers.20.input_layernorm.weight', 'layers.24.post_attention_layernorm.weight', 'layers.9.mlp.gate_up_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.13.mlp.down_proj.weight', 'layers.14.mlp.gate_up_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.7.input_layernorm.weight', 'layers.25.post_attention_layernorm.weight', 'layers.18.input_layernorm.weight', 'layers.9.self_attn.qkv_proj.weight', 'norm.weight', 'layers.21.input_layernorm.weight', 'layers.11.self_attn.qkv_proj.weight', 'layers.3.mlp.down_proj.weight', 'layers.24.mlp.down_proj.weight', 'layers.1.mlp.down_proj.weight', 'layers.6.mlp.down_proj.weight', 'layers.18.mlp.down_proj.weight', 'layers.29.mlp.down_proj.weight', 'layers.28.mlp.down_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.6.self_attn.o_proj.weight', 'layers.24.input_layernorm.weight', 'layers.7.mlp.gate_up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.8.self_attn.qkv_proj.weight', 'layers.10.self_attn.qkv_proj.weight', 'layers.20.self_attn.qkv_proj.weight', 'layers.28.mlp.gate_up_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.3.input_layernorm.weight', 'layers.8.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.15.self_attn.qkv_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.14.post_attention_layernorm.weight', 'layers.17.self_attn.o_proj.weight', 'layers.18.mlp.gate_up_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.30.input_layernorm.weight', 'layers.22.mlp.gate_up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.30.self_attn.qkv_proj.weight', 'layers.15.mlp.gate_up_proj.weight', 'layers.15.mlp.down_proj.weight', 'layers.2.self_attn.qkv_proj.weight', 'layers.13.self_attn.qkv_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.qkv_proj.weight', 'layers.29.self_attn.qkv_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.5.self_attn.o_proj.weight', 'layers.8.mlp.down_proj.weight', 'layers.1.input_layernorm.weight', 'layers.23.self_attn.qkv_proj.weight', 'layers.6.self_attn.qkv_proj.weight', 'layers.1.mlp.gate_up_proj.weight', 'layers.4.mlp.down_proj.weight', 'layers.27.self_attn.qkv_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.6.post_attention_layernorm.weight', 'layers.22.input_layernorm.weight', 'layers.28.post_attention_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.19.mlp.down_proj.weight', 'layers.23.mlp.gate_up_proj.weight', 'layers.26.mlp.gate_up_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.2.mlp.gate_up_proj.weight', 'layers.12.input_layernorm.weight', 'layers.4.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.7.mlp.down_proj.weight', 'layers.21.mlp.gate_up_proj.weight', 'layers.0.mlp.gate_up_proj.weight', 'layers.31.mlp.gate_up_proj.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_up_proj.weight', 'layers.4.mlp.gate_up_proj.weight', 'layers.12.mlp.down_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.9.post_attention_layernorm.weight', 'layers.31.self_attn.qkv_proj.weight', 'layers.6.mlp.gate_up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.mlp.gate_up_proj.weight', 'layers.16.self_attn.qkv_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.26.self_attn.qkv_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.10.input_layernorm.weight', 'layers.16.self_attn.o_proj.weight', 'layers.20.mlp.down_proj.weight', 'layers.30.mlp.down_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.24.mlp.gate_up_proj.weight', 'layers.8.mlp.gate_up_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.31.mlp.down_proj.weight', 'layers.26.mlp.down_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.9.mlp.down_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.19.mlp.gate_up_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.13.input_layernorm.weight', 'layers.29.mlp.gate_up_proj.weight', 'layers.24.self_attn.qkv_proj.weight', 'layers.23.input_layernorm.weight', 'layers.13.post_attention_layernorm.weight', 'layers.16.mlp.gate_up_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.27.mlp.down_proj.weight', 'layers.10.mlp.gate_up_proj.weight', 'layers.0.input_layernorm.weight', 'layers.16.post_attention_layernorm.weight', 'layers.9.input_layernorm.weight', 'layers.22.self_attn.o_proj.weight', 'layers.5.mlp.down_proj.weight', 'layers.15.input_layernorm.weight', 'layers.6.input_layernorm.weight', 'layers.0.self_attn.qkv_proj.weight', 'layers.25.mlp.gate_up_proj.weight'}]
INFO 01-25 02:24:12 llama.py:581] hosseins: LlamaModel -> load_weights() [len(params_dict.keys())=194]
INFO 01-25 02:24:12 llama.py:582] hosseins: LlamaModel -> load_weights() [total_loaded_params=15009849344]
INFO 01-25 02:24:12 llama.py:586] hosseins: LlamaModel -> load_weights() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:12 llama.py:587] hosseins: LlamaModel -> load_weights() [cpu_mem_util=4.0]
INFO 01-25 02:24:12 loader.py:385] hosseins: DefaultModelLoader load_model() 1 [195 weights loaded]
INFO 01-25 02:24:12 loader.py:389] hosseins: DefaultModelLoader load_model() 2 [{'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:12 loader.py:390] hosseins: DefaultModelLoader load_model() 2 [4.0]
INFO 01-25 02:24:12 llm_engine.py:424] hosseins: LLMEngine -> _initialize_kv_caches()
INFO 01-25 02:24:12 tpu_worker.py:104] hosseins: TPUWorker -> determine_num_available_blocks()
INFO 01-25 02:24:12 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:12 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
INFO 01-25 02:24:12 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 4147575808}]
INFO 01-25 02:24:12 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.0]
INFO 01-25 02:24:13 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:13 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-25 02:24:13 tpu_worker.py:132] hosseins: TPUWorker -> determine_num_available_blocks() -> get_tpu_info(0) [m={'bytes_limit': 33550237696, 'peak_bytes_used': 3883344384}]
INFO 01-25 02:24:13 tpu_executor.py:81] # TPU blocks: 12544, # CPU blocks: 8192
INFO 01-25 02:24:15 tpu_model_runner.py:286] Compiling the model with different input shapes...
INFO 01-25 02:24:15 tpu_model_runner.py:289] hosseins: TPUModelRunner -> warmup_model() 1 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883344384}]
INFO 01-25 02:24:15 tpu_model_runner.py:290] hosseins: TPUModelRunner -> warmup_model() 1 [cpu_mem_util=4.1]
INFO 01-25 02:24:15 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:15 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
INFO 01-25 02:24:15 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883344384}]
INFO 01-25 02:24:15 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:15 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:15 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-25 02:24:15 tpu_model_runner.py:301] batch_size: 1, seq_len: 16
INFO 01-25 02:24:15 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:15 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
INFO 01-25 02:24:15 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883344384}]
INFO 01-25 02:24:15 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:16 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:16 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-25 02:24:16 tpu_model_runner.py:301] batch_size: 1, seq_len: 32
INFO 01-25 02:24:16 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:16 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
INFO 01-25 02:24:16 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883347456}]
INFO 01-25 02:24:16 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:16 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:16 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-25 02:24:16 tpu_model_runner.py:301] batch_size: 1, seq_len: 64
INFO 01-25 02:24:16 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:16 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
INFO 01-25 02:24:16 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883347456}]
INFO 01-25 02:24:16 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:17 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:17 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-25 02:24:17 tpu_model_runner.py:301] batch_size: 1, seq_len: 128
INFO 01-25 02:24:17 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:17 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
INFO 01-25 02:24:17 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883347456}]
INFO 01-25 02:24:17 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:18 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:18 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-25 02:24:18 tpu_model_runner.py:301] batch_size: 1, seq_len: 256
INFO 01-25 02:24:18 tpu_model_runner.py:309] hosseins: TPUModelRunner -> warmup_model() 2 [cpu_mem_util=4.1]
INFO 01-25 02:24:18 tpu_model_runner.py:310] hosseins: TPUModelRunner -> warmup_model() 2 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883348992}]
INFO 01-25 02:24:18 tpu_model_runner.py:313] Compilation for prefill done in 2.92 s.
INFO 01-25 02:24:18 tpu_model_runner.py:341] hosseins: TPUModelRunner -> warmup_model() 3 [cpu_mem_util=4.1]
INFO 01-25 02:24:18 tpu_model_runner.py:342] hosseins: TPUModelRunner -> warmup_model() 3 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883348992}]
INFO 01-25 02:24:18 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:18 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
INFO 01-25 02:24:18 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883348992}]
INFO 01-25 02:24:18 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:19 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:19 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
INFO 01-25 02:24:19 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883348992}]
INFO 01-25 02:24:19 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:19 tpu_model_runner.py:359] batch_size: 8, seq_len: 1
INFO 01-25 02:24:19 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:19 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=16]
INFO 01-25 02:24:19 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883348992}]
INFO 01-25 02:24:19 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:20 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:20 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([16, 128256])]
INFO 01-25 02:24:20 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883348992}]
INFO 01-25 02:24:20 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:20 tpu_model_runner.py:359] batch_size: 16, seq_len: 1
INFO 01-25 02:24:20 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:20 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=32]
INFO 01-25 02:24:20 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883348992}]
INFO 01-25 02:24:20 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:21 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:21 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([32, 128256])]
INFO 01-25 02:24:21 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883422720}]
INFO 01-25 02:24:21 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:21 tpu_model_runner.py:359] batch_size: 32, seq_len: 1
INFO 01-25 02:24:21 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:21 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=48]
INFO 01-25 02:24:21 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883422720}]
INFO 01-25 02:24:21 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:22 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:24:23 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:23 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([48, 128256])]
INFO 01-25 02:24:23 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883473920}]
INFO 01-25 02:24:23 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:23 tpu_model_runner.py:359] batch_size: 48, seq_len: 1
INFO 01-25 02:24:23 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:23 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=64]
INFO 01-25 02:24:23 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883473920}]
INFO 01-25 02:24:23 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:24 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:24 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([64, 128256])]
INFO 01-25 02:24:24 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883473920}]
INFO 01-25 02:24:24 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:24 tpu_model_runner.py:359] batch_size: 64, seq_len: 1
INFO 01-25 02:24:24 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:24 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=80]
INFO 01-25 02:24:24 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883473920}]
INFO 01-25 02:24:24 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:25 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:25 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([80, 128256])]
INFO 01-25 02:24:25 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883473920}]
INFO 01-25 02:24:25 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:25 tpu_model_runner.py:359] batch_size: 80, seq_len: 1
INFO 01-25 02:24:25 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:25 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=96]
INFO 01-25 02:24:25 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883473920}]
INFO 01-25 02:24:25 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:26 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:26 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([96, 128256])]
INFO 01-25 02:24:26 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883546624}]
INFO 01-25 02:24:26 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:26 tpu_model_runner.py:359] batch_size: 96, seq_len: 1
INFO 01-25 02:24:26 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:26 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=112]
INFO 01-25 02:24:26 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883546624}]
INFO 01-25 02:24:26 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:27 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:27 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([112, 128256])]
INFO 01-25 02:24:27 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883546624}]
INFO 01-25 02:24:27 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:27 tpu_model_runner.py:359] batch_size: 112, seq_len: 1
INFO 01-25 02:24:27 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:27 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=128]
INFO 01-25 02:24:27 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883546624}]
INFO 01-25 02:24:27 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:29 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:29 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([128, 128256])]
INFO 01-25 02:24:29 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883602944}]
INFO 01-25 02:24:29 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:29 tpu_model_runner.py:359] batch_size: 128, seq_len: 1
INFO 01-25 02:24:29 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:29 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=144]
INFO 01-25 02:24:29 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883602944}]
INFO 01-25 02:24:29 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:30 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:30 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([144, 128256])]
INFO 01-25 02:24:30 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883602944}]
INFO 01-25 02:24:30 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:30 tpu_model_runner.py:359] batch_size: 144, seq_len: 1
INFO 01-25 02:24:30 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:30 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=160]
INFO 01-25 02:24:30 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883602944}]
INFO 01-25 02:24:30 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:31 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:31 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([160, 128256])]
INFO 01-25 02:24:31 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883678720}]
INFO 01-25 02:24:31 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:31 tpu_model_runner.py:359] batch_size: 160, seq_len: 1
INFO 01-25 02:24:31 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:31 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=176]
INFO 01-25 02:24:31 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883678720}]
INFO 01-25 02:24:31 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:32 client.py:180] hosseins: MQLLMEngineClient -> run_output_handler_loop()
INFO 01-25 02:24:33 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:33 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([176, 128256])]
INFO 01-25 02:24:33 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883725824}]
INFO 01-25 02:24:33 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:33 tpu_model_runner.py:359] batch_size: 176, seq_len: 1
INFO 01-25 02:24:33 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:33 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=192]
INFO 01-25 02:24:33 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883725824}]
INFO 01-25 02:24:33 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:34 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:34 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([192, 128256])]
INFO 01-25 02:24:34 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883725824}]
INFO 01-25 02:24:34 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:34 tpu_model_runner.py:359] batch_size: 192, seq_len: 1
INFO 01-25 02:24:34 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:34 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=208]
INFO 01-25 02:24:34 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883725824}]
INFO 01-25 02:24:34 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:35 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:35 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([208, 128256])]
INFO 01-25 02:24:35 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883725824}]
INFO 01-25 02:24:35 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:35 tpu_model_runner.py:359] batch_size: 208, seq_len: 1
INFO 01-25 02:24:35 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:35 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=224]
INFO 01-25 02:24:35 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883725824}]
INFO 01-25 02:24:35 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:36 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:36 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([224, 128256])]
INFO 01-25 02:24:36 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883794432}]
INFO 01-25 02:24:36 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:36 tpu_model_runner.py:359] batch_size: 224, seq_len: 1
INFO 01-25 02:24:36 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:36 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=240]
INFO 01-25 02:24:36 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883794432}]
INFO 01-25 02:24:36 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:38 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:38 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([240, 128256])]
INFO 01-25 02:24:38 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883852800}]
INFO 01-25 02:24:38 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:38 tpu_model_runner.py:359] batch_size: 240, seq_len: 1
INFO 01-25 02:24:38 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:24:38 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=256]
INFO 01-25 02:24:38 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883852800}]
INFO 01-25 02:24:38 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:24:39 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:24:39 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([256, 128256])]
INFO 01-25 02:24:39 tpu_model_runner.py:355] hosseins: TPUModelRunner -> warmup_model() 4 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883852800}]
INFO 01-25 02:24:39 tpu_model_runner.py:356] hosseins: TPUModelRunner -> warmup_model() 4 [cpu_mem_util=4.1]
INFO 01-25 02:24:39 tpu_model_runner.py:359] batch_size: 256, seq_len: 1
INFO 01-25 02:24:39 tpu_model_runner.py:366] Compilation for decode done in 21.27 s.
INFO 01-25 02:24:39 llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 26.79 seconds
INFO 01-25 02:24:39 llm_engine.py:290] hosseins: LLMEngine -> __init__() 2 [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883852800}]
INFO 01-25 02:24:39 llm_engine.py:291] hosseins: LLMEngine -> __init__() 2 [cpu_mem_util=4.1]
INFO 01-25 02:24:39 api_server.py:677] hosseins: init_app_state
INFO 01-25 02:24:39 api_server.py:697] Using supplied chat template:
INFO 01-25 02:24:39 api_server.py:697] None
INFO 01-25 02:24:39 launcher.py:19] Available routes are:
INFO 01-25 02:24:39 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET
INFO 01-25 02:24:39 launcher.py:27] Route: /docs, Methods: HEAD, GET
INFO 01-25 02:24:39 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 01-25 02:24:39 launcher.py:27] Route: /redoc, Methods: HEAD, GET
INFO 01-25 02:24:39 launcher.py:27] Route: /health, Methods: GET
INFO 01-25 02:24:39 launcher.py:27] Route: /ping, Methods: GET, POST
INFO 01-25 02:24:39 launcher.py:27] Route: /tokenize, Methods: POST
INFO 01-25 02:24:39 launcher.py:27] Route: /detokenize, Methods: POST
INFO 01-25 02:24:39 launcher.py:27] Route: /v1/models, Methods: GET
INFO 01-25 02:24:39 launcher.py:27] Route: /version, Methods: GET
INFO 01-25 02:24:39 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 01-25 02:24:39 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 01-25 02:24:39 launcher.py:27] Route: /v1/embeddings, Methods: POST
INFO 01-25 02:24:39 launcher.py:27] Route: /pooling, Methods: POST
INFO 01-25 02:24:39 launcher.py:27] Route: /score, Methods: POST
INFO 01-25 02:24:39 launcher.py:27] Route: /v1/score, Methods: POST
INFO 01-25 02:24:39 launcher.py:27] Route: /invocations, Methods: POST
INFO:     Started server process [3475013]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO 01-25 02:25:25 api_server.py:396] hosseins: create_completion
INFO 01-25 02:25:25 api_server.py:398] hosseins: handler=<vllm.entrypoints.openai.serving_completion.OpenAIServingCompletion object at 0x7c3206fcd990>
INFO 01-25 02:25:25 serving_completion.py:72] hosseins: OpenAIServingCompletion.create_completion
INFO 01-25 02:25:25 serving_completion.py:84] hosseins: 1
INFO 01-25 02:25:25 serving_completion.py:92] hosseins: 2
INFO 01-25 02:25:25 serving_completion.py:102] hosseins: 3
INFO 01-25 02:25:25 serving_completion.py:105] hosseins: 4 - request=CompletionRequest(model='meta-llama/Meta-Llama-3.1-8B', prompt='Humans superpowers are ', best_of=None, echo=False, frequency_penalty=0.0, logit_bias=None, logprobs=None, max_tokens=5, n=1, presence_penalty=0.0, seed=None, stop=[], stream=True, stream_options=None, suffix=None, temperature=0.0, top_p=None, user=None, use_beam_search=False, top_k=None, min_p=None, repetition_penalty=None, length_penalty=1.0, stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, min_tokens=0, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, allowed_token_ids=None, prompt_logprobs=None, add_special_tokens=True, response_format=None, guided_json=None, guided_regex=None, guided_choice=None, guided_grammar=None, guided_decoding_backend=None, guided_whitespace_pattern=None, priority=0, logits_processors=None)
INFO 01-25 02:25:25 serving_completion.py:114] hosseins: 5 - request_prompts=[{'prompt': 'Humans superpowers are ', 'prompt_token_ids': [128000, 95668, 2307, 78404, 527, 220]}]
INFO 01-25 02:25:25 serving_completion.py:115] hosseins: 6 - engine_prompts=[{'prompt_token_ids': [128000, 95668, 2307, 78404, 527, 220]}]
INFO 01-25 02:25:25 serving_completion.py:125] hosseins: 7 - i=0 - engine_prompt={'prompt_token_ids': [128000, 95668, 2307, 78404, 527, 220]}
INFO 01-25 02:25:25 serving_completion.py:129] hosseins: 8 - default_max_tokens=250
INFO 01-25 02:25:25 serving_completion.py:142] hosseins: 9 - sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None)
INFO 01-25 02:25:25 serving_completion.py:144] hosseins: 10 - request_id_item='cmpl-8b67a07303f04bfd8678dd25630ffc41-0'
INFO 01-25 02:25:25 serving_completion.py:171] hosseins: 11 - generator=<async_generator object MQLLMEngineClient._process_request at 0x7c3206f89ac0>
INFO 01-25 02:25:25 serving_completion.py:183] hosseins: 12 - num_prompts=1
INFO 01-25 02:25:25 serving_completion.py:192] hosseins: 13 - stream=True
INFO:     127.0.0.1:33744 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-25 02:25:25 serving_completion.py:261] hosseins - 13
INFO 01-25 02:25:25 llm_engine.py:583] hosseins: LLMEngine -> get_tokenizer()
INFO 01-25 02:25:25 llm_engine.py:566] hosseins: LLMEngine -> get_tokenizer_group() group_type=<class 'vllm.transformers_utils.tokenizer_group.base_tokenizer_group.BaseTokenizerGroup'>
INFO 01-25 02:25:25 llm_engine.py:617] hosseins: LLMEngine -> _add_processed_request()
INFO 01-25 02:25:25 llm_engine.py:1324] hosseins: LLMEngine -> step()
INFO 01-25 02:25:25 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:25:25 worker_base.py:318] hosseins: LocalOrDistributedWorkerBase -> execute_model()
INFO 01-25 02:25:25 worker_base.py:329] hosseins: LocalOrDistributedWorkerBase -> execute_model() [self.execute_worker=<bound method TPUWorker.execute_worker of <vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker object at 0x7af1ac314190>>]
INFO 01-25 02:25:25 worker_base.py:330] hosseins: LocalOrDistributedWorkerBase -> execute_model() [num_steps=1]
INFO 01-25 02:25:25 tpu_model_runner.py:620] hosseins: TPUModelRunner -> execute_model()
INFO 01-25 02:25:25 tpu_model_runner.py:654] hosseins: TPUModelRunner -> execute_model() [is_prompt=True]
INFO 01-25 02:25:25 tpu_model_runner.py:655] hosseins: TPUModelRunner -> execute_model() [model_input.attn_metadata.num_prefills=1]
INFO 01-25 02:25:25 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:25:25 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=1]
INFO 01-25 02:25:25 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3883888640}]
INFO 01-25 02:25:25 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:25:26 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:25:26 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([1, 128256])]
INFO 01-25 02:25:26 llm_engine.py:1033] hosseins: LLMEngine -> _process_model_outputs()
INFO 01-25 02:25:26 llm_engine.py:1042] hosseins: LLMEngine -> _process_model_outputs() -1 - [0]
INFO 01-25 02:25:33 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:25:33 llm_engine.py:1282] hosseins: LLMEngine -> _advance_to_next_step()
INFO 01-25 02:25:33 llm_engine.py:1324] hosseins: LLMEngine -> step()
INFO 01-25 02:25:33 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:25:33 worker_base.py:318] hosseins: LocalOrDistributedWorkerBase -> execute_model()
INFO 01-25 02:25:33 worker_base.py:329] hosseins: LocalOrDistributedWorkerBase -> execute_model() [self.execute_worker=<bound method TPUWorker.execute_worker of <vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker object at 0x7af1ac314190>>]
INFO 01-25 02:25:33 worker_base.py:330] hosseins: LocalOrDistributedWorkerBase -> execute_model() [num_steps=4]
INFO 01-25 02:25:33 tpu_model_runner.py:620] hosseins: TPUModelRunner -> execute_model()
INFO 01-25 02:25:33 tpu_model_runner.py:654] hosseins: TPUModelRunner -> execute_model() [is_prompt=False]
INFO 01-25 02:25:33 tpu_model_runner.py:655] hosseins: TPUModelRunner -> execute_model() [model_input.attn_metadata.num_prefills=0]
INFO 01-25 02:25:33 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:25:33 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
INFO 01-25 02:25:33 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3944712704}]
INFO 01-25 02:25:33 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:25:33 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:25:33 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
INFO 01-25 02:25:33 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:25:33 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
INFO 01-25 02:25:33 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3944712704}]
INFO 01-25 02:25:33 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:25:34 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:25:34 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
INFO 01-25 02:25:34 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:25:34 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
INFO 01-25 02:25:34 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3944712704}]
INFO 01-25 02:25:34 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:25:35 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:25:35 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
INFO 01-25 02:25:35 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:25:35 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
INFO 01-25 02:25:35 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 3944712704}]
INFO 01-25 02:25:35 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=4.1]
INFO 01-25 02:25:36 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:25:36 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
INFO 01-25 02:25:36 llm_engine.py:1033] hosseins: LLMEngine -> _process_model_outputs()
INFO 01-25 02:25:36 llm_engine.py:1042] hosseins: LLMEngine -> _process_model_outputs() -1 - [1]
INFO 01-25 02:25:36 llm_engine.py:1046] hosseins: LLMEngine -> _process_model_outputs() 0
INFO 01-25 02:25:36 llm_engine.py:1059] hosseins: LLMEngine -> _process_model_outputs() 1
INFO 01-25 02:25:36 llm_engine.py:1066] hosseins: LLMEngine -> _process_model_outputs() 2
INFO 01-25 02:25:36 llm_engine.py:1083] hosseins: LLMEngine -> _process_model_outputs() 3
INFO 01-25 02:25:36 llm_engine.py:1102] hosseins: LLMEngine -> _process_model_outputs() 4
INFO 01-25 02:25:36 llm_engine.py:1114] hosseins: LLMEngine -> _process_model_outputs() 4 - [0]
INFO 01-25 02:25:36 llm_engine.py:1125] hosseins: LLMEngine -> _process_model_outputs() 5 - [0]
INFO 01-25 02:25:36 llm_engine.py:1135] hosseins: LLMEngine -> _process_model_outputs() 6 - [0]
INFO 01-25 02:25:36 llm_engine.py:1153] hosseins: LLMEngine -> _process_model_outputs() 7 - [0]
INFO 01-25 02:25:36 llm_engine.py:1165] hosseins: LLMEngine -> _process_model_outputs() 8
INFO 01-25 02:25:36 llm_engine.py:1195] hosseins: LLMEngine -> _process_model_outputs() 10
INFO 01-25 02:25:36 llm_engine.py:1211] hosseins: LLMEngine -> _process_model_outputs() 11
INFO 01-25 02:25:36 llm_engine.py:1231] hosseins: LLMEngine -> _process_model_outputs() 12 [0]
INFO 01-25 02:25:36 llm_engine.py:1240] hosseins: LLMEngine -> _process_model_outputs() 13
INFO 01-25 02:25:36 llm_engine.py:1255] hosseins: LLMEngine -> _process_model_outputs() 14
INFO 01-25 02:25:36 llm_engine.py:1262] hosseins: LLMEngine -> _process_model_outputs() 15
INFO 01-25 02:25:36 metrics.py:467] Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-25 02:25:36 llm_engine.py:1274] hosseins: LLMEngine -> _process_model_outputs() 16
INFO 01-25 02:25:36 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:25:36 llm_engine.py:1324] hosseins: LLMEngine -> step()
INFO 01-25 02:25:36 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:25:36 worker_base.py:318] hosseins: LocalOrDistributedWorkerBase -> execute_model()
INFO 01-25 02:25:36 worker_base.py:329] hosseins: LocalOrDistributedWorkerBase -> execute_model() [self.execute_worker=<bound method TPUWorker.execute_worker of <vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker object at 0x7af1ac314190>>]
INFO 01-25 02:25:36 worker_base.py:330] hosseins: LocalOrDistributedWorkerBase -> execute_model() [num_steps=1]
INFO 01-25 02:25:36 tpu_model_runner.py:620] hosseins: TPUModelRunner -> execute_model()
INFO 01-25 02:25:36 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:25:36 llm_engine.py:1324] hosseins: LLMEngine -> step()
INFO 01-25 02:25:36 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:25:36 worker_base.py:318] hosseins: LocalOrDistributedWorkerBase -> execute_model()
INFO 01-25 02:25:36 worker_base.py:329] hosseins: LocalOrDistributedWorkerBase -> execute_model() [self.execute_worker=<bound method TPUWorker.execute_worker of <vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker object at 0x7af1ac314190>>]
INFO 01-25 02:25:36 worker_base.py:330] hosseins: LocalOrDistributedWorkerBase -> execute_model() [num_steps=1]
INFO 01-25 02:25:36 serving_completion.py:278] hosseins - 14 prompt_idx=0
INFO 01-25 02:25:36 tpu_model_runner.py:620] hosseins: TPUModelRunner -> execute_model()
INFO 01-25 02:25:36 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:25:36 llm_engine.py:1324] hosseins: LLMEngine -> step()
INFO 01-25 02:25:36 serving_completion.py:279] hosseins - 15 res=RequestOutput(request_id=cmpl-8b67a07303f04bfd8678dd25630ffc41-0, prompt=None, prompt_token_ids=[128000, 95668, 2307, 78404, 527, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='1', token_ids=[16], cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1737771925.8623707, last_token_time=1737771936.0906353, first_scheduled_time=1737771925.86312, first_token_time=1737771936.0906353, time_in_queue=0.0007493495941162109, finished_time=None, scheduler_time=0.0004300600121496245, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 01-25 02:25:36 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:25:36 worker_base.py:318] hosseins: LocalOrDistributedWorkerBase -> execute_model()
INFO 01-25 02:25:36 serving_completion.py:294] hosseins - 16.0 i=0
INFO 01-25 02:25:36 serving_completion.py:295] hosseins - 16 output=CompletionOutput(index=0, text='1', token_ids=[16], cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)
INFO 01-25 02:25:36 worker_base.py:329] hosseins: LocalOrDistributedWorkerBase -> execute_model() [self.execute_worker=<bound method TPUWorker.execute_worker of <vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker object at 0x7af1ac314190>>]
INFO 01-25 02:25:36 serving_completion.py:296] hosseins - 17 request.echo=False
INFO 01-25 02:25:36 worker_base.py:330] hosseins: LocalOrDistributedWorkerBase -> execute_model() [num_steps=1]
INFO 01-25 02:25:36 serving_completion.py:330] hosseins - 18 request.logprobs=None
INFO 01-25 02:25:36 tpu_model_runner.py:620] hosseins: TPUModelRunner -> execute_model()
INFO 01-25 02:25:36 serving_completion.py:351] hosseins - 21 previous_text_lens[i]=1
INFO 01-25 02:25:36 serving_completion.py:352] hosseins - 22 previous_num_tokens[i]=1
INFO 01-25 02:25:36 serving_completion.py:353] hosseins - 23 previous_num_tokens[i]=1
INFO 01-25 02:25:36 serving_completion.py:369] hosseins - 24 chunk=CompletionStreamResponse(id='cmpl-8b67a07303f04bfd8678dd25630ffc41', object='text_completion', created=1737771925, model='meta-llama/Meta-Llama-3.1-8B', choices=[CompletionResponseStreamChoice(index=0, text='1', logprobs=None, finish_reason=None, stop_reason=None)], usage=None)
INFO 01-25 02:25:36 serving_completion.py:370] hosseins - 25 include_continuous_usage=False
INFO 01-25 02:25:36 serving_completion.py:385] hosseins - 28 response_json='{"id":"cmpl-8b67a07303f04bfd8678dd25630ffc41","object":"text_completion","created":1737771925,"model":"meta-llama/Meta-Llama-3.1-8B","choices":[{"index":0,"text":"1","logprobs":null,"finish_reason":null,"stop_reason":null}],"usage":null}'
INFO 01-25 02:27:36 llm_engine.py:1033] hosseins: LLMEngine -> _process_model_outputs()
INFO 01-25 02:27:36 llm_engine.py:1042] hosseins: LLMEngine -> _process_model_outputs() -1 - [1]
INFO 01-25 02:27:36 llm_engine.py:1046] hosseins: LLMEngine -> _process_model_outputs() 0
INFO 01-25 02:27:36 llm_engine.py:1059] hosseins: LLMEngine -> _process_model_outputs() 1
INFO 01-25 02:27:36 llm_engine.py:1066] hosseins: LLMEngine -> _process_model_outputs() 2
INFO 01-25 02:27:36 llm_engine.py:1083] hosseins: LLMEngine -> _process_model_outputs() 3
INFO 01-25 02:27:36 llm_engine.py:1102] hosseins: LLMEngine -> _process_model_outputs() 4
INFO 01-25 02:27:36 llm_engine.py:1114] hosseins: LLMEngine -> _process_model_outputs() 4 - [0]
INFO 01-25 02:27:36 llm_engine.py:1125] hosseins: LLMEngine -> _process_model_outputs() 5 - [0]
INFO 01-25 02:27:36 llm_engine.py:1135] hosseins: LLMEngine -> _process_model_outputs() 6 - [0]
INFO 01-25 02:27:36 llm_engine.py:1153] hosseins: LLMEngine -> _process_model_outputs() 7 - [0]
INFO 01-25 02:27:36 llm_engine.py:1165] hosseins: LLMEngine -> _process_model_outputs() 8
INFO 01-25 02:27:36 llm_engine.py:1195] hosseins: LLMEngine -> _process_model_outputs() 10
INFO 01-25 02:27:36 llm_engine.py:1211] hosseins: LLMEngine -> _process_model_outputs() 11
INFO 01-25 02:27:36 llm_engine.py:1231] hosseins: LLMEngine -> _process_model_outputs() 12 [0]
INFO 01-25 02:27:36 serving_completion.py:278] hosseins - 14 prompt_idx=0
INFO 01-25 02:27:36 serving_completion.py:279] hosseins - 15 res=RequestOutput(request_id=cmpl-8b67a07303f04bfd8678dd25630ffc41-0, prompt=None, prompt_token_ids=None, encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=')', token_ids=[8], cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1737771925.8623707, last_token_time=1737772056.8548653, first_scheduled_time=1737771925.86312, first_token_time=1737771936.0906353, time_in_queue=0.0007493495941162109, finished_time=None, scheduler_time=0.0004300600121496245, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 01-25 02:27:36 serving_completion.py:294] hosseins - 16.0 i=0
INFO 01-25 02:27:36 serving_completion.py:295] hosseins - 16 output=CompletionOutput(index=0, text=')', token_ids=[8], cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)
INFO 01-25 02:27:36 serving_completion.py:296] hosseins - 17 request.echo=False
INFO 01-25 02:27:36 serving_completion.py:330] hosseins - 18 request.logprobs=None
INFO 01-25 02:27:36 serving_completion.py:351] hosseins - 21 previous_text_lens[i]=2
INFO 01-25 02:27:36 serving_completion.py:352] hosseins - 22 previous_num_tokens[i]=2
INFO 01-25 02:27:36 serving_completion.py:353] hosseins - 23 previous_num_tokens[i]=2
INFO 01-25 02:27:36 serving_completion.py:369] hosseins - 24 chunk=CompletionStreamResponse(id='cmpl-8b67a07303f04bfd8678dd25630ffc41', object='text_completion', created=1737771925, model='meta-llama/Meta-Llama-3.1-8B', choices=[CompletionResponseStreamChoice(index=0, text=')', logprobs=None, finish_reason=None, stop_reason=None)], usage=None)
INFO 01-25 02:27:36 serving_completion.py:370] hosseins - 25 include_continuous_usage=False
INFO 01-25 02:27:36 serving_completion.py:385] hosseins - 28 response_json='{"id":"cmpl-8b67a07303f04bfd8678dd25630ffc41","object":"text_completion","created":1737771925,"model":"meta-llama/Meta-Llama-3.1-8B","choices":[{"index":0,"text":")","logprobs":null,"finish_reason":null,"stop_reason":null}],"usage":null}'
INFO 01-25 02:29:54 llm_engine.py:1033] hosseins: LLMEngine -> _process_model_outputs()
INFO 01-25 02:29:54 llm_engine.py:1042] hosseins: LLMEngine -> _process_model_outputs() -1 - [1]
INFO 01-25 02:29:54 llm_engine.py:1046] hosseins: LLMEngine -> _process_model_outputs() 0
INFO 01-25 02:29:54 llm_engine.py:1059] hosseins: LLMEngine -> _process_model_outputs() 1
INFO 01-25 02:29:54 llm_engine.py:1066] hosseins: LLMEngine -> _process_model_outputs() 2
INFO 01-25 02:29:54 llm_engine.py:1083] hosseins: LLMEngine -> _process_model_outputs() 3
INFO 01-25 02:29:54 llm_engine.py:1102] hosseins: LLMEngine -> _process_model_outputs() 4
INFO 01-25 02:29:54 llm_engine.py:1114] hosseins: LLMEngine -> _process_model_outputs() 4 - [0]
INFO 01-25 02:29:54 llm_engine.py:1125] hosseins: LLMEngine -> _process_model_outputs() 5 - [0]
INFO 01-25 02:29:54 llm_engine.py:1135] hosseins: LLMEngine -> _process_model_outputs() 6 - [0]
INFO 01-25 02:29:54 llm_engine.py:1153] hosseins: LLMEngine -> _process_model_outputs() 7 - [0]
INFO 01-25 02:29:54 llm_engine.py:1165] hosseins: LLMEngine -> _process_model_outputs() 8
INFO 01-25 02:29:54 llm_engine.py:1195] hosseins: LLMEngine -> _process_model_outputs() 10
INFO 01-25 02:29:54 llm_engine.py:1211] hosseins: LLMEngine -> _process_model_outputs() 11
INFO 01-25 02:29:54 llm_engine.py:1231] hosseins: LLMEngine -> _process_model_outputs() 12 [0]
INFO 01-25 02:29:54 serving_completion.py:278] hosseins - 14 prompt_idx=0
INFO 01-25 02:29:54 serving_completion.py:279] hosseins - 15 res=RequestOutput(request_id=cmpl-8b67a07303f04bfd8678dd25630ffc41-0, prompt=None, prompt_token_ids=None, encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' the', token_ids=[279], cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1737771925.8623707, last_token_time=1737772194.0091197, first_scheduled_time=1737771925.86312, first_token_time=1737771936.0906353, time_in_queue=0.0007493495941162109, finished_time=None, scheduler_time=0.0004300600121496245, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 01-25 02:29:54 serving_completion.py:294] hosseins - 16.0 i=0
INFO 01-25 02:29:54 serving_completion.py:295] hosseins - 16 output=CompletionOutput(index=0, text=' the', token_ids=[279], cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)
INFO 01-25 02:29:54 serving_completion.py:296] hosseins - 17 request.echo=False
INFO 01-25 02:29:54 serving_completion.py:330] hosseins - 18 request.logprobs=None
INFO 01-25 02:29:54 serving_completion.py:351] hosseins - 21 previous_text_lens[i]=6
INFO 01-25 02:29:54 serving_completion.py:352] hosseins - 22 previous_num_tokens[i]=3
INFO 01-25 02:29:54 serving_completion.py:353] hosseins - 23 previous_num_tokens[i]=3
INFO 01-25 02:29:54 serving_completion.py:369] hosseins - 24 chunk=CompletionStreamResponse(id='cmpl-8b67a07303f04bfd8678dd25630ffc41', object='text_completion', created=1737771925, model='meta-llama/Meta-Llama-3.1-8B', choices=[CompletionResponseStreamChoice(index=0, text=' the', logprobs=None, finish_reason=None, stop_reason=None)], usage=None)
INFO 01-25 02:29:54 serving_completion.py:370] hosseins - 25 include_continuous_usage=False
INFO 01-25 02:29:54 serving_completion.py:385] hosseins - 28 response_json='{"id":"cmpl-8b67a07303f04bfd8678dd25630ffc41","object":"text_completion","created":1737771925,"model":"meta-llama/Meta-Llama-3.1-8B","choices":[{"index":0,"text":" the","logprobs":null,"finish_reason":null,"stop_reason":null}],"usage":null}'
INFO 01-25 02:32:21 llm_engine.py:1033] hosseins: LLMEngine -> _process_model_outputs()
INFO 01-25 02:32:21 llm_engine.py:1042] hosseins: LLMEngine -> _process_model_outputs() -1 - [1]
INFO 01-25 02:32:21 llm_engine.py:1046] hosseins: LLMEngine -> _process_model_outputs() 0
INFO 01-25 02:32:21 llm_engine.py:1059] hosseins: LLMEngine -> _process_model_outputs() 1
INFO 01-25 02:32:21 llm_engine.py:1066] hosseins: LLMEngine -> _process_model_outputs() 2
INFO 01-25 02:32:21 llm_engine.py:1083] hosseins: LLMEngine -> _process_model_outputs() 3
INFO 01-25 02:32:21 llm_engine.py:1102] hosseins: LLMEngine -> _process_model_outputs() 4
INFO 01-25 02:32:21 llm_engine.py:1114] hosseins: LLMEngine -> _process_model_outputs() 4 - [0]
INFO 01-25 02:32:21 llm_engine.py:1125] hosseins: LLMEngine -> _process_model_outputs() 5 - [0]
INFO 01-25 02:32:21 llm_engine.py:1135] hosseins: LLMEngine -> _process_model_outputs() 6 - [0]
INFO 01-25 02:32:21 llm_engine.py:1153] hosseins: LLMEngine -> _process_model_outputs() 7 - [0]
INFO 01-25 02:32:21 llm_engine.py:1165] hosseins: LLMEngine -> _process_model_outputs() 8
INFO 01-25 02:32:21 llm_engine.py:1195] hosseins: LLMEngine -> _process_model_outputs() 10
INFO 01-25 02:32:21 llm_engine.py:1211] hosseins: LLMEngine -> _process_model_outputs() 11
INFO 01-25 02:32:21 llm_engine.py:1231] hosseins: LLMEngine -> _process_model_outputs() 12 [0]
INFO 01-25 02:32:21 serving_completion.py:278] hosseins - 14 prompt_idx=0
INFO 01-25 02:32:21 serving_completion.py:279] hosseins - 15 res=RequestOutput(request_id=cmpl-8b67a07303f04bfd8678dd25630ffc41-0, prompt=None, prompt_token_ids=None, encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' ability', token_ids=[5845], cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1737771925.8623707, last_token_time=1737772341.2936146, first_scheduled_time=1737771925.86312, first_token_time=1737771936.0906353, time_in_queue=0.0007493495941162109, finished_time=None, scheduler_time=0.0004300600121496245, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 01-25 02:32:21 serving_completion.py:294] hosseins - 16.0 i=0
INFO 01-25 02:32:21 serving_completion.py:295] hosseins - 16 output=CompletionOutput(index=0, text=' ability', token_ids=[5845], cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)
INFO 01-25 02:32:21 serving_completion.py:296] hosseins - 17 request.echo=False
INFO 01-25 02:32:21 serving_completion.py:330] hosseins - 18 request.logprobs=None
INFO 01-25 02:32:21 serving_completion.py:351] hosseins - 21 previous_text_lens[i]=14
INFO 01-25 02:32:21 serving_completion.py:352] hosseins - 22 previous_num_tokens[i]=4
INFO 01-25 02:32:21 serving_completion.py:353] hosseins - 23 previous_num_tokens[i]=4
INFO 01-25 02:32:21 serving_completion.py:369] hosseins - 24 chunk=CompletionStreamResponse(id='cmpl-8b67a07303f04bfd8678dd25630ffc41', object='text_completion', created=1737771925, model='meta-llama/Meta-Llama-3.1-8B', choices=[CompletionResponseStreamChoice(index=0, text=' ability', logprobs=None, finish_reason=None, stop_reason=None)], usage=None)
INFO 01-25 02:32:21 serving_completion.py:370] hosseins - 25 include_continuous_usage=False
INFO 01-25 02:32:21 serving_completion.py:385] hosseins - 28 response_json='{"id":"cmpl-8b67a07303f04bfd8678dd25630ffc41","object":"text_completion","created":1737771925,"model":"meta-llama/Meta-Llama-3.1-8B","choices":[{"index":0,"text":" ability","logprobs":null,"finish_reason":null,"stop_reason":null}],"usage":null}'
INFO 01-25 02:34:52 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:34:52 llm_engine.py:1282] hosseins: LLMEngine -> _advance_to_next_step()
INFO 01-25 02:34:52 llm_engine.py:1324] hosseins: LLMEngine -> step()
INFO 01-25 02:34:52 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:34:52 worker_base.py:318] hosseins: LocalOrDistributedWorkerBase -> execute_model()
INFO 01-25 02:34:52 worker_base.py:329] hosseins: LocalOrDistributedWorkerBase -> execute_model() [self.execute_worker=<bound method TPUWorker.execute_worker of <vllm.worker.multi_step_tpu_worker.MultiStepTPUWorker object at 0x7af1ac314190>>]
INFO 01-25 02:34:52 worker_base.py:330] hosseins: LocalOrDistributedWorkerBase -> execute_model() [num_steps=4]
INFO 01-25 02:34:52 tpu_model_runner.py:620] hosseins: TPUModelRunner -> execute_model()
INFO 01-25 02:34:52 tpu_model_runner.py:654] hosseins: TPUModelRunner -> execute_model() [is_prompt=False]
INFO 01-25 02:34:52 tpu_model_runner.py:655] hosseins: TPUModelRunner -> execute_model() [model_input.attn_metadata.num_prefills=0]
INFO 01-25 02:34:52 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:34:52 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
INFO 01-25 02:34:52 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 32162423936}]
INFO 01-25 02:34:52 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=5.2]
INFO 01-25 02:34:52 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:34:52 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
INFO 01-25 02:34:52 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:34:52 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
INFO 01-25 02:34:52 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 32162423936}]
INFO 01-25 02:34:52 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=5.2]
INFO 01-25 02:34:53 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:34:53 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
INFO 01-25 02:34:53 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:34:53 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
INFO 01-25 02:34:53 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 32162469376}]
INFO 01-25 02:34:53 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=5.2]
INFO 01-25 02:34:54 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:34:54 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
INFO 01-25 02:34:54 llama.py:733] hosseins: LlamaForCausalLM -> forward
INFO 01-25 02:34:54 llama.py:399] hosseins: LlamaModel -> get_input_embeddings : [len(input_ids)=8]
INFO 01-25 02:34:54 llama.py:402] hosseins: LlamaModel -> get_input_embeddings() [tpu_activities={'bytes_limit': 33550237696, 'peak_bytes_used': 32162469376}]
INFO 01-25 02:34:54 llama.py:403] hosseins: LlamaModel -> get_input_embeddings() [cpu_mem_util=5.2]
INFO 01-25 02:34:55 llama.py:744] hosseins: LlamaForCausalLM -> compute_logits
INFO 01-25 02:34:55 llama.py:748] hosseins: LlamaForCausalLM -> compute_logits() [logits.shape=torch.Size([8, 128256])]
INFO 01-25 02:34:55 llm_engine.py:1033] hosseins: LLMEngine -> _process_model_outputs()
INFO 01-25 02:34:55 llm_engine.py:1042] hosseins: LLMEngine -> _process_model_outputs() -1 - [1]
INFO 01-25 02:34:55 llm_engine.py:1046] hosseins: LLMEngine -> _process_model_outputs() 0
INFO 01-25 02:34:55 llm_engine.py:1059] hosseins: LLMEngine -> _process_model_outputs() 1
INFO 01-25 02:34:55 llm_engine.py:1066] hosseins: LLMEngine -> _process_model_outputs() 2
INFO 01-25 02:34:55 llm_engine.py:1083] hosseins: LLMEngine -> _process_model_outputs() 3
INFO 01-25 02:34:55 llm_engine.py:1102] hosseins: LLMEngine -> _process_model_outputs() 4
INFO 01-25 02:34:55 llm_engine.py:1114] hosseins: LLMEngine -> _process_model_outputs() 4 - [0]
INFO 01-25 02:34:55 llm_engine.py:1125] hosseins: LLMEngine -> _process_model_outputs() 5 - [0]
INFO 01-25 02:34:55 llm_engine.py:1135] hosseins: LLMEngine -> _process_model_outputs() 6 - [0]
INFO 01-25 02:34:55 llm_engine.py:1153] hosseins: LLMEngine -> _process_model_outputs() 7 - [0]
INFO 01-25 02:34:55 llm_engine.py:1165] hosseins: LLMEngine -> _process_model_outputs() 8
INFO 01-25 02:34:55 llm_engine.py:1181] hosseins: LLMEngine -> _process_model_outputs() 9 [0]
INFO 01-25 02:34:55 llm_engine.py:1195] hosseins: LLMEngine -> _process_model_outputs() 10
INFO 01-25 02:34:55 llm_engine.py:1211] hosseins: LLMEngine -> _process_model_outputs() 11
INFO 01-25 02:34:55 llm_engine.py:1240] hosseins: LLMEngine -> _process_model_outputs() 13
INFO 01-25 02:34:55 llm_engine.py:1255] hosseins: LLMEngine -> _process_model_outputs() 14
INFO 01-25 02:34:55 llm_engine.py:1262] hosseins: LLMEngine -> _process_model_outputs() 15
INFO 01-25 02:34:55 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-25 02:34:55 serving_completion.py:278] hosseins - 14 prompt_idx=0
INFO 01-25 02:34:55 serving_completion.py:279] hosseins - 15 res=RequestOutput(request_id=cmpl-8b67a07303f04bfd8678dd25630ffc41-0, prompt=None, prompt_token_ids=None, encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' to', token_ids=[311], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1737771925.8623707, last_token_time=1737772495.0848513, first_scheduled_time=1737771925.86312, first_token_time=1737771936.0906353, time_in_queue=0.0007493495941162109, finished_time=1737772495.0852277, scheduler_time=0.0009207900147885084, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 01-25 02:34:55 serving_completion.py:294] hosseins - 16.0 i=0
INFO 01-25 02:34:55 serving_completion.py:295] hosseins - 16 output=CompletionOutput(index=0, text=' to', token_ids=[311], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)
INFO 01-25 02:34:55 serving_completion.py:296] hosseins - 17 request.echo=False
INFO 01-25 02:34:55 serving_completion.py:330] hosseins - 18 request.logprobs=None
INFO 01-25 02:34:55 serving_completion.py:351] hosseins - 21 previous_text_lens[i]=17
INFO 01-25 02:34:55 serving_completion.py:352] hosseins - 22 previous_num_tokens[i]=5
INFO 01-25 02:34:55 serving_completion.py:353] hosseins - 23 previous_num_tokens[i]=5
INFO 01-25 02:34:55 serving_completion.py:369] hosseins - 24 chunk=CompletionStreamResponse(id='cmpl-8b67a07303f04bfd8678dd25630ffc41', object='text_completion', created=1737771925, model='meta-llama/Meta-Llama-3.1-8B', choices=[CompletionResponseStreamChoice(index=0, text=' to', logprobs=None, finish_reason='length', stop_reason=None)], usage=None)
INFO 01-25 02:34:55 serving_completion.py:370] hosseins - 25 include_continuous_usage=False
INFO 01-25 02:34:55 serving_completion.py:385] hosseins - 28 response_json='{"id":"cmpl-8b67a07303f04bfd8678dd25630ffc41","object":"text_completion","created":1737771925,"model":"meta-llama/Meta-Llama-3.1-8B","choices":[{"index":0,"text":" to","logprobs":null,"finish_reason":"length","stop_reason":null}],"usage":null}'
INFO 01-25 02:34:55 serving_completion.py:396] hosseins - 29 final_usage_info=UsageInfo(prompt_tokens=6, total_tokens=11, completion_tokens=5, prompt_tokens_details=None)
INFO 01-25 02:34:55 serving_completion.py:397] hosseins - 30 include_usage=False
INFO 01-25 02:34:55 serving_completion.py:418] hosseins - 33 request_metadata.final_usage_info=UsageInfo(prompt_tokens=6, total_tokens=11, completion_tokens=5, prompt_tokens_details=None)
INFO 01-25 02:34:55 llm_engine.py:1274] hosseins: LLMEngine -> _process_model_outputs() 16
INFO 01-25 02:34:55 llm_engine.py:1532] hosseins: LLMEngine -> _has_remaining_steps()
INFO 01-25 02:35:05 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
